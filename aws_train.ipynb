{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3802534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbc661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker boto3 awscli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a5e00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --profile asher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dc47ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f59a2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "Role:  arn:aws:iam::345594598345:role/service-role/AmazonSageMaker-ExecutionRole-20250707T163996\n",
      "sagemaker-ap-southeast-1-345594598345\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.local import LocalSession\n",
    "role = \"arn:aws:iam::345594598345:role/service-role/AmazonSageMaker-ExecutionRole-20250707T163996\" # get_execution_role() #\n",
    "print(\"Role: \", role)\n",
    "# local_sess = LocalSession()  # for local session\n",
    "# local_sess.config = {'local': {'local_code': True}}\n",
    "\n",
    "sess = sagemaker.Session(boto_session=boto3.Session(region_name=\"ap-southeast-1\")) #name of sso session remove profile name in sagemaker studio , profile_name=\"asher\"\n",
    "# #for remote session\n",
    "# settings = dict(\n",
    "#     sagemaker_session=sess,\n",
    "#     role=role,\n",
    "#     instance_type=\"ml.t3.large\",\n",
    "#     dependencies='./requirements.txt',\n",
    "# )\n",
    "\n",
    "bucket_name = sess.default_bucket()\n",
    "print(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924f7915",
   "metadata": {},
   "source": [
    "Uploaded training, testing and validation datasets to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd2b3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 cp ./data s3://{bucket_name}/prostate158 --recursive --profile asher\n",
    "# print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072598c1",
   "metadata": {},
   "source": [
    "Upload weights of pre-trained UNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa3ccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 cp models/tumor.pt s3://{bucket_name}/models/tumor.pt --profile asher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0daea875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 's3://sagemaker-ap-southeast-1-345594598345/prostate158/prostate158_train/train', 'test': 's3://sagemaker-ap-southeast-1-345594598345/prostate158/prostate158_test/test', 'model': 's3://sagemaker-ap-southeast-1-345594598345/models/tumor.pt'}\n"
     ]
    }
   ],
   "source": [
    "if bucket_name:\n",
    "    train_channel = f\"s3://{bucket_name}/prostate158/prostate158_train/train\"\n",
    "    test_channel = f\"s3://{bucket_name}/prostate158/prostate158_test/test\"\n",
    "    model_channel = f\"s3://{bucket_name}/models/tumor.pt\"\n",
    "inputs = {\"train\": train_channel, \"test\": test_channel, \"model\": model_channel}\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2060c433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting munch\n",
      "  Using cached munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Using cached munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
      "Installing collected packages: munch\n",
      "Successfully installed munch-4.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install munch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c50c9f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "from munch import Munch\n",
    "# config = yaml.safe_load(Path(\"tumor.yaml\").read_text())\n",
    "config = Munch.fromDict(yaml.safe_load(Path(\"tumor.yaml\").read_text()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e6a9081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2025-07-15-08-01-51-392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-15 08:01:51 Starting - Starting the training job...\n",
      "2025-07-15 08:02:14 Starting - Preparing the instances for training...\n",
      "2025-07-15 08:02:37 Downloading - Downloading input data...\n",
      "2025-07-15 08:03:08 Downloading - Downloading the training image...\n",
      "2025-07-15 08:03:38 Training - Training image download completed. Training in progress..bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2025-07-15 08:03:59,537 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2025-07-15 08:03:59,552 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2025-07-15 08:03:59,561 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2025-07-15 08:03:59,562 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2025-07-15 08:04:01,127 sagemaker-training-toolkit INFO     Installing module.\n",
      "Processing /opt/ml/code\n",
      "Installing build dependencies: started\n",
      "Installing build dependencies: finished with status 'done'\n",
      "Getting requirements to build wheel: started\n",
      "Getting requirements to build wheel: finished with status 'done'\n",
      "Preparing metadata (pyproject.toml): started\n",
      "Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting monai (from -r requirements.txt (line 1))\n",
      "Downloading monai-1.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (2.6.0+cpu)\n",
      "Collecting pytorch-ignite (from -r requirements.txt (line 3))\n",
      "Downloading pytorch_ignite-0.5.2-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (2.3.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (6.0.2)\n",
      "Collecting munch (from -r requirements.txt (line 6))\n",
      "Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: imageio in /usr/local/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (2.37.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (4.67.1)\n",
      "Collecting nibabel (from -r requirements.txt (line 9))\n",
      "Downloading nibabel-5.3.2-py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting protobuf<=3.20.3 (from -r requirements.txt (line 10))\n",
      "Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
      "Collecting scikit-image (from -r requirements.txt (line 11))\n",
      "Downloading scikit_image-0.25.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/site-packages (from -r requirements.txt (line 12)) (3.10.3)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.12/site-packages (from -r requirements.txt (line 13)) (0.1.7)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/site-packages (from -r requirements.txt (line 14)) (4.11.0.86)\n",
      "Requirement already satisfied: sagemaker in /usr/local/lib/python3.12/site-packages (from -r requirements.txt (line 15)) (2.247.1)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.12/site-packages (from -r requirements.txt (line 16)) (1.38.41)\n",
      "Requirement already satisfied: numpy<3.0,>=1.24 in /usr/local/lib/python3.12/site-packages (from monai->-r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (4.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (2025.5.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/site-packages (from sympy==1.13.1->torch->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/site-packages (from pytorch-ignite->-r requirements.txt (line 3)) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 4)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 4)) (2025.2)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.12/site-packages (from imageio->-r requirements.txt (line 7)) (11.2.1)\n",
      "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.12/site-packages (from scikit-image->-r requirements.txt (line 11)) (1.16.0)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image->-r requirements.txt (line 11))\n",
      "Downloading tifffile-2025.6.11-py3-none-any.whl.metadata (32 kB)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image->-r requirements.txt (line 11))\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 12)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 12)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 12)) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 12)) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 12)) (3.2.3)\n",
      "Requirement already satisfied: traitlets in /usr/local/lib/python3.12/site-packages (from matplotlib-inline->-r requirements.txt (line 13)) (5.14.3)\n",
      "Requirement already satisfied: attrs<26,>=24 in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (25.3.0)\n",
      "Requirement already satisfied: cloudpickle>=2.2.1 in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (3.1.1)\n",
      "Requirement already satisfied: docker in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (7.1.0)\n",
      "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (0.115.13)\n",
      "Requirement already satisfied: google-pasta in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (0.2.0)\n",
      "Requirement already satisfied: graphene<4,>=3 in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (3.4.3)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (6.11.0)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (4.24.0)\n",
      "Requirement already satisfied: omegaconf<3,>=2.2 in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (2.3.0)\n",
      "Requirement already satisfied: pathos in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (0.3.4)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (4.3.8)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (7.0.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (2.32.4)\n",
      "Requirement already satisfied: sagemaker-core<2.0.0,>=1.0.17 in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (1.0.40)\n",
      "Requirement already satisfied: schema in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (0.7.7)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (1.0.1)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (2.5.0)\n",
      "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (0.34.3)\n",
      "Requirement already satisfied: botocore<1.39.0,>=1.38.41 in /usr/local/lib/python3.12/site-packages (from boto3->-r requirements.txt (line 16)) (1.38.41)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.12/site-packages (from boto3->-r requirements.txt (line 16)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.12/site-packages (from boto3->-r requirements.txt (line 16)) (0.13.0)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.12/site-packages (from graphene<4,>=3->sagemaker->-r requirements.txt (line 15)) (3.2.6)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/local/lib/python3.12/site-packages (from graphene<4,>=3->sagemaker->-r requirements.txt (line 15)) (3.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.12/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker->-r requirements.txt (line 15)) (3.23.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/site-packages (from omegaconf<3,>=2.2->sagemaker->-r requirements.txt (line 15)) (4.9.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 4)) (1.17.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker->-r requirements.txt (line 15)) (2.11.7)\n",
      "Requirement already satisfied: rich<15.0.0,>=14.0.0 in /usr/local/lib/python3.12/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker->-r requirements.txt (line 15)) (14.0.0)\n",
      "Requirement already satisfied: mock<5.0,>4.0 in /usr/local/lib/python3.12/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker->-r requirements.txt (line 15)) (4.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/site-packages (from jsonschema->sagemaker->-r requirements.txt (line 15)) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/site-packages (from jsonschema->sagemaker->-r requirements.txt (line 15)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/site-packages (from jsonschema->sagemaker->-r requirements.txt (line 15)) (0.25.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker->-r requirements.txt (line 15)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker->-r requirements.txt (line 15)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker->-r requirements.txt (line 15)) (0.4.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/site-packages (from rich<15.0.0,>=14.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker->-r requirements.txt (line 15)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/site-packages (from rich<15.0.0,>=14.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker->-r requirements.txt (line 15)) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=14.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker->-r requirements.txt (line 15)) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests->sagemaker->-r requirements.txt (line 15)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests->sagemaker->-r requirements.txt (line 15)) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests->sagemaker->-r requirements.txt (line 15)) (2025.6.15)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.12/site-packages (from fastapi->sagemaker->-r requirements.txt (line 15)) (0.46.2)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/site-packages (from starlette<0.47.0,>=0.40.0->fastapi->sagemaker->-r requirements.txt (line 15)) (4.9.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi->sagemaker->-r requirements.txt (line 15)) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/site-packages (from jinja2->torch->-r requirements.txt (line 2)) (3.0.2)\n",
      "Requirement already satisfied: ppft>=1.7.7 in /usr/local/lib/python3.12/site-packages (from pathos->sagemaker->-r requirements.txt (line 15)) (1.7.7)\n",
      "Requirement already satisfied: dill>=0.4.0 in /usr/local/lib/python3.12/site-packages (from pathos->sagemaker->-r requirements.txt (line 15)) (0.4.0)\n",
      "Requirement already satisfied: pox>=0.3.6 in /usr/local/lib/python3.12/site-packages (from pathos->sagemaker->-r requirements.txt (line 15)) (0.3.6)\n",
      "Requirement already satisfied: multiprocess>=0.70.18 in /usr/local/lib/python3.12/site-packages (from pathos->sagemaker->-r requirements.txt (line 15)) (0.70.18)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/site-packages (from uvicorn->sagemaker->-r requirements.txt (line 15)) (8.2.1)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/site-packages (from uvicorn->sagemaker->-r requirements.txt (line 15)) (0.16.0)\n",
      "Downloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "Downloading monai-1.5.0-py3-none-any.whl (2.7 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.7/2.7 MB 98.1 MB/s eta 0:00:00\n",
      "Downloading pytorch_ignite-0.5.2-py3-none-any.whl (343 kB)\n",
      "Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
      "Downloading nibabel-5.3.2-py3-none-any.whl (3.3 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.3/3.3 MB 153.2 MB/s eta 0:00:00\n",
      "Downloading scikit_image-0.25.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.0/15.0 MB 200.5 MB/s eta 0:00:00\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading tifffile-2025.6.11-py3-none-any.whl (230 kB)\n",
      "Building wheels for collected packages: prostate158\n",
      "Building wheel for prostate158 (pyproject.toml): started\n",
      "Building wheel for prostate158 (pyproject.toml): finished with status 'done'\n",
      "Created wheel for prostate158: filename=prostate158-0.0.0-py3-none-any.whl size=35352 sha256=60ef008d94c0beffd70c82944e16f99b29c0087bedeb506f118e2c223ae3fb27\n",
      "Stored in directory: /tmp/pip-ephem-wheel-cache-ent6ji2h/wheels/6e/f2/4a/da972bde801c4b516b25757b9156b5d85fe41e8b907f1e3a0e\n",
      "Successfully built prostate158\n",
      "Installing collected packages: prostate158, tifffile, protobuf, nibabel, munch, lazy-loader, scikit-image, pytorch-ignite, monai\n",
      "Attempting uninstall: protobuf\n",
      "Found existing installation: protobuf 5.29.5\n",
      "Uninstalling protobuf-5.29.5:\n",
      "Successfully uninstalled protobuf-5.29.5\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker-training 5.0.0 requires protobuf>=5.28.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "Successfully installed lazy-loader-0.4 monai-1.5.0 munch-4.0.0 nibabel-5.3.2 prostate158-0.0.0 protobuf-3.20.3 pytorch-ignite-0.5.2 scikit-image-0.25.2 tifffile-2025.6.11\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "2025-07-15 08:04:08,805 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2025-07-15 08:04:08,805 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2025-07-15 08:04:08,842 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2025-07-15 08:04:08,867 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2025-07-15 08:04:08,891 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2025-07-15 08:04:08,900 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"model\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": false,\n",
      "    \"job_name\": \"pytorch-training-2025-07-15-08-01-51-392\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-southeast-1-345594598345/pytorch-training-2025-07-15-08-01-51-392/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_gpt\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"topology\": null\n",
      "    },\n",
      "    \"user_entry_point\": \"train_gpt.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={}\n",
      "SM_USER_ENTRY_POINT=train_gpt.py\n",
      "SM_FRAMEWORK_PARAMS={}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}],\"network_interface_name\":\"eth0\",\"topology\":null}\n",
      "SM_INPUT_DATA_CONFIG={\"model\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"model\",\"test\",\"train\"]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.g4dn.2xlarge\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=train_gpt\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=8\n",
      "SM_NUM_GPUS=1\n",
      "SM_NUM_NEURONS=0\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-ap-southeast-1-345594598345/pytorch-training-2025-07-15-08-01-51-392/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"model\":\"/opt/ml/input/data/model\",\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":false,\"job_name\":\"pytorch-training-2025-07-15-08-01-51-392\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-southeast-1-345594598345/pytorch-training-2025-07-15-08-01-51-392/source/sourcedir.tar.gz\",\"module_name\":\"train_gpt\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}],\"network_interface_name\":\"eth0\",\"topology\":null},\"user_entry_point\":\"train_gpt.py\"}\n",
      "SM_USER_ARGS=[]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_MODEL=/opt/ml/input/data/model\n",
      "SM_CHANNEL_TEST=/opt/ml/input/data/test\n",
      "SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "PYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python312.zip:/usr/local/lib/python3.12:/usr/local/lib/python3.12/lib-dynload:/usr/local/lib/python3.12/site-packages\n",
      "Invoking script with the following command:\n",
      "/usr/local/bin/python -m train_gpt\n",
      "2025-07-15 08:04:08,900 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\n",
      "2025-07-15 08:04:08,900 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\n",
      "2025-07-15 08:04:13,547 - INFO - \n",
      "        Running supervised segmentation training on single GPU\n",
      "        Run ID:     tumor_1\n",
      "        Debug:      False\n",
      "        Out dir:    /opt/ml/output/data\n",
      "        Model dir:  /opt/ml/model\n",
      "        Log dir:    tumor_1/logs\n",
      "        Images:     ['t2', 'adc', 'dwi']\n",
      "        Labels:     ['adc_tumor_reader1']\n",
      "        Data dir:   /opt/ml/input/data/train\n",
      "[get_model] model.type = 'unet'\n",
      "No previous checkpoint found. Starting from scratch.\n",
      "/usr/local/lib/python3.12/site-packages/monai/engines/trainer.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler() if self.amp else None\n",
      "/usr/local/lib/python3.12/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "2025-07-15 08:04:13,877 - INFO - Engine run resuming from iteration 0, epoch 0 until 5 epochs\n",
      "/usr/local/lib/python3.12/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "2025-07-15 08:04:29,654 - INFO - Epoch: 1/5, Iter: 1/119 -- train_loss: 1.4754\n",
      "[1/119]   1%|           [00:00<?]\n",
      "Epoch [1/5]: [1/119]   1%|           [00:00<?]\n",
      "Epoch [1/5]: [1/119]   1%|          , loss=1.48 [00:00<?]\n",
      "2025-07-15 08:04:29,658 - INFO - Current learning rate: 4.00751801062372e-05\n",
      "2025-07-15 08:04:30,902 - INFO - Epoch: 1/5, Iter: 2/119 -- train_loss: 1.5041\n",
      "Epoch [1/5]: [1/119]   1%|          , loss=1.48 [00:01<?]\n",
      "Epoch [1/5]: [1/119]   1%|          , loss=1.5 [00:01<?]\n",
      "Epoch [1/5]: [2/119]   2%|▏         , loss=1.5 [00:01<02:25]\n",
      "2025-07-15 08:04:30,904 - INFO - Current learning rate: 4.0300696874747094e-05\n",
      "2025-07-15 08:04:33,919 - INFO - Epoch: 1/5, Iter: 3/119 -- train_loss: 1.4878\n",
      "Epoch [1/5]: [2/119]   2%|▏         , loss=1.5 [00:04<02:25]\n",
      "Epoch [1/5]: [2/119]   2%|▏         , loss=1.49 [00:04<02:25]\n",
      "Epoch [1/5]: [3/119]   3%|▎         , loss=1.49 [00:04<04:25]\n",
      "2025-07-15 08:04:33,920 - INFO - Current learning rate: 4.067647966230208e-05\n",
      "2025-07-15 08:04:35,436 - INFO - Epoch: 1/5, Iter: 4/119 -- train_loss: 1.4864\n",
      "Epoch [1/5]: [3/119]   3%|▎         , loss=1.49 [00:05<04:25]\n",
      "Epoch [1/5]: [3/119]   3%|▎         , loss=1.49 [00:05<04:25]\n",
      "Epoch [1/5]: [4/119]   3%|▎         , loss=1.49 [00:05<03:42]\n",
      "2025-07-15 08:04:35,438 - INFO - Current learning rate: 4.120241075477769e-05\n",
      "2025-07-15 08:04:37,550 - INFO - Epoch: 1/5, Iter: 5/119 -- train_loss: 1.4783\n",
      "Epoch [1/5]: [4/119]   3%|▎         , loss=1.49 [00:07<03:42]\n",
      "Epoch [1/5]: [4/119]   3%|▎         , loss=1.48 [00:07<03:42]\n",
      "Epoch [1/5]: [5/119]   4%|▍         , loss=1.48 [00:07<03:48]\n",
      "2025-07-15 08:04:37,552 - INFO - Current learning rate: 4.18783254040265e-05\n",
      "2025-07-15 08:04:47,032 - INFO - Epoch: 1/5, Iter: 6/119 -- train_loss: 1.4903\n",
      "Epoch [1/5]: [5/119]   4%|▍         , loss=1.48 [00:17<03:48]\n",
      "Epoch [1/5]: [5/119]   4%|▍         , loss=1.49 [00:17<03:48]\n",
      "Epoch [1/5]: [6/119]   5%|▌         , loss=1.49 [00:17<08:51]\n",
      "2025-07-15 08:04:47,033 - INFO - Current learning rate: 4.2704011879485205e-05\n",
      "2025-07-15 08:04:48,360 - INFO - Epoch: 1/5, Iter: 7/119 -- train_loss: 1.3939\n",
      "Epoch [1/5]: [6/119]   5%|▌         , loss=1.49 [00:18<08:51]\n",
      "Epoch [1/5]: [6/119]   5%|▌         , loss=1.39 [00:18<08:51]\n",
      "Epoch [1/5]: [7/119]   6%|▌         , loss=1.39 [00:18<06:38]\n",
      "2025-07-15 08:04:48,361 - INFO - Current learning rate: 4.367921153450023e-05\n",
      "2025-07-15 08:04:49,937 - INFO - Epoch: 1/5, Iter: 8/119 -- train_loss: 1.5025\n",
      "Epoch [1/5]: [7/119]   6%|▌         , loss=1.39 [00:20<06:38]\n",
      "Epoch [1/5]: [7/119]   6%|▌         , loss=1.5 [00:20<06:38]\n",
      "Epoch [1/5]: [8/119]   7%|▋         , loss=1.5 [00:20<05:22]\n",
      "2025-07-15 08:04:49,938 - INFO - Current learning rate: 4.4803618887347563e-05\n",
      "2025-07-15 08:04:51,830 - INFO - Epoch: 1/5, Iter: 9/119 -- train_loss: 1.4877\n",
      "Epoch [1/5]: [8/119]   7%|▋         , loss=1.5 [00:22<05:22]\n",
      "Epoch [1/5]: [8/119]   7%|▋         , loss=1.49 [00:22<05:22]\n",
      "Epoch [1/5]: [9/119]   8%|▊         , loss=1.49 [00:22<04:44]\n",
      "2025-07-15 08:04:51,832 - INFO - Current learning rate: 4.607688171692629e-05\n",
      "2025-07-15 08:04:57,330 - INFO - Epoch: 1/5, Iter: 10/119 -- train_loss: 1.4888\n",
      "Epoch [1/5]: [9/119]   8%|▊         , loss=1.49 [00:27<04:44]\n",
      "Epoch [1/5]: [9/119]   8%|▊         , loss=1.49 [00:27<04:44]\n",
      "Epoch [1/5]: [10/119]   8%|▊         , loss=1.49 [00:27<06:21]\n",
      "2025-07-15 08:04:57,331 - INFO - Current learning rate: 4.749860117309176e-05\n",
      "2025-07-15 08:05:01,818 - INFO - Epoch: 1/5, Iter: 11/119 -- train_loss: 1.4555\n",
      "Epoch [1/5]: [10/119]   8%|▊         , loss=1.49 [00:32<06:21]\n",
      "Epoch [1/5]: [10/119]   8%|▊         , loss=1.46 [00:32<06:21]\n",
      "Epoch [1/5]: [11/119]   9%|▉         , loss=1.46 [00:32<06:50]\n",
      "2025-07-15 08:05:01,819 - INFO - Current learning rate: 4.906833190159452e-05\n",
      "2025-07-15 08:05:03,327 - INFO - Epoch: 1/5, Iter: 12/119 -- train_loss: 1.4817\n",
      "Epoch [1/5]: [11/119]   9%|▉         , loss=1.46 [00:33<06:50]\n",
      "Epoch [1/5]: [11/119]   9%|▉         , loss=1.48 [00:33<06:50]\n",
      "Epoch [1/5]: [12/119]  10%|█         , loss=1.48 [00:33<05:31]\n",
      "2025-07-15 08:05:03,330 - INFO - Current learning rate: 5.078558218358934e-05\n",
      "2025-07-15 08:05:04,928 - INFO - Epoch: 1/5, Iter: 13/119 -- train_loss: 1.4919\n",
      "Epoch [1/5]: [12/119]  10%|█         , loss=1.48 [00:35<05:31]\n",
      "Epoch [1/5]: [12/119]  10%|█         , loss=1.49 [00:35<05:31]\n",
      "Epoch [1/5]: [13/119]  11%|█         , loss=1.49 [00:35<04:40]\n",
      "2025-07-15 08:05:04,929 - INFO - Current learning rate: 5.2649814089665244e-05\n",
      "2025-07-15 08:05:09,782 - INFO - Epoch: 1/5, Iter: 14/119 -- train_loss: 1.4610\n",
      "Epoch [1/5]: [13/119]  11%|█         , loss=1.49 [00:40<04:40]\n",
      "Epoch [1/5]: [13/119]  11%|█         , loss=1.46 [00:40<04:40]\n",
      "Epoch [1/5]: [14/119]  12%|█▏        , loss=1.46 [00:40<05:47]\n",
      "2025-07-15 08:05:09,783 - INFO - Current learning rate: 5.466044364835221e-05\n",
      "2025-07-15 08:05:12,584 - INFO - Epoch: 1/5, Iter: 15/119 -- train_loss: 1.4433\n",
      "Epoch [1/5]: [14/119]  12%|█▏        , loss=1.46 [00:42<05:47]\n",
      "Epoch [1/5]: [14/119]  12%|█▏        , loss=1.44 [00:42<05:47]\n",
      "Epoch [1/5]: [15/119]  13%|█▎        , loss=1.44 [00:42<05:28]\n",
      "2025-07-15 08:05:12,587 - INFO - Current learning rate: 5.6816841029051204e-05\n",
      "2025-07-15 08:05:14,046 - INFO - Epoch: 1/5, Iter: 16/119 -- train_loss: 1.4803\n",
      "Epoch [1/5]: [15/119]  13%|█▎        , loss=1.44 [00:44<05:28]\n",
      "Epoch [1/5]: [15/119]  13%|█▎        , loss=1.48 [00:44<05:28]\n",
      "Epoch [1/5]: [16/119]  13%|█▎        , loss=1.48 [00:44<04:32]\n",
      "2025-07-15 08:05:14,047 - INFO - Current learning rate: 5.9118330739328e-05\n",
      "2025-07-15 08:05:15,628 - INFO - Epoch: 1/5, Iter: 17/119 -- train_loss: 1.4704\n",
      "Epoch [1/5]: [16/119]  13%|█▎        , loss=1.48 [00:45<04:32]\n",
      "Epoch [1/5]: [16/119]  13%|█▎        , loss=1.47 [00:45<04:32]\n",
      "Epoch [1/5]: [17/119]  14%|█▍        , loss=1.47 [00:45<03:57]\n",
      "2025-07-15 08:05:15,629 - INFO - Current learning rate: 6.156419183651223e-05\n",
      "2025-07-15 08:05:20,617 - INFO - Epoch: 1/5, Iter: 18/119 -- train_loss: 1.4423\n",
      "Epoch [1/5]: [17/119]  14%|█▍        , loss=1.47 [00:50<03:57]\n",
      "Epoch [1/5]: [17/119]  14%|█▍        , loss=1.44 [00:50<03:57]\n",
      "Epoch [1/5]: [18/119]  15%|█▌        , loss=1.44 [00:50<05:15]\n",
      "2025-07-15 08:05:20,618 - INFO - Current learning rate: 6.415365815353239e-05\n",
      "2025-07-15 08:05:22,598 - INFO - Epoch: 1/5, Iter: 19/119 -- train_loss: 1.4082\n",
      "Epoch [1/5]: [18/119]  15%|█▌        , loss=1.44 [00:52<05:15]\n",
      "Epoch [1/5]: [18/119]  15%|█▌        , loss=1.41 [00:52<05:15]\n",
      "Epoch [1/5]: [19/119]  16%|█▌        , loss=1.41 [00:52<04:38]\n",
      "2025-07-15 08:05:22,599 - INFO - Current learning rate: 6.688591853891868e-05\n",
      "2025-07-15 08:05:23,924 - INFO - Epoch: 1/5, Iter: 20/119 -- train_loss: 1.4558\n",
      "Epoch [1/5]: [19/119]  16%|█▌        , loss=1.41 [00:54<04:38]\n",
      "Epoch [1/5]: [19/119]  16%|█▌        , loss=1.46 [00:54<04:38]\n",
      "Epoch [1/5]: [20/119]  17%|█▋        , loss=1.46 [00:54<03:52]\n",
      "2025-07-15 08:05:23,925 - INFO - Current learning rate: 6.976011711089562e-05\n",
      "2025-07-15 08:05:29,787 - INFO - Epoch: 1/5, Iter: 21/119 -- train_loss: 1.4583\n",
      "Epoch [1/5]: [20/119]  17%|█▋        , loss=1.46 [01:00<03:52]\n",
      "Epoch [1/5]: [20/119]  17%|█▋        , loss=1.46 [01:00<03:52]\n",
      "Epoch [1/5]: [21/119]  18%|█▊        , loss=1.46 [01:00<05:33]\n",
      "2025-07-15 08:05:29,791 - INFO - Current learning rate: 7.277535352548836e-05\n",
      "2025-07-15 08:05:31,305 - INFO - Epoch: 1/5, Iter: 22/119 -- train_loss: 1.4544\n",
      "Epoch [1/5]: [21/119]  18%|█▊        , loss=1.46 [01:01<05:33]\n",
      "Epoch [1/5]: [21/119]  18%|█▊        , loss=1.45 [01:01<05:33]\n",
      "Epoch [1/5]: [22/119]  18%|█▊        , loss=1.45 [01:01<04:35]\n",
      "2025-07-15 08:05:31,306 - INFO - Current learning rate: 7.593068325855551e-05\n",
      "2025-07-15 08:05:32,966 - INFO - Epoch: 1/5, Iter: 23/119 -- train_loss: 1.4526\n",
      "Epoch [1/5]: [22/119]  18%|█▊        , loss=1.45 [01:03<04:35]\n",
      "Epoch [1/5]: [22/119]  18%|█▊        , loss=1.45 [01:03<04:35]\n",
      "Epoch [1/5]: [23/119]  19%|█▉        , loss=1.45 [01:03<03:58]\n",
      "2025-07-15 08:05:32,967 - INFO - Current learning rate: 7.922511790166124e-05\n",
      "2025-07-15 08:05:35,144 - INFO - Epoch: 1/5, Iter: 24/119 -- train_loss: 1.4476\n",
      "Epoch [1/5]: [23/119]  19%|█▉        , loss=1.45 [01:05<03:58]\n",
      "Epoch [1/5]: [23/119]  19%|█▉        , loss=1.45 [01:05<03:58]\n",
      "Epoch [1/5]: [24/119]  20%|██        , loss=1.45 [01:05<03:47]\n",
      "2025-07-15 08:05:35,145 - INFO - Current learning rate: 8.265762547169536e-05\n",
      "2025-07-15 08:05:41,644 - INFO - Epoch: 1/5, Iter: 25/119 -- train_loss: 1.4572\n",
      "Epoch [1/5]: [24/119]  20%|██        , loss=1.45 [01:11<03:47]\n",
      "Epoch [1/5]: [24/119]  20%|██        , loss=1.46 [01:11<03:47]\n",
      "Epoch [1/5]: [25/119]  21%|██        , loss=1.46 [01:11<05:40]\n",
      "2025-07-15 08:05:41,645 - INFO - Current learning rate: 8.622713073414218e-05\n",
      "2025-07-15 08:05:43,148 - INFO - Epoch: 1/5, Iter: 26/119 -- train_loss: 1.4277\n",
      "Epoch [1/5]: [25/119]  21%|██        , loss=1.46 [01:13<05:40]\n",
      "Epoch [1/5]: [25/119]  21%|██        , loss=1.43 [01:13<05:40]\n",
      "Epoch [1/5]: [26/119]  22%|██▏       , loss=1.43 [01:13<04:37]\n",
      "2025-07-15 08:05:43,150 - INFO - Current learning rate: 8.993251553989813e-05\n",
      "2025-07-15 08:05:44,533 - INFO - Epoch: 1/5, Iter: 27/119 -- train_loss: 1.4004\n",
      "Epoch [1/5]: [26/119]  22%|██▏       , loss=1.43 [01:14<04:37]\n",
      "Epoch [1/5]: [26/119]  22%|██▏       , loss=1.4 [01:14<04:37]\n",
      "Epoch [1/5]: [27/119]  23%|██▎       , loss=1.4 [01:14<03:50]\n",
      "2025-07-15 08:05:44,535 - INFO - Current learning rate: 9.377261917553267e-05\n",
      "2025-07-15 08:05:45,857 - INFO - Epoch: 1/5, Iter: 28/119 -- train_loss: 1.4222\n",
      "Epoch [1/5]: [27/119]  23%|██▎       , loss=1.4 [01:16<03:50]\n",
      "Epoch [1/5]: [27/119]  23%|██▎       , loss=1.42 [01:16<03:50]\n",
      "Epoch [1/5]: [28/119]  24%|██▎       , loss=1.42 [01:16<03:15]\n",
      "2025-07-15 08:05:45,859 - INFO - Current learning rate: 9.774623872688202e-05\n",
      "2025-07-15 08:05:52,816 - INFO - Epoch: 1/5, Iter: 29/119 -- train_loss: 1.4277\n",
      "Epoch [1/5]: [28/119]  24%|██▎       , loss=1.42 [01:23<03:15]\n",
      "Epoch [1/5]: [28/119]  24%|██▎       , loss=1.43 [01:23<03:15]\n",
      "Epoch [1/5]: [29/119]  24%|██▍       , loss=1.43 [01:23<05:23]\n",
      "2025-07-15 08:05:52,818 - INFO - Current learning rate: 0.00010185212945586274\n",
      "2025-07-15 08:05:56,377 - INFO - Epoch: 1/5, Iter: 30/119 -- train_loss: 1.4141\n",
      "Epoch [1/5]: [29/119]  24%|██▍       , loss=1.43 [01:26<05:23]\n",
      "Epoch [1/5]: [29/119]  24%|██▍       , loss=1.41 [01:26<05:23]\n",
      "Epoch [1/5]: [30/119]  25%|██▌       , loss=1.41 [01:26<05:18]\n",
      "2025-07-15 08:05:56,378 - INFO - Current learning rate: 0.0001060890051903866\n",
      "2025-07-15 08:05:57,736 - INFO - Epoch: 1/5, Iter: 31/119 -- train_loss: 1.4233\n",
      "Epoch [1/5]: [30/119]  25%|██▌       , loss=1.41 [01:28<05:18]\n",
      "Epoch [1/5]: [30/119]  25%|██▌       , loss=1.42 [01:28<05:18]\n",
      "Epoch [1/5]: [31/119]  26%|██▌       , loss=1.42 [01:28<04:16]\n",
      "2025-07-15 08:05:57,737 - INFO - Current learning rate: 0.00011045553872725429\n",
      "2025-07-15 08:05:59,127 - INFO - Epoch: 1/5, Iter: 32/119 -- train_loss: 1.4062\n",
      "Epoch [1/5]: [31/119]  26%|██▌       , loss=1.42 [01:29<04:16]\n",
      "Epoch [1/5]: [31/119]  26%|██▌       , loss=1.41 [01:29<04:16]\n",
      "Epoch [1/5]: [32/119]  27%|██▋       , loss=1.41 [01:29<03:33]\n",
      "2025-07-15 08:05:59,129 - INFO - Current learning rate: 0.00011495036224790252\n",
      "2025-07-15 08:06:05,823 - INFO - Epoch: 1/5, Iter: 33/119 -- train_loss: 1.4094\n",
      "Epoch [1/5]: [32/119]  27%|██▋       , loss=1.41 [01:36<03:33]\n",
      "Epoch [1/5]: [32/119]  27%|██▋       , loss=1.41 [01:36<03:33]\n",
      "Epoch [1/5]: [33/119]  28%|██▊       , loss=1.41 [01:36<05:20]\n",
      "2025-07-15 08:06:05,824 - INFO - Current learning rate: 0.00011957206774687373\n",
      "2025-07-15 08:06:07,327 - INFO - Epoch: 1/5, Iter: 34/119 -- train_loss: 1.3832\n",
      "Epoch [1/5]: [33/119]  28%|██▊       , loss=1.41 [01:37<05:20]\n",
      "Epoch [1/5]: [33/119]  28%|██▊       , loss=1.38 [01:37<05:20]\n",
      "Epoch [1/5]: [34/119]  29%|██▊       , loss=1.38 [01:37<04:20]\n",
      "2025-07-15 08:06:07,329 - INFO - Current learning rate: 0.0001243192074728745\n",
      "2025-07-15 08:06:08,792 - INFO - Epoch: 1/5, Iter: 35/119 -- train_loss: 1.3852\n",
      "Epoch [1/5]: [34/119]  29%|██▊       , loss=1.38 [01:39<04:20]\n",
      "Epoch [1/5]: [34/119]  29%|██▊       , loss=1.39 [01:39<04:20]\n",
      "Epoch [1/5]: [35/119]  29%|██▉       , loss=1.39 [01:39<03:36]\n",
      "2025-07-15 08:06:08,793 - INFO - Current learning rate: 0.00012919029438228423\n",
      "2025-07-15 08:06:10,228 - INFO - Epoch: 1/5, Iter: 36/119 -- train_loss: 1.3677\n",
      "Epoch [1/5]: [35/119]  29%|██▉       , loss=1.39 [01:40<03:36]\n",
      "Epoch [1/5]: [35/119]  29%|██▉       , loss=1.37 [01:40<03:36]\n",
      "Epoch [1/5]: [36/119]  30%|███       , loss=1.37 [01:40<03:05]\n",
      "2025-07-15 08:06:10,231 - INFO - Current learning rate: 0.0001341838026049729\n",
      "2025-07-15 08:06:18,018 - INFO - Epoch: 1/5, Iter: 37/119 -- train_loss: 1.3768\n",
      "Epoch [1/5]: [36/119]  30%|███       , loss=1.37 [01:48<03:05]\n",
      "Epoch [1/5]: [36/119]  30%|███       , loss=1.38 [01:48<03:05]\n",
      "Epoch [1/5]: [37/119]  31%|███       , loss=1.38 [01:48<05:20]\n",
      "2025-07-15 08:06:18,019 - INFO - Current learning rate: 0.00013929816792227867\n",
      "2025-07-15 08:06:19,402 - INFO - Epoch: 1/5, Iter: 38/119 -- train_loss: 1.3223\n",
      "Epoch [1/5]: [37/119]  31%|███       , loss=1.38 [01:49<05:20]\n",
      "Epoch [1/5]: [37/119]  31%|███       , loss=1.32 [01:49<05:20]\n",
      "Epoch [1/5]: [38/119]  32%|███▏      , loss=1.32 [01:49<04:14]\n",
      "2025-07-15 08:06:19,403 - INFO - Current learning rate: 0.00014453178825700225\n",
      "2025-07-15 08:06:21,001 - INFO - Epoch: 1/5, Iter: 39/119 -- train_loss: 1.3759\n",
      "Epoch [1/5]: [38/119]  32%|███▏      , loss=1.32 [01:51<04:14]\n",
      "Epoch [1/5]: [38/119]  32%|███▏      , loss=1.38 [01:51<04:14]\n",
      "Epoch [1/5]: [39/119]  33%|███▎      , loss=1.38 [01:51<03:34]\n",
      "2025-07-15 08:06:21,003 - INFO - Current learning rate: 0.00014988302417525713\n",
      "2025-07-15 08:06:22,623 - INFO - Epoch: 1/5, Iter: 40/119 -- train_loss: 1.3447\n",
      "Epoch [1/5]: [39/119]  33%|███▎      , loss=1.38 [01:52<03:34]\n",
      "Epoch [1/5]: [39/119]  33%|███▎      , loss=1.34 [01:52<03:34]\n",
      "Epoch [1/5]: [40/119]  34%|███▎      , loss=1.34 [01:52<03:06]\n",
      "2025-07-15 08:06:22,624 - INFO - Current learning rate: 0.00015535019940002325\n",
      "2025-07-15 08:06:27,053 - INFO - Epoch: 1/5, Iter: 41/119 -- train_loss: 1.3543\n",
      "Epoch [1/5]: [40/119]  34%|███▎      , loss=1.34 [01:57<03:06]\n",
      "Epoch [1/5]: [40/119]  34%|███▎      , loss=1.35 [01:57<03:06]\n",
      "Epoch [1/5]: [41/119]  34%|███▍      , loss=1.35 [01:57<03:52]\n",
      "2025-07-15 08:06:27,054 - INFO - Current learning rate: 0.00016093160133624112\n",
      "2025-07-15 08:06:28,343 - INFO - Epoch: 1/5, Iter: 42/119 -- train_loss: 1.3031\n",
      "Epoch [1/5]: [41/119]  34%|███▍      , loss=1.35 [01:58<03:52]\n",
      "Epoch [1/5]: [41/119]  34%|███▍      , loss=1.3 [01:58<03:52]\n",
      "Epoch [1/5]: [42/119]  35%|███▌      , loss=1.3 [01:58<03:10]\n",
      "2025-07-15 08:06:28,345 - INFO - Current learning rate: 0.0001666254816072839\n",
      "2025-07-15 08:06:29,660 - INFO - Epoch: 1/5, Iter: 43/119 -- train_loss: 1.3066\n",
      "Epoch [1/5]: [42/119]  35%|███▌      , loss=1.3 [02:00<03:10]\n",
      "Epoch [1/5]: [42/119]  35%|███▌      , loss=1.31 [02:00<03:10]\n",
      "Epoch [1/5]: [43/119]  36%|███▌      , loss=1.31 [02:00<02:41]\n",
      "2025-07-15 08:06:29,662 - INFO - Current learning rate: 0.0001724300566026361\n",
      "2025-07-15 08:06:30,799 - INFO - Epoch: 1/5, Iter: 44/119 -- train_loss: 1.3333\n",
      "Epoch [1/5]: [43/119]  36%|███▌      , loss=1.31 [02:01<02:41]\n",
      "Epoch [1/5]: [43/119]  36%|███▌      , loss=1.33 [02:01<02:41]\n",
      "Epoch [1/5]: [44/119]  37%|███▋      , loss=1.33 [02:01<02:17]\n",
      "2025-07-15 08:06:30,801 - INFO - Current learning rate: 0.0001783435080366109\n",
      "2025-07-15 08:06:36,827 - INFO - Epoch: 1/5, Iter: 45/119 -- train_loss: 1.3331\n",
      "Epoch [1/5]: [44/119]  37%|███▋      , loss=1.33 [02:07<02:17]\n",
      "Epoch [1/5]: [44/119]  37%|███▋      , loss=1.33 [02:07<02:17]\n",
      "Epoch [1/5]: [45/119]  38%|███▊      , loss=1.33 [02:07<03:48]\n",
      "2025-07-15 08:06:36,828 - INFO - Current learning rate: 0.0001843639835179292\n",
      "2025-07-15 08:06:38,290 - INFO - Epoch: 1/5, Iter: 46/119 -- train_loss: 1.2697\n",
      "Epoch [1/5]: [45/119]  38%|███▊      , loss=1.33 [02:08<03:48]\n",
      "Epoch [1/5]: [45/119]  38%|███▊      , loss=1.27 [02:08<03:48]\n",
      "Epoch [1/5]: [46/119]  39%|███▊      , loss=1.27 [02:08<03:09]\n",
      "2025-07-15 08:06:38,293 - INFO - Current learning rate: 0.00019048959712998138\n",
      "2025-07-15 08:06:39,616 - INFO - Epoch: 1/5, Iter: 47/119 -- train_loss: 1.3087\n",
      "Epoch [1/5]: [46/119]  39%|███▊      , loss=1.27 [02:09<03:09]\n",
      "Epoch [1/5]: [46/119]  39%|███▊      , loss=1.31 [02:09<03:09]\n",
      "Epoch [1/5]: [47/119]  39%|███▉      , loss=1.31 [02:09<02:39]\n",
      "2025-07-15 08:06:39,618 - INFO - Current learning rate: 0.00019671843002159193\n",
      "2025-07-15 08:06:40,876 - INFO - Epoch: 1/5, Iter: 48/119 -- train_loss: 1.2383\n",
      "Epoch [1/5]: [47/119]  39%|███▉      , loss=1.31 [02:11<02:39]\n",
      "Epoch [1/5]: [47/119]  39%|███▉      , loss=1.24 [02:11<02:39]\n",
      "Epoch [1/5]: [48/119]  40%|████      , loss=1.24 [02:11<02:17]\n",
      "2025-07-15 08:06:40,877 - INFO - Current learning rate: 0.0002030485310081004\n",
      "2025-07-15 08:06:46,298 - INFO - Epoch: 1/5, Iter: 49/119 -- train_loss: 1.2878\n",
      "Epoch [1/5]: [48/119]  40%|████      , loss=1.24 [02:16<02:17]\n",
      "Epoch [1/5]: [48/119]  40%|████      , loss=1.29 [02:16<02:17]\n",
      "Epoch [1/5]: [49/119]  41%|████      , loss=1.29 [02:16<03:28]\n",
      "2025-07-15 08:06:46,300 - INFO - Current learning rate: 0.00020947791718257057\n",
      "2025-07-15 08:06:47,854 - INFO - Epoch: 1/5, Iter: 50/119 -- train_loss: 1.2664\n",
      "Epoch [1/5]: [49/119]  41%|████      , loss=1.29 [02:18<03:28]\n",
      "Epoch [1/5]: [49/119]  41%|████      , loss=1.27 [02:18<03:28]\n",
      "Epoch [1/5]: [50/119]  42%|████▏     , loss=1.27 [02:18<02:56]\n",
      "2025-07-15 08:06:47,855 - INFO - Current learning rate: 0.00021600457453693745\n",
      "2025-07-15 08:06:49,460 - INFO - Epoch: 1/5, Iter: 51/119 -- train_loss: 1.2748\n",
      "Epoch [1/5]: [50/119]  42%|████▏     , loss=1.27 [02:19<02:56]\n",
      "Epoch [1/5]: [50/119]  42%|████▏     , loss=1.27 [02:19<02:56]\n",
      "Epoch [1/5]: [51/119]  43%|████▎     , loss=1.27 [02:19<02:34]\n",
      "2025-07-15 08:06:49,462 - INFO - Current learning rate: 0.00022262645859289572\n",
      "2025-07-15 08:06:50,773 - INFO - Epoch: 1/5, Iter: 52/119 -- train_loss: 1.2375\n",
      "Epoch [1/5]: [51/119]  43%|████▎     , loss=1.27 [02:21<02:34]\n",
      "Epoch [1/5]: [51/119]  43%|████▎     , loss=1.24 [02:21<02:34]\n",
      "Epoch [1/5]: [52/119]  44%|████▎     , loss=1.24 [02:21<02:12]\n",
      "2025-07-15 08:06:50,775 - INFO - Current learning rate: 0.0002293414950423334\n",
      "2025-07-15 08:06:58,421 - INFO - Epoch: 1/5, Iter: 53/119 -- train_loss: 1.2729\n",
      "Epoch [1/5]: [52/119]  44%|████▎     , loss=1.24 [02:28<02:12]\n",
      "Epoch [1/5]: [52/119]  44%|████▎     , loss=1.27 [02:28<02:12]\n",
      "Epoch [1/5]: [53/119]  45%|████▍     , loss=1.27 [02:28<04:02]\n",
      "2025-07-15 08:06:58,423 - INFO - Current learning rate: 0.0002361475803971106\n",
      "2025-07-15 08:07:01,109 - INFO - Epoch: 1/5, Iter: 54/119 -- train_loss: 1.2489\n",
      "Epoch [1/5]: [53/119]  45%|████▍     , loss=1.27 [02:31<04:02]\n",
      "Epoch [1/5]: [53/119]  45%|████▍     , loss=1.25 [02:31<04:02]\n",
      "Epoch [1/5]: [54/119]  45%|████▌     , loss=1.25 [02:31<03:39]\n",
      "2025-07-15 08:07:01,111 - INFO - Current learning rate: 0.0002430425826479769\n",
      "2025-07-15 08:07:02,477 - INFO - Epoch: 1/5, Iter: 55/119 -- train_loss: 1.2365\n",
      "Epoch [1/5]: [54/119]  45%|████▌     , loss=1.25 [02:32<03:39]\n",
      "Epoch [1/5]: [54/119]  45%|████▌     , loss=1.24 [02:32<03:39]\n",
      "Epoch [1/5]: [55/119]  46%|████▌     , loss=1.24 [02:32<02:57]\n",
      "2025-07-15 08:07:02,479 - INFO - Current learning rate: 0.00025002434193242525\n",
      "2025-07-15 08:07:04,103 - INFO - Epoch: 1/5, Iter: 56/119 -- train_loss: 1.2441\n",
      "Epoch [1/5]: [55/119]  46%|████▌     , loss=1.24 [02:34<02:57]\n",
      "Epoch [1/5]: [55/119]  46%|████▌     , loss=1.24 [02:34<02:57]\n",
      "Epoch [1/5]: [56/119]  47%|████▋     , loss=1.24 [02:34<02:33]\n",
      "2025-07-15 08:07:04,105 - INFO - Current learning rate: 0.0002570906712112695\n",
      "2025-07-15 08:07:10,374 - INFO - Epoch: 1/5, Iter: 57/119 -- train_loss: 1.2232\n",
      "Epoch [1/5]: [56/119]  47%|████▋     , loss=1.24 [02:40<02:33]\n",
      "Epoch [1/5]: [56/119]  47%|████▋     , loss=1.22 [02:40<02:33]\n",
      "Epoch [1/5]: [57/119]  48%|████▊     , loss=1.22 [02:40<03:42]\n",
      "2025-07-15 08:07:10,375 - INFO - Current learning rate: 0.00026423935695373446\n",
      "2025-07-15 08:07:11,674 - INFO - Epoch: 1/5, Iter: 58/119 -- train_loss: 1.2425\n",
      "Epoch [1/5]: [57/119]  48%|████▊     , loss=1.22 [02:42<03:42]\n",
      "Epoch [1/5]: [57/119]  48%|████▊     , loss=1.24 [02:42<03:42]\n",
      "Epoch [1/5]: [58/119]  49%|████▊     , loss=1.24 [02:42<02:56]\n",
      "2025-07-15 08:07:11,675 - INFO - Current learning rate: 0.00027146815983084775\n",
      "2025-07-15 08:07:13,110 - INFO - Epoch: 1/5, Iter: 59/119 -- train_loss: 1.2098\n",
      "Epoch [1/5]: [58/119]  49%|████▊     , loss=1.24 [02:43<02:56]\n",
      "Epoch [1/5]: [58/119]  49%|████▊     , loss=1.21 [02:43<02:56]\n",
      "Epoch [1/5]: [59/119]  50%|████▉     , loss=1.21 [02:43<02:27]\n",
      "2025-07-15 08:07:13,111 - INFO - Current learning rate: 0.0002787748154169076\n",
      "2025-07-15 08:07:14,587 - INFO - Epoch: 1/5, Iter: 60/119 -- train_loss: 1.2269\n",
      "Epoch [1/5]: [59/119]  50%|████▉     , loss=1.21 [02:44<02:27]\n",
      "Epoch [1/5]: [59/119]  50%|████▉     , loss=1.23 [02:44<02:27]\n",
      "Epoch [1/5]: [60/119]  50%|█████     , loss=1.23 [02:44<02:07]\n",
      "2025-07-15 08:07:14,588 - INFO - Current learning rate: 0.00028615703489881766\n",
      "2025-07-15 08:07:19,310 - INFO - Epoch: 1/5, Iter: 61/119 -- train_loss: 1.2000\n",
      "Epoch [1/5]: [60/119]  50%|█████     , loss=1.23 [02:49<02:07]\n",
      "Epoch [1/5]: [60/119]  50%|█████     , loss=1.2 [02:49<02:07]\n",
      "Epoch [1/5]: [61/119]  51%|█████▏    , loss=1.2 [02:49<02:50]\n",
      "2025-07-15 08:07:19,311 - INFO - Current learning rate: 0.00029361250579305735\n",
      "2025-07-15 08:07:25,246 - INFO - Epoch: 1/5, Iter: 62/119 -- train_loss: 1.2003\n",
      "Epoch [1/5]: [61/119]  51%|█████▏    , loss=1.2 [02:55<02:50]\n",
      "Epoch [1/5]: [61/119]  51%|█████▏    , loss=1.2 [02:55<02:50]\n",
      "Epoch [1/5]: [62/119]  52%|█████▏    , loss=1.2 [02:55<03:38]\n",
      "2025-07-15 08:07:25,247 - INFO - Current learning rate: 0.0003011388926700688\n",
      "2025-07-15 08:07:26,805 - INFO - Epoch: 1/5, Iter: 63/119 -- train_loss: 1.2256\n",
      "Epoch [1/5]: [62/119]  52%|█████▏    , loss=1.2 [02:57<03:38]\n",
      "Epoch [1/5]: [62/119]  52%|█████▏    , loss=1.23 [02:57<03:38]\n",
      "Epoch [1/5]: [63/119]  53%|█████▎    , loss=1.23 [02:57<02:56]\n",
      "2025-07-15 08:07:26,806 - INFO - Current learning rate: 0.0003087338378858315\n",
      "2025-07-15 08:07:28,256 - INFO - Epoch: 1/5, Iter: 64/119 -- train_loss: 1.1995\n",
      "Epoch [1/5]: [63/119]  53%|█████▎    , loss=1.23 [02:58<02:56]\n",
      "Epoch [1/5]: [63/119]  53%|█████▎    , loss=1.2 [02:58<02:56]\n",
      "Epoch [1/5]: [64/119]  54%|█████▍    , loss=1.2 [02:58<02:25]\n",
      "2025-07-15 08:07:28,258 - INFO - Current learning rate: 0.00031639496232039417\n",
      "2025-07-15 08:07:29,580 - INFO - Epoch: 1/5, Iter: 65/119 -- train_loss: 1.1954\n",
      "Epoch [1/5]: [64/119]  54%|█████▍    , loss=1.2 [02:59<02:25]\n",
      "Epoch [1/5]: [64/119]  54%|█████▍    , loss=1.2 [02:59<02:25]\n",
      "Epoch [1/5]: [65/119]  55%|█████▍    , loss=1.2 [02:59<02:01]\n",
      "2025-07-15 08:07:29,582 - INFO - Current learning rate: 0.00032411986612313685\n",
      "2025-07-15 08:07:36,505 - INFO - Epoch: 1/5, Iter: 66/119 -- train_loss: 1.2022\n",
      "Epoch [1/5]: [65/119]  55%|█████▍    , loss=1.2 [03:06<02:01]\n",
      "Epoch [1/5]: [65/119]  55%|█████▍    , loss=1.2 [03:06<02:01]\n",
      "Epoch [1/5]: [66/119]  55%|█████▌    , loss=1.2 [03:06<03:13]\n",
      "2025-07-15 08:07:36,506 - INFO - Current learning rate: 0.0003319061294645237\n",
      "2025-07-15 08:07:37,815 - INFO - Epoch: 1/5, Iter: 67/119 -- train_loss: 1.2152\n",
      "Epoch [1/5]: [66/119]  55%|█████▌    , loss=1.2 [03:08<03:13]\n",
      "Epoch [1/5]: [66/119]  55%|█████▌    , loss=1.22 [03:08<03:13]\n",
      "Epoch [1/5]: [67/119]  56%|█████▋    , loss=1.22 [03:08<02:33]\n",
      "2025-07-15 08:07:37,816 - INFO - Current learning rate: 0.00033975131329411646\n",
      "2025-07-15 08:07:39,283 - INFO - Epoch: 1/5, Iter: 68/119 -- train_loss: 1.1819\n",
      "Epoch [1/5]: [67/119]  56%|█████▋    , loss=1.22 [03:09<02:33]\n",
      "Epoch [1/5]: [67/119]  56%|█████▋    , loss=1.18 [03:09<02:33]\n",
      "Epoch [1/5]: [68/119]  57%|█████▋    , loss=1.18 [03:09<02:07]\n",
      "2025-07-15 08:07:39,284 - INFO - Current learning rate: 0.00034765296010460774\n",
      "2025-07-15 08:07:40,896 - INFO - Epoch: 1/5, Iter: 69/119 -- train_loss: 1.1822\n",
      "Epoch [1/5]: [68/119]  57%|█████▋    , loss=1.18 [03:11<02:07]\n",
      "Epoch [1/5]: [68/119]  57%|█████▋    , loss=1.18 [03:11<02:07]\n",
      "Epoch [1/5]: [69/119]  58%|█████▊    , loss=1.18 [03:11<01:51]\n",
      "2025-07-15 08:07:40,897 - INFO - Current learning rate: 0.00035560859470163634\n",
      "2025-07-15 08:07:49,646 - INFO - Epoch: 1/5, Iter: 70/119 -- train_loss: 1.1740\n",
      "Epoch [1/5]: [69/119]  58%|█████▊    , loss=1.18 [03:19<01:51]\n",
      "Epoch [1/5]: [69/119]  58%|█████▊    , loss=1.17 [03:19<01:51]\n",
      "Epoch [1/5]: [70/119]  59%|█████▉    , loss=1.17 [03:19<03:25]\n",
      "2025-07-15 08:07:49,647 - INFO - Current learning rate: 0.0003636157249791435\n",
      "2025-07-15 08:07:50,823 - INFO - Epoch: 1/5, Iter: 71/119 -- train_loss: 1.1671\n",
      "Epoch [1/5]: [70/119]  59%|█████▉    , loss=1.17 [03:21<03:25]\n",
      "Epoch [1/5]: [70/119]  59%|█████▉    , loss=1.17 [03:21<03:25]\n",
      "Epoch [1/5]: [71/119]  60%|█████▉    , loss=1.17 [03:21<02:37]\n",
      "2025-07-15 08:07:50,824 - INFO - Current learning rate: 0.0003716718427000253\n",
      "2025-07-15 08:07:52,508 - INFO - Epoch: 1/5, Iter: 72/119 -- train_loss: 1.1788\n",
      "Epoch [1/5]: [71/119]  60%|█████▉    , loss=1.17 [03:22<02:37]\n",
      "Epoch [1/5]: [71/119]  60%|█████▉    , loss=1.18 [03:22<02:37]\n",
      "Epoch [1/5]: [72/119]  61%|██████    , loss=1.18 [03:22<02:11]\n",
      "2025-07-15 08:07:52,509 - INFO - Current learning rate: 0.0003797744242818392\n",
      "2025-07-15 08:07:54,012 - INFO - Epoch: 1/5, Iter: 73/119 -- train_loss: 1.1722\n",
      "Epoch [1/5]: [72/119]  61%|██████    , loss=1.18 [03:24<02:11]\n",
      "Epoch [1/5]: [72/119]  61%|██████    , loss=1.17 [03:24<02:11]\n",
      "Epoch [1/5]: [73/119]  61%|██████▏   , loss=1.17 [03:24<01:51]\n",
      "2025-07-15 08:07:54,014 - INFO - Current learning rate: 0.0003879209315873186\n",
      "2025-07-15 08:08:00,175 - INFO - Epoch: 1/5, Iter: 74/119 -- train_loss: 1.1619\n",
      "Epoch [1/5]: [73/119]  61%|██████▏   , loss=1.17 [03:30<01:51]\n",
      "Epoch [1/5]: [73/119]  61%|██████▏   , loss=1.16 [03:30<01:51]#015Epoch [1/5]: [74/119]  62%|██████▏   , loss=1.16 [03:30<02:39]\n",
      "2025-07-15 08:08:00,177 - INFO - Current learning rate: 0.00039610881271944355\n",
      "2025-07-15 08:08:01,960 - INFO - Epoch: 1/5, Iter: 75/119 -- train_loss: 1.1667\n",
      "Epoch [1/5]: [74/119]  62%|██████▏   , loss=1.16 [03:32<02:39]\n",
      "Epoch [1/5]: [74/119]  62%|██████▏   , loss=1.17 [03:32<02:39]\n",
      "Epoch [1/5]: [75/119]  63%|██████▎   , loss=1.17 [03:32<02:12]\n",
      "2025-07-15 08:08:01,962 - INFO - Current learning rate: 0.00040433550282082515\n",
      "2025-07-15 08:08:03,193 - INFO - Epoch: 1/5, Iter: 76/119 -- train_loss: 1.1567\n",
      "Epoch [1/5]: [75/119]  63%|██████▎   , loss=1.17 [03:33<02:12]\n",
      "Epoch [1/5]: [75/119]  63%|██████▎   , loss=1.16 [03:33<02:12]\n",
      "Epoch [1/5]: [76/119]  64%|██████▍   , loss=1.16 [03:33<01:46]\n",
      "2025-07-15 08:08:03,194 - INFO - Current learning rate: 0.0004125984248771475\n",
      "2025-07-15 08:08:04,636 - INFO - Epoch: 1/5, Iter: 77/119 -- train_loss: 1.1292\n",
      "Epoch [1/5]: [76/119]  64%|██████▍   , loss=1.16 [03:34<01:46]\n",
      "Epoch [1/5]: [76/119]  64%|██████▍   , loss=1.13 [03:34<01:46]\n",
      "Epoch [1/5]: [77/119]  65%|██████▍   , loss=1.13 [03:34<01:31]\n",
      "2025-07-15 08:08:04,637 - INFO - Current learning rate: 0.000420894990524419\n",
      "2025-07-15 08:08:08,873 - INFO - Epoch: 1/5, Iter: 78/119 -- train_loss: 1.1318\n",
      "Epoch [1/5]: [77/119]  65%|██████▍   , loss=1.13 [03:39<01:31]\n",
      "Epoch [1/5]: [77/119]  65%|██████▍   , loss=1.13 [03:39<01:31]\n",
      "Epoch [1/5]: [78/119]  66%|██████▌   , loss=1.13 [03:39<01:54]\n",
      "2025-07-15 08:08:08,876 - INFO - Current learning rate: 0.000429222600859778\n",
      "2025-07-15 08:08:10,476 - INFO - Epoch: 1/5, Iter: 79/119 -- train_loss: 1.1564\n",
      "Epoch [1/5]: [78/119]  66%|██████▌   , loss=1.13 [03:40<01:54]\n",
      "Epoch [1/5]: [78/119]  66%|██████▌   , loss=1.16 [03:40<01:54]\n",
      "Epoch [1/5]: [79/119]  66%|██████▋   , loss=1.16 [03:40<01:37]\n",
      "2025-07-15 08:08:10,477 - INFO - Current learning rate: 0.0004375786472556008\n",
      "2025-07-15 08:08:11,944 - INFO - Epoch: 1/5, Iter: 80/119 -- train_loss: 1.1395\n",
      "Epoch [1/5]: [79/119]  66%|██████▋   , loss=1.16 [03:42<01:37]\n",
      "Epoch [1/5]: [79/119]  66%|██████▋   , loss=1.14 [03:42<01:37]\n",
      "Epoch [1/5]: [80/119]  67%|██████▋   , loss=1.14 [03:42<01:23]\n",
      "2025-07-15 08:08:11,945 - INFO - Current learning rate: 0.00044596051217665495\n",
      "2025-07-15 08:08:13,212 - INFO - Epoch: 1/5, Iter: 81/119 -- train_loss: 1.1388\n",
      "Epoch [1/5]: [80/119]  67%|██████▋   , loss=1.14 [03:43<01:23]\n",
      "Epoch [1/5]: [80/119]  67%|██████▋   , loss=1.14 [03:43<01:23]\n",
      "Epoch [1/5]: [81/119]  68%|██████▊   , loss=1.14 [03:43<01:11]\n",
      "2025-07-15 08:08:13,213 - INFO - Current learning rate: 0.00045436557000004356\n",
      "2025-07-15 08:08:21,312 - INFO - Epoch: 1/5, Iter: 82/119 -- train_loss: 1.0560\n",
      "Epoch [1/5]: [81/119]  68%|██████▊   , loss=1.14 [03:51<01:11]\n",
      "Epoch [1/5]: [81/119]  68%|██████▊   , loss=1.06 [03:51<01:11]\n",
      "Epoch [1/5]: [82/119]  69%|██████▉   , loss=1.06 [03:51<02:18]\n",
      "2025-07-15 08:08:21,313 - INFO - Current learning rate: 0.0004627911878376832\n",
      "2025-07-15 08:08:22,730 - INFO - Epoch: 1/5, Iter: 83/119 -- train_loss: 1.1394\n",
      "Epoch [1/5]: [82/119]  69%|██████▉   , loss=1.06 [03:53<02:18]\n",
      "Epoch [1/5]: [82/119]  69%|██████▉   , loss=1.14 [03:53<02:18]\n",
      "Epoch [1/5]: [83/119]  70%|██████▉   , loss=1.14 [03:53<01:49]\n",
      "2025-07-15 08:08:22,732 - INFO - Current learning rate: 0.00047123472636105643\n",
      "2025-07-15 08:08:24,622 - INFO - Epoch: 1/5, Iter: 84/119 -- train_loss: 1.1459\n",
      "Epoch [1/5]: [83/119]  70%|██████▉   , loss=1.14 [03:54<01:49]\n",
      "Epoch [1/5]: [83/119]  70%|██████▉   , loss=1.15 [03:54<01:49]\n",
      "Epoch [1/5]: [84/119]  71%|███████   , loss=1.15 [03:54<01:34]\n",
      "2025-07-15 08:08:24,624 - INFO - Current learning rate: 0.00047969354062798325\n",
      "2025-07-15 08:08:26,289 - INFO - Epoch: 1/5, Iter: 85/119 -- train_loss: 1.1500\n",
      "Epoch [1/5]: [84/119]  71%|███████   , loss=1.15 [03:56<01:34]\n",
      "Epoch [1/5]: [84/119]  71%|███████   , loss=1.15 [03:56<01:34]\n",
      "Epoch [1/5]: [85/119]  71%|███████▏  , loss=1.15 [03:56<01:21]\n",
      "2025-07-15 08:08:26,290 - INFO - Current learning rate: 0.0004881649809111501\n",
      "2025-07-15 08:08:31,735 - INFO - Epoch: 1/5, Iter: 86/119 -- train_loss: 1.1294\n",
      "Epoch [1/5]: [85/119]  71%|███████▏  , loss=1.15 [04:02<01:21]\n",
      "Epoch [1/5]: [85/119]  71%|███████▏  , loss=1.13 [04:02<01:21]\n",
      "Epoch [1/5]: [86/119]  72%|███████▏  , loss=1.13 [04:02<01:49]\n",
      "2025-07-15 08:08:31,736 - INFO - Current learning rate: 0.0004966463935281385\n",
      "2025-07-15 08:08:33,164 - INFO - Epoch: 1/5, Iter: 87/119 -- train_loss: 1.1348\n",
      "Epoch [1/5]: [86/119]  72%|███████▏  , loss=1.13 [04:03<01:49]\n",
      "Epoch [1/5]: [86/119]  72%|███████▏  , loss=1.13 [04:03<01:49]\n",
      "Epoch [1/5]: [87/119]  73%|███████▎  , loss=1.13 [04:03<01:27]\n",
      "2025-07-15 08:08:33,165 - INFO - Current learning rate: 0.0005051351216726919\n",
      "2025-07-15 08:08:34,697 - INFO - Epoch: 1/5, Iter: 88/119 -- train_loss: 1.1342\n",
      "Epoch [1/5]: [87/119]  73%|███████▎  , loss=1.13 [04:05<01:27]\n",
      "Epoch [1/5]: [87/119]  73%|███████▎  , loss=1.13 [04:05<01:27]\n",
      "Epoch [1/5]: [88/119]  74%|███████▍  , loss=1.13 [04:05<01:13]\n",
      "2025-07-15 08:08:34,699 - INFO - Current learning rate: 0.0005136285062469611\n",
      "2025-07-15 08:08:40,541 - INFO - Epoch: 1/5, Iter: 89/119 -- train_loss: 1.1316\n",
      "Epoch [1/5]: [88/119]  74%|███████▍  , loss=1.13 [04:10<01:13]\n",
      "Epoch [1/5]: [88/119]  74%|███████▍  , loss=1.13 [04:10<01:13]\n",
      "Epoch [1/5]: [89/119]  75%|███████▍  , loss=1.13 [04:10<01:42]\n",
      "2025-07-15 08:08:40,542 - INFO - Current learning rate: 0.000522123886694469\n",
      "2025-07-15 08:08:41,863 - INFO - Epoch: 1/5, Iter: 90/119 -- train_loss: 1.1393\n",
      "Epoch [1/5]: [89/119]  75%|███████▍  , loss=1.13 [04:12<01:42]\n",
      "Epoch [1/5]: [89/119]  75%|███████▍  , loss=1.14 [04:12<01:42]\n",
      "Epoch [1/5]: [90/119]  76%|███████▌  , loss=1.14 [04:12<01:20]\n",
      "2025-07-15 08:08:41,865 - INFO - Current learning rate: 0.0005306186018335297\n",
      "2025-07-15 08:08:43,441 - INFO - Epoch: 1/5, Iter: 91/119 -- train_loss: 1.1274\n",
      "Epoch [1/5]: [90/119]  76%|███████▌  , loss=1.14 [04:13<01:20]\n",
      "Epoch [1/5]: [90/119]  76%|███████▌  , loss=1.13 [04:13<01:20]\n",
      "Epoch [1/5]: [91/119]  76%|███████▋  , loss=1.13 [04:13<01:07]\n",
      "2025-07-15 08:08:43,443 - INFO - Current learning rate: 0.0005391099906908656\n",
      "2025-07-15 08:08:45,005 - INFO - Epoch: 1/5, Iter: 92/119 -- train_loss: 1.1257\n",
      "Epoch [1/5]: [91/119]  76%|███████▋  , loss=1.13 [04:15<01:07]\n",
      "Epoch [1/5]: [91/119]  76%|███████▋  , loss=1.13 [04:15<01:07]\n",
      "Epoch [1/5]: [92/119]  77%|███████▋  , loss=1.13 [04:15<00:58]\n",
      "2025-07-15 08:08:45,006 - INFO - Current learning rate: 0.0005475953933351584\n",
      "2025-07-15 08:08:50,363 - INFO - Epoch: 1/5, Iter: 93/119 -- train_loss: 1.1395\n",
      "Epoch [1/5]: [92/119]  77%|███████▋  , loss=1.13 [04:20<00:58]\n",
      "Epoch [1/5]: [92/119]  77%|███████▋  , loss=1.14 [04:20<00:58]\n",
      "Epoch [1/5]: [93/119]  78%|███████▊  , loss=1.14 [04:20<01:21]\n",
      "2025-07-15 08:08:50,365 - INFO - Current learning rate: 0.000556072151710274\n",
      "2025-07-15 08:08:51,784 - INFO - Epoch: 1/5, Iter: 94/119 -- train_loss: 1.0933\n",
      "Epoch [1/5]: [93/119]  78%|███████▊  , loss=1.14 [04:22<01:21]\n",
      "Epoch [1/5]: [93/119]  78%|███████▊  , loss=1.09 [04:22<01:21]\n",
      "Epoch [1/5]: [94/119]  79%|███████▉  , loss=1.09 [04:22<01:05]\n",
      "2025-07-15 08:08:51,786 - INFO - Current learning rate: 0.0005645376104678986\n",
      "2025-07-15 08:08:53,136 - INFO - Epoch: 1/5, Iter: 95/119 -- train_loss: 1.1203\n",
      "Epoch [1/5]: [94/119]  79%|███████▉  , loss=1.09 [04:23<01:05]\n",
      "Epoch [1/5]: [94/119]  79%|███████▉  , loss=1.12 [04:23<01:05]\n",
      "Epoch [1/5]: [95/119]  80%|███████▉  , loss=1.12 [04:23<00:53]\n",
      "2025-07-15 08:08:53,137 - INFO - Current learning rate: 0.0005729891177993294\n",
      "2025-07-15 08:08:54,676 - INFO - Epoch: 1/5, Iter: 96/119 -- train_loss: 1.1213\n",
      "Epoch [1/5]: [95/119]  80%|███████▉  , loss=1.12 [04:25<00:53]\n",
      "Epoch [1/5]: [95/119]  80%|███████▉  , loss=1.12 [04:25<00:53]\n",
      "Epoch [1/5]: [96/119]  81%|████████  , loss=1.12 [04:25<00:46]\n",
      "2025-07-15 08:08:54,678 - INFO - Current learning rate: 0.0005814240262661536\n",
      "2025-07-15 08:09:00,527 - INFO - Epoch: 1/5, Iter: 97/119 -- train_loss: 1.1168\n",
      "Epoch [1/5]: [96/119]  81%|████████  , loss=1.12 [04:30<00:46]\n",
      "Epoch [1/5]: [96/119]  81%|████████  , loss=1.12 [04:30<00:46]\n",
      "Epoch [1/5]: [97/119]  82%|████████▏ , loss=1.12 [04:30<01:09]\n",
      "2025-07-15 08:09:00,529 - INFO - Current learning rate: 0.0005898396936295607\n",
      "2025-07-15 08:09:01,974 - INFO - Epoch: 1/5, Iter: 98/119 -- train_loss: 1.1045\n",
      "Epoch [1/5]: [97/119]  82%|████████▏ , loss=1.12 [04:32<01:09]\n",
      "Epoch [1/5]: [97/119]  82%|████████▏ , loss=1.1 [04:32<01:09]\n",
      "Epoch [1/5]: [98/119]  82%|████████▏ , loss=1.1 [04:32<00:55]\n",
      "2025-07-15 08:09:01,975 - INFO - Current learning rate: 0.0005982334836780231\n",
      "2025-07-15 08:09:03,895 - INFO - Epoch: 1/5, Iter: 99/119 -- train_loss: 1.1126\n",
      "Epoch [1/5]: [98/119]  82%|████████▏ , loss=1.1 [04:34<00:55]\n",
      "Epoch [1/5]: [98/119]  82%|████████▏ , loss=1.11 [04:34<00:55]\n",
      "Epoch [1/5]: [99/119]  83%|████████▎ , loss=1.11 [04:34<00:48]\n",
      "2025-07-15 08:09:03,896 - INFO - Current learning rate: 0.000606602767053093\n",
      "2025-07-15 08:09:05,469 - INFO - Epoch: 1/5, Iter: 100/119 -- train_loss: 1.1129\n",
      "Epoch [1/5]: [99/119]  83%|████████▎ , loss=1.11 [04:35<00:48]\n",
      "Epoch [1/5]: [99/119]  83%|████████▎ , loss=1.11 [04:35<00:48]\n",
      "Epoch [1/5]: [100/119]  84%|████████▍ , loss=1.11 [04:35<00:41]\n",
      "2025-07-15 08:09:05,471 - INFO - Current learning rate: 0.0006149449220730483\n",
      "2025-07-15 08:09:11,318 - INFO - Epoch: 1/5, Iter: 101/119 -- train_loss: 1.1107\n",
      "Epoch [1/5]: [100/119]  84%|████████▍ , loss=1.11 [04:41<00:41]\n",
      "Epoch [1/5]: [100/119]  84%|████████▍ , loss=1.11 [04:41<00:41]\n",
      "Epoch [1/5]: [101/119]  85%|████████▍ , loss=1.11 [04:41<00:59]\n",
      "2025-07-15 08:09:11,319 - INFO - Current learning rate: 0.0006232573355541365\n",
      "2025-07-15 08:09:12,768 - INFO - Epoch: 1/5, Iter: 102/119 -- train_loss: 1.1038\n",
      "Epoch [1/5]: [101/119]  85%|████████▍ , loss=1.11 [04:43<00:59]\n",
      "Epoch [1/5]: [101/119]  85%|████████▍ , loss=1.1 [04:43<00:59]\n",
      "Epoch [1/5]: [102/119]  86%|████████▌ , loss=1.1 [04:43<00:46]\n",
      "2025-07-15 08:09:12,769 - INFO - Current learning rate: 0.000631537403629155\n",
      "2025-07-15 08:09:14,243 - INFO - Epoch: 1/5, Iter: 103/119 -- train_loss: 1.1112\n",
      "Epoch [1/5]: [102/119]  86%|████████▌ , loss=1.1 [04:44<00:46]\n",
      "Epoch [1/5]: [102/119]  86%|████████▌ , loss=1.11 [04:44<00:46]\n",
      "Epoch [1/5]: [103/119]  87%|████████▋ , loss=1.11 [04:44<00:37]\n",
      "2025-07-15 08:09:14,245 - INFO - Current learning rate: 0.000639782532563114\n",
      "2025-07-15 08:09:17,110 - INFO - Epoch: 1/5, Iter: 104/119 -- train_loss: 1.1068\n",
      "Epoch [1/5]: [103/119]  87%|████████▋ , loss=1.11 [04:47<00:37]\n",
      "Epoch [1/5]: [103/119]  87%|████████▋ , loss=1.11 [04:47<00:37]\n",
      "Epoch [1/5]: [104/119]  87%|████████▋ , loss=1.11 [04:47<00:37]\n",
      "2025-07-15 08:09:17,112 - INFO - Current learning rate: 0.0006479901395657253\n",
      "2025-07-15 08:09:21,257 - INFO - Epoch: 1/5, Iter: 105/119 -- train_loss: 1.0964\n",
      "Epoch [1/5]: [104/119]  87%|████████▋ , loss=1.11 [04:51<00:37]\n",
      "Epoch [1/5]: [104/119]  87%|████████▋ , loss=1.1 [04:51<00:37]\n",
      "Epoch [1/5]: [105/119]  88%|████████▊ , loss=1.1 [04:51<00:41]\n",
      "2025-07-15 08:09:21,258 - INFO - Current learning rate: 0.0006561576536004625\n",
      "2025-07-15 08:09:22,869 - INFO - Epoch: 1/5, Iter: 106/119 -- train_loss: 1.0999\n",
      "Epoch [1/5]: [105/119]  88%|████████▊ , loss=1.1 [04:53<00:41]\n",
      "Epoch [1/5]: [105/119]  88%|████████▊ , loss=1.1 [04:53<00:41]\n",
      "Epoch [1/5]: [106/119]  89%|████████▉ , loss=1.1 [04:53<00:33]\n",
      "2025-07-15 08:09:22,871 - INFO - Current learning rate: 0.0006642825161899382\n",
      "2025-07-15 08:09:24,144 - INFO - Epoch: 1/5, Iter: 107/119 -- train_loss: 1.0938\n",
      "Epoch [1/5]: [106/119]  89%|████████▉ , loss=1.1 [04:54<00:33]\n",
      "Epoch [1/5]: [106/119]  89%|████████▉ , loss=1.09 [04:54<00:33]\n",
      "Epoch [1/5]: [107/119]  90%|████████▉ , loss=1.09 [04:54<00:26]\n",
      "2025-07-15 08:09:24,145 - INFO - Current learning rate: 0.0006723621822173502\n",
      "2025-07-15 08:09:27,427 - INFO - Epoch: 1/5, Iter: 108/119 -- train_loss: 1.1054\n",
      "Epoch [1/5]: [107/119]  90%|████████▉ , loss=1.09 [04:57<00:26]\n",
      "Epoch [1/5]: [107/119]  90%|████████▉ , loss=1.11 [04:57<00:26]\n",
      "Epoch [1/5]: [108/119]  91%|█████████ , loss=1.11 [04:57<00:27]\n",
      "2025-07-15 08:09:27,428 - INFO - Current learning rate: 0.0006803941207237387\n",
      "2025-07-15 08:09:29,951 - INFO - Epoch: 1/5, Iter: 109/119 -- train_loss: 1.1044\n",
      "Epoch [1/5]: [108/119]  91%|█████████ , loss=1.11 [05:00<00:27]\n",
      "Epoch [1/5]: [108/119]  91%|█████████ , loss=1.1 [05:00<00:27]\n",
      "Epoch [1/5]: [109/119]  92%|█████████▏, loss=1.1 [05:00<00:25]\n",
      "2025-07-15 08:09:29,953 - INFO - Current learning rate: 0.0006883758157008112\n",
      "2025-07-15 08:09:32,847 - INFO - Epoch: 1/5, Iter: 110/119 -- train_loss: 1.0922\n",
      "Epoch [1/5]: [109/119]  92%|█████████▏, loss=1.1 [05:03<00:25]\n",
      "Epoch [1/5]: [109/119]  92%|█████████▏, loss=1.09 [05:03<00:25]\n",
      "Epoch [1/5]: [110/119]  92%|█████████▏, loss=1.09 [05:03<00:23]\n",
      "2025-07-15 08:09:32,848 - INFO - Current learning rate: 0.0006963047668790832\n",
      "2025-07-15 08:09:34,179 - INFO - Epoch: 1/5, Iter: 111/119 -- train_loss: 1.0913\n",
      "Epoch [1/5]: [110/119]  92%|█████████▏, loss=1.09 [05:04<00:23]\n",
      "Epoch [1/5]: [110/119]  92%|█████████▏, loss=1.09 [05:04<00:23]\n",
      "Epoch [1/5]: [111/119]  93%|█████████▎, loss=1.09 [05:04<00:17]\n",
      "2025-07-15 08:09:34,180 - INFO - Current learning rate: 0.0007041784905110883\n",
      "2025-07-15 08:09:37,774 - INFO - Epoch: 1/5, Iter: 112/119 -- train_loss: 1.0995\n",
      "Epoch [1/5]: [111/119]  93%|█████████▎, loss=1.09 [05:08<00:17]\n",
      "Epoch [1/5]: [111/119]  93%|█████████▎, loss=1.1 [05:08<00:17]\n",
      "Epoch [1/5]: [112/119]  94%|█████████▍, loss=1.1 [05:08<00:18]\n",
      "2025-07-15 08:09:37,776 - INFO - Current learning rate: 0.0007119945201494132\n",
      "2025-07-15 08:09:45,711 - INFO - Epoch: 1/5, Iter: 113/119 -- train_loss: 1.0708\n",
      "Epoch [1/5]: [112/119]  94%|█████████▍, loss=1.1 [05:16<00:18]\n",
      "Epoch [1/5]: [112/119]  94%|█████████▍, loss=1.07 [05:16<00:18]\n",
      "Epoch [1/5]: [113/119]  95%|█████████▍, loss=1.07 [05:16<00:25]\n",
      "2025-07-15 08:09:45,713 - INFO - Current learning rate: 0.0007197504074193139\n",
      "2025-07-15 08:09:47,016 - INFO - Epoch: 1/5, Iter: 114/119 -- train_loss: 1.0864\n",
      "Epoch [1/5]: [113/119]  95%|█████████▍, loss=1.07 [05:17<00:25]\n",
      "Epoch [1/5]: [113/119]  95%|█████████▍, loss=1.09 [05:17<00:25]\n",
      "Epoch [1/5]: [114/119]  96%|█████████▌, loss=1.09 [05:17<00:16]2025-07-15 08:09:47,018 - INFO - Current learning rate: 0.00072744372278567\n",
      "2025-07-15 08:09:48,207 - INFO - Epoch: 1/5, Iter: 115/119 -- train_loss: 1.0951\n",
      "Epoch [1/5]: [114/119]  96%|█████████▌, loss=1.09 [05:18<00:16]\n",
      "Epoch [1/5]: [114/119]  96%|█████████▌, loss=1.1 [05:18<00:16]\n",
      "Epoch [1/5]: [115/119]  97%|█████████▋, loss=1.1 [05:18<00:10]\n",
      "2025-07-15 08:09:48,208 - INFO - Current learning rate: 0.0007350720563140377\n",
      "2025-07-15 08:09:50,162 - INFO - Epoch: 1/5, Iter: 116/119 -- train_loss: 1.0873\n",
      "Epoch [1/5]: [115/119]  97%|█████████▋, loss=1.1 [05:20<00:10]\n",
      "Epoch [1/5]: [115/119]  97%|█████████▋, loss=1.09 [05:20<00:10]\n",
      "Epoch [1/5]: [116/119]  97%|█████████▋, loss=1.09 [05:20<00:07]\n",
      "2025-07-15 08:09:50,163 - INFO - Current learning rate: 0.0007426330184255622\n",
      "2025-07-15 08:09:51,768 - INFO - Epoch: 1/5, Iter: 117/119 -- train_loss: 1.0611\n",
      "Epoch [1/5]: [116/119]  97%|█████████▋, loss=1.09 [05:22<00:07]\n",
      "Epoch [1/5]: [116/119]  97%|█████████▋, loss=1.06 [05:22<00:07]\n",
      "Epoch [1/5]: [117/119]  98%|█████████▊, loss=1.06 [05:22<00:04]\n",
      "2025-07-15 08:09:51,769 - INFO - Current learning rate: 0.0007501242406455155\n",
      "2025-07-15 08:09:52,533 - INFO - Epoch: 1/5, Iter: 118/119 -- train_loss: 1.0748\n",
      "Epoch [1/5]: [117/119]  98%|█████████▊, loss=1.06 [05:22<00:04]\n",
      "Epoch [1/5]: [117/119]  98%|█████████▊, loss=1.07 [05:22<00:04]\n",
      "Epoch [1/5]: [118/119]  99%|█████████▉, loss=1.07 [05:22<00:01]\n",
      "2025-07-15 08:09:52,534 - INFO - Current learning rate: 0.0007575433763452208\n",
      "2025-07-15 08:09:53,282 - INFO - Epoch: 1/5, Iter: 119/119 -- train_loss: 1.0924\n",
      "Epoch [1/5]: [118/119]  99%|█████████▉, loss=1.07 [05:23<00:01]\n",
      "Epoch [1/5]: [118/119]  99%|█████████▉, loss=1.09 [05:23<00:01]\n",
      "Epoch [1/5]: [119/119] 100%|██████████, loss=1.09 [05:23<00:00]\n",
      "2025-07-15 08:09:53,284 - INFO - Current learning rate: 0.0007648881014771364\n",
      "2025-07-15 08:09:53,284 - INFO - Engine run resuming from iteration 0, epoch 0 until 1 epochs\n",
      "[1/20]   5%|#033[32m▌         #033[0m [00:00<?]#033[A\n",
      "Iteration: [1/20]   5%|#033[32m▌         #033[0m [00:00<?]#033[A\n",
      "/usr/local/lib/python3.12/site-packages/monai/utils/deprecate_utils.py:221: FutureWarning: monai.metrics.utils get_mask_edges:always_return_as_numpy: Argument `always_return_as_numpy` has been deprecated since version 1.5.0. It will be removed in version 1.7.0. The option is removed and the return type will always be equal to the input type.\n",
      "  warn_deprecated(argname, msg, warning_category)\n",
      "Iteration: [1/20]   5%|#033[32m▌         #033[0m [00:10<?]#033[A\n",
      "Iteration: [2/20]  10%|#033[32m█         #033[0m [00:10<03:01]#033[A\n",
      "Iteration: [2/20]  10%|#033[32m█         #033[0m [00:19<03:01]#033[A\n",
      "Iteration: [3/20]  15%|#033[32m█▌        #033[0m [00:19<02:47]#033[A\n",
      "/usr/local/lib/python3.12/site-packages/monai/metrics/utils.py:327: UserWarning: the ground truth of class 0 is all 0, this may result in nan/inf distance.\n",
      "  warnings.warn(\n",
      "Iteration: [3/20]  15%|#033[32m█▌        #033[0m [00:26<02:47]#033[A\n",
      "Iteration: [4/20]  20%|#033[32m██        #033[0m [00:26<02:16]#033[A\n",
      "Iteration: [4/20]  20%|#033[32m██        #033[0m [00:34<02:16]#033[A\n",
      "Iteration: [5/20]  25%|#033[32m██▌       #033[0m [00:34<02:03]#033[A\n",
      "Iteration: [5/20]  25%|#033[32m██▌       #033[0m [00:43<02:03]#033[A\n",
      "Iteration: [6/20]  30%|#033[32m███       #033[0m [00:43<02:01]#033[A\n",
      "Iteration: [6/20]  30%|#033[32m███       #033[0m [00:53<02:01]#033[A\n",
      "Iteration: [7/20]  35%|#033[32m███▌      #033[0m [00:53<01:55]#033[A\n",
      "Iteration: [7/20]  35%|#033[32m███▌      #033[0m [01:02<01:55]#033[A\n",
      "Iteration: [8/20]  40%|#033[32m████      #033[0m [01:02<01:47]#033[A\n",
      "Iteration: [8/20]  40%|#033[32m████      #033[0m [01:09<01:47]#033[A\n",
      "Iteration: [9/20]  45%|#033[32m████▌     #033[0m [01:09<01:32]#033[A\n",
      "Iteration: [9/20]  45%|#033[32m████▌     #033[0m [01:23<01:32]#033[A\n",
      "Iteration: [10/20]  50%|#033[32m█████     #033[0m [01:23<01:40]#033[A\n",
      "Iteration: [10/20]  50%|#033[32m█████     #033[0m [01:35<01:40]#033[A\n",
      "Iteration: [11/20]  55%|#033[32m█████▌    #033[0m [01:35<01:37]#033[A\n",
      "Iteration: [11/20]  55%|#033[32m█████▌    #033[0m [01:47<01:37]#033[A\n",
      "Iteration: [12/20]  60%|#033[32m██████    #033[0m [01:47<01:28]#033[A\n",
      "Iteration: [12/20]  60%|#033[32m██████    #033[0m [01:58<01:28]#033[A\n",
      "Iteration: [13/20]  65%|#033[32m██████▌   #033[0m [01:58<01:16]#033[A\n",
      "Iteration: [13/20]  65%|#033[32m██████▌   #033[0m [02:07<01:16]#033[A\n",
      "Iteration: [14/20]  70%|#033[32m███████   #033[0m [02:07<01:03]#033[A\n",
      "Iteration: [14/20]  70%|#033[32m███████   #033[0m [02:13<01:03]#033[A\n",
      "Iteration: [15/20]  75%|#033[32m███████▌  #033[0m [02:13<00:45]#033[A\n",
      "Iteration: [15/20]  75%|#033[32m███████▌  #033[0m [02:19<00:45]#033[A\n",
      "Iteration: [16/20]  80%|#033[32m████████  #033[0m [02:19<00:32]#033[A\n",
      "Iteration: [16/20]  80%|#033[32m████████  #033[0m [02:27<00:32]#033[A\n",
      "Iteration: [17/20]  85%|#033[32m████████▌ #033[0m [02:27<00:24]#033[A\n",
      "Iteration: [17/20]  85%|#033[32m████████▌ #033[0m [02:34<00:24]#033[A\n",
      "Iteration: [18/20]  90%|#033[32m█████████ #033[0m [02:34<00:15]#033[A\n",
      "Iteration: [18/20]  90%|#033[32m█████████ #033[0m [02:42<00:15]#033[A\n",
      "Iteration: [19/20]  95%|#033[32m█████████▌#033[0m [02:42<00:07]#033[A\n",
      "Iteration: [19/20]  95%|#033[32m█████████▌#033[0m [02:51<00:07]#033[A\n",
      "Iteration: [20/20] 100%|#033[32m██████████#033[0m [02:51<00:00]#033[A\n",
      "2025-07-15 08:12:57,566 - INFO - Got new best metric of val_mean_dice: 0.006623285356909037\n",
      "#033[A\n",
      "/usr/local/lib/python3.12/site-packages/monai/handlers/utils.py:130: RuntimeWarning: Mean of empty slice\n",
      "  v = np.concatenate([v, np.nanmean(v, axis=1, keepdims=True)], axis=1)\n",
      "2025-07-15 08:12:59,897 - INFO - Epoch[1] Complete. Time taken: 00:03:06.542\n",
      "2025-07-15 08:12:59,898 - INFO - Engine run finished. Time taken: 00:03:06.614\n",
      "2025-07-15 08:12:59,965 - INFO - Epoch[1] Complete. Time taken: 00:08:46.059\n",
      "2025-07-15 08:13:08,972 - INFO - Epoch: 2/5, Iter: 1/119 -- train_loss: 1.0829\n",
      "[1/119]   1%|           [00:00<?]\n",
      "Epoch [2/5]: [1/119]   1%|           [00:00<?]\n",
      "Epoch [2/5]: [1/119]   1%|          , loss=1.08 [00:00<?]\n",
      "2025-07-15 08:13:08,975 - INFO - Current learning rate: 0.0007721561153028634\n",
      "2025-07-15 08:13:14,029 - INFO - Epoch: 2/5, Iter: 2/119 -- train_loss: 1.0802\n",
      "Epoch [2/5]: [1/119]   1%|          , loss=1.08 [00:05<?]\n",
      "Epoch [2/5]: [1/119]   1%|          , loss=1.08 [00:05<?]\n",
      "Epoch [2/5]: [2/119]   2%|▏         , loss=1.08 [00:05<09:51]2025-07-15 08:13:14,030 - INFO - Current learning rate: 0.0007793451411138535\n",
      "2025-07-15 08:13:15,398 - INFO - Epoch: 2/5, Iter: 3/119 -- train_loss: 1.0666\n",
      "Epoch [2/5]: [2/119]   2%|▏         , loss=1.08 [00:06<09:51]\n",
      "Epoch [2/5]: [2/119]   2%|▏         , loss=1.07 [00:06<09:51]\n",
      "Epoch [2/5]: [3/119]   3%|▎         , loss=1.07 [00:06<05:34]\n",
      "2025-07-15 08:13:15,399 - INFO - Current learning rate: 0.0007864529269445896\n",
      "2025-07-15 08:13:16,773 - INFO - Epoch: 2/5, Iter: 4/119 -- train_loss: 1.0871\n",
      "Epoch [2/5]: [3/119]   3%|▎         , loss=1.07 [00:07<05:34]\n",
      "Epoch [2/5]: [3/119]   3%|▎         , loss=1.09 [00:07<05:34]\n",
      "Epoch [2/5]: [4/119]   3%|▎         , loss=1.09 [00:07<04:12]\n",
      "2025-07-15 08:13:16,775 - INFO - Current learning rate: 0.0007934772462780135\n",
      "2025-07-15 08:13:18,104 - INFO - Epoch: 2/5, Iter: 5/119 -- train_loss: 1.0583\n",
      "Epoch [2/5]: [4/119]   3%|▎         , loss=1.09 [00:09<04:12]\n",
      "Epoch [2/5]: [4/119]   3%|▎         , loss=1.06 [00:09<04:12]\n",
      "Epoch [2/5]: [5/119]   4%|▍         , loss=1.06 [00:09<03:31]\n",
      "2025-07-15 08:13:18,106 - INFO - Current learning rate: 0.0008004158987429848\n",
      "2025-07-15 08:13:27,204 - INFO - Epoch: 2/5, Iter: 6/119 -- train_loss: 1.0871\n",
      "Epoch [2/5]: [5/119]   4%|▍         , loss=1.06 [00:18<03:31]\n",
      "Epoch [2/5]: [5/119]   4%|▍         , loss=1.09 [00:18<03:31]\n",
      "Epoch [2/5]: [6/119]   5%|▌         , loss=1.09 [00:18<08:24]\n",
      "2025-07-15 08:13:27,206 - INFO - Current learning rate: 0.0008072667108035476\n",
      "2025-07-15 08:13:28,848 - INFO - Epoch: 2/5, Iter: 7/119 -- train_loss: 1.0831\n",
      "Epoch [2/5]: [6/119]   5%|▌         , loss=1.09 [00:19<08:24]\n",
      "Epoch [2/5]: [6/119]   5%|▌         , loss=1.08 [00:19<08:24]\n",
      "Epoch [2/5]: [7/119]   6%|▌         , loss=1.08 [00:19<06:32]\n",
      "2025-07-15 08:13:28,850 - INFO - Current learning rate: 0.0008140275364397912\n",
      "2025-07-15 08:13:30,074 - INFO - Epoch: 2/5, Iter: 8/119 -- train_loss: 1.0805\n",
      "Epoch [2/5]: [7/119]   6%|▌         , loss=1.08 [00:21<06:32]\n",
      "Epoch [2/5]: [7/119]   6%|▌         , loss=1.08 [00:21<06:32]\n",
      "Epoch [2/5]: [8/119]   7%|▋         , loss=1.08 [00:21<05:06]\n",
      "2025-07-15 08:13:30,075 - INFO - Current learning rate: 0.0008206962578200921\n",
      "2025-07-15 08:13:31,391 - INFO - Epoch: 2/5, Iter: 9/119 -- train_loss: 1.0864\n",
      "Epoch [2/5]: [8/119]   7%|▋         , loss=1.08 [00:22<05:06]\n",
      "Epoch [2/5]: [8/119]   7%|▋         , loss=1.09 [00:22<05:06]\n",
      "Epoch [2/5]: [9/119]   8%|▊         , loss=1.09 [00:22<04:13]\n",
      "2025-07-15 08:13:31,393 - INFO - Current learning rate: 0.0008272707859645263\n",
      "2025-07-15 08:13:37,488 - INFO - Epoch: 2/5, Iter: 10/119 -- train_loss: 1.0768\n",
      "Epoch [2/5]: [9/119]   8%|▊         , loss=1.09 [00:28<04:13]\n",
      "Epoch [2/5]: [9/119]   8%|▊         , loss=1.08 [00:28<04:13]\n",
      "Epoch [2/5]: [10/119]   8%|▊         , loss=1.08 [00:28<06:20]\n",
      "2025-07-15 08:13:37,490 - INFO - Current learning rate: 0.0008337490613992423\n",
      "2025-07-15 08:13:38,671 - INFO - Epoch: 2/5, Iter: 11/119 -- train_loss: 1.0593\n",
      "Epoch [2/5]: [10/119]   8%|▊         , loss=1.08 [00:29<06:20]\n",
      "Epoch [2/5]: [10/119]   8%|▊         , loss=1.06 [00:29<06:20]\n",
      "Epoch [2/5]: [11/119]   9%|▉         , loss=1.06 [00:29<04:59]\n",
      "2025-07-15 08:13:38,672 - INFO - Current learning rate: 0.0008401290548015924\n",
      "2025-07-15 08:13:41,519 - INFO - Epoch: 2/5, Iter: 12/119 -- train_loss: 1.0919\n",
      "Epoch [2/5]: [11/119]   9%|▉         , loss=1.06 [00:32<04:59]\n",
      "Epoch [2/5]: [11/119]   9%|▉         , loss=1.09 [00:32<04:59]\n",
      "Epoch [2/5]: [12/119]  10%|█         , loss=1.09 [00:32<04:59]\n",
      "2025-07-15 08:13:41,521 - INFO - Current learning rate: 0.0008464087676358193\n",
      "2025-07-15 08:13:42,627 - INFO - Epoch: 2/5, Iter: 13/119 -- train_loss: 1.0814\n",
      "Epoch [2/5]: [12/119]  10%|█         , loss=1.09 [00:33<04:59]\n",
      "Epoch [2/5]: [12/119]  10%|█         , loss=1.08 [00:33<04:59]\n",
      "Epoch [2/5]: [13/119]  11%|█         , loss=1.08 [00:33<04:02]\n",
      "2025-07-15 08:13:42,629 - INFO - Current learning rate: 0.0008525862327790979\n",
      "2025-07-15 08:13:45,010 - INFO - Epoch: 2/5, Iter: 14/119 -- train_loss: 1.0829\n",
      "Epoch [2/5]: [13/119]  11%|█         , loss=1.08 [00:36<04:02]\n",
      "Epoch [2/5]: [13/119]  11%|█         , loss=1.08 [00:36<04:02]\n",
      "Epoch [2/5]: [14/119]  12%|█▏        , loss=1.08 [00:36<04:02]\n",
      "2025-07-15 08:13:45,012 - INFO - Current learning rate: 0.0008586595151377375\n",
      "2025-07-15 08:13:46,614 - INFO - Epoch: 2/5, Iter: 15/119 -- train_loss: 1.0845\n",
      "Epoch [2/5]: [14/119]  12%|█▏        , loss=1.08 [00:37<04:02]\n",
      "Epoch [2/5]: [14/119]  12%|█▏        , loss=1.08 [00:37<04:02]\n",
      "Epoch [2/5]: [15/119]  13%|█▎        , loss=1.08 [00:37<03:38]\n",
      "2025-07-15 08:13:46,616 - INFO - Current learning rate: 0.000864626712253351\n",
      "2025-07-15 08:13:54,141 - INFO - Epoch: 2/5, Iter: 16/119 -- train_loss: 1.0810\n",
      "Epoch [2/5]: [15/119]  13%|█▎        , loss=1.08 [00:45<03:38]\n",
      "Epoch [2/5]: [15/119]  13%|█▎        , loss=1.08 [00:45<03:38]\n",
      "Epoch [2/5]: [16/119]  13%|█▎        , loss=1.08 [00:45<06:24]\n",
      "2025-07-15 08:13:54,142 - INFO - Current learning rate: 0.0008704859548988004\n",
      "2025-07-15 08:13:55,581 - INFO - Epoch: 2/5, Iter: 17/119 -- train_loss: 1.0819\n",
      "Epoch [2/5]: [16/119]  13%|█▎        , loss=1.08 [00:46<06:24]\n",
      "Epoch [2/5]: [16/119]  13%|█▎        , loss=1.08 [00:46<06:24]\n",
      "Epoch [2/5]: [17/119]  14%|█▍        , loss=1.08 [00:46<05:10]\n",
      "2025-07-15 08:13:55,582 - INFO - Current learning rate: 0.0008762354076637336\n",
      "2025-07-15 08:13:56,673 - INFO - Epoch: 2/5, Iter: 18/119 -- train_loss: 1.0791\n",
      "Epoch [2/5]: [17/119]  14%|█▍        , loss=1.08 [00:47<05:10]\n",
      "Epoch [2/5]: [17/119]  14%|█▍        , loss=1.08 [00:47<05:10]\n",
      "Epoch [2/5]: [18/119]  15%|█▌        , loss=1.08 [00:47<04:08]\n",
      "2025-07-15 08:13:56,674 - INFO - Current learning rate: 0.0008818732695295267\n",
      "2025-07-15 08:13:58,629 - INFO - Epoch: 2/5, Iter: 19/119 -- train_loss: 1.0767\n",
      "Epoch [2/5]: [18/119]  15%|█▌        , loss=1.08 [00:49<04:08]\n",
      "Epoch [2/5]: [18/119]  15%|█▌        , loss=1.08 [00:49<04:08]\n",
      "Epoch [2/5]: [19/119]  16%|█▌        , loss=1.08 [00:49<03:50]\n",
      "2025-07-15 08:13:58,631 - INFO - Current learning rate: 0.0008873977744334548\n",
      "2025-07-15 08:14:01,565 - INFO - Epoch: 2/5, Iter: 20/119 -- train_loss: 1.0696\n",
      "Epoch [2/5]: [19/119]  16%|█▌        , loss=1.08 [00:52<03:50]\n",
      "Epoch [2/5]: [19/119]  16%|█▌        , loss=1.07 [00:52<03:50]\n",
      "Epoch [2/5]: [20/119]  17%|█▋        , loss=1.07 [00:52<04:07]\n",
      "2025-07-15 08:14:01,566 - INFO - Current learning rate: 0.0008928071918219112\n",
      "2025-07-15 08:14:05,018 - INFO - Epoch: 2/5, Iter: 21/119 -- train_loss: 1.0724\n",
      "Epoch [2/5]: [20/119]  17%|█▋        , loss=1.07 [00:56<04:07]\n",
      "Epoch [2/5]: [20/119]  17%|█▋        , loss=1.07 [00:56<04:07]\n",
      "Epoch [2/5]: [21/119]  18%|█▊        , loss=1.07 [00:56<04:32]\n",
      "2025-07-15 08:14:05,020 - INFO - Current learning rate: 0.0008980998271925047\n",
      "2025-07-15 08:14:06,278 - INFO - Epoch: 2/5, Iter: 22/119 -- train_loss: 1.0727\n",
      "Epoch [2/5]: [21/119]  18%|█▊        , loss=1.07 [00:57<04:32]\n",
      "Epoch [2/5]: [21/119]  18%|█▊        , loss=1.07 [00:57<04:32]\n",
      "Epoch [2/5]: [22/119]  18%|█▊        , loss=1.07 [00:57<03:45]\n",
      "2025-07-15 08:14:06,280 - INFO - Current learning rate: 0.0009032740226248624\n",
      "2025-07-15 08:14:07,749 - INFO - Epoch: 2/5, Iter: 23/119 -- train_loss: 1.0710\n",
      "Epoch [2/5]: [22/119]  18%|█▊        , loss=1.07 [00:58<03:45]\n",
      "Epoch [2/5]: [22/119]  18%|█▊        , loss=1.07 [00:58<03:45]\n",
      "Epoch [2/5]: [23/119]  19%|█▉        , loss=1.07 [00:58<03:18]\n",
      "2025-07-15 08:14:07,750 - INFO - Current learning rate: 0.0009083281572999747\n",
      "2025-07-15 08:14:09,978 - INFO - Epoch: 2/5, Iter: 24/119 -- train_loss: 1.0733\n",
      "Epoch [2/5]: [23/119]  19%|█▉        , loss=1.07 [01:01<03:18]\n",
      "Epoch [2/5]: [23/119]  19%|█▉        , loss=1.07 [01:01<03:18]\n",
      "Epoch [2/5]: [24/119]  20%|██        , loss=1.07 [01:01<03:21]\n",
      "2025-07-15 08:14:09,980 - INFO - Current learning rate: 0.0009132606480079176\n",
      "2025-07-15 08:14:14,924 - INFO - Epoch: 2/5, Iter: 25/119 -- train_loss: 1.0632\n",
      "Epoch [2/5]: [24/119]  20%|██        , loss=1.07 [01:05<03:21]\n",
      "Epoch [2/5]: [24/119]  20%|██        , loss=1.06 [01:05<03:21]\n",
      "Epoch [2/5]: [25/119]  21%|██        , loss=1.06 [01:05<04:38]\n",
      "2025-07-15 08:14:14,926 - INFO - Current learning rate: 0.0009180699496437932\n",
      "2025-07-15 08:14:16,489 - INFO - Epoch: 2/5, Iter: 26/119 -- train_loss: 1.0735\n",
      "Epoch [2/5]: [25/119]  21%|██        , loss=1.06 [01:07<04:38]\n",
      "Epoch [2/5]: [25/119]  21%|██        , loss=1.07 [01:07<04:38]\n",
      "Epoch [2/5]: [26/119]  22%|██▏       , loss=1.07 [01:07<03:56]\n",
      "2025-07-15 08:14:16,490 - INFO - Current learning rate: 0.0009227545556917353\n",
      "2025-07-15 08:14:20,879 - INFO - Epoch: 2/5, Iter: 27/119 -- train_loss: 1.0676\n",
      "Epoch [2/5]: [26/119]  22%|██▏       , loss=1.07 [01:11<03:56]\n",
      "Epoch [2/5]: [26/119]  22%|██▏       , loss=1.07 [01:11<03:56]\n",
      "Epoch [2/5]: [27/119]  23%|██▎       , loss=1.07 [01:11<04:45]\n",
      "2025-07-15 08:14:20,880 - INFO - Current learning rate: 0.0009273129986968259\n",
      "2025-07-15 08:14:23,496 - INFO - Epoch: 2/5, Iter: 28/119 -- train_loss: 1.0730\n",
      "Epoch [2/5]: [27/119]  23%|██▎       , loss=1.07 [01:14<04:45]\n",
      "Epoch [2/5]: [27/119]  23%|██▎       , loss=1.07 [01:14<04:45]\n",
      "Epoch [2/5]: [28/119]  24%|██▎       , loss=1.07 [01:14<04:28]\n",
      "2025-07-15 08:14:23,497 - INFO - Current learning rate: 0.0009317438507247756\n",
      "2025-07-15 08:14:27,454 - INFO - Epoch: 2/5, Iter: 29/119 -- train_loss: 1.0720\n",
      "Epoch [2/5]: [28/119]  24%|██▎       , loss=1.07 [01:18<04:28]\n",
      "Epoch [2/5]: [28/119]  24%|██▎       , loss=1.07 [01:18<04:28]\n",
      "Epoch [2/5]: [29/119]  24%|██▍       , loss=1.07 [01:18<04:52]\n",
      "2025-07-15 08:14:27,455 - INFO - Current learning rate: 0.0009360457238092254\n",
      "2025-07-15 08:14:28,794 - INFO - Epoch: 2/5, Iter: 30/119 -- train_loss: 1.0692\n",
      "Epoch [2/5]: [29/119]  24%|██▍       , loss=1.07 [01:19<04:52]\n",
      "Epoch [2/5]: [29/119]  24%|██▍       , loss=1.07 [01:19<04:52]\n",
      "Epoch [2/5]: [30/119]  25%|██▌       , loss=1.07 [01:19<03:58]\n",
      "2025-07-15 08:14:28,798 - INFO - Current learning rate: 0.0009402172703865272\n",
      "2025-07-15 08:14:30,139 - INFO - Epoch: 2/5, Iter: 31/119 -- train_loss: 1.0691\n",
      "Epoch [2/5]: [30/119]  25%|██▌       , loss=1.07 [01:21<03:58]\n",
      "Epoch [2/5]: [30/119]  25%|██▌       , loss=1.07 [01:21<03:58]\n",
      "Epoch [2/5]: [31/119]  26%|██▌       , loss=1.07 [01:21<03:20]\n",
      "2025-07-15 08:14:30,141 - INFO - Current learning rate: 0.0009442571837178694\n",
      "2025-07-15 08:14:31,749 - INFO - Epoch: 2/5, Iter: 32/119 -- train_loss: 1.0630\n",
      "Epoch [2/5]: [31/119]  26%|██▌       , loss=1.07 [01:22<03:20]\n",
      "Epoch [2/5]: [31/119]  26%|██▌       , loss=1.06 [01:22<03:20]\n",
      "Epoch [2/5]: [32/119]  27%|██▋       , loss=1.06 [01:22<03:00]\n",
      "2025-07-15 08:14:31,751 - INFO - Current learning rate: 0.0009481641982986124\n",
      "2025-07-15 08:14:34,704 - INFO - Epoch: 2/5, Iter: 33/119 -- train_loss: 1.0686\n",
      "Epoch [2/5]: [32/119]  27%|██▋       , loss=1.06 [01:25<03:00]\n",
      "Epoch [2/5]: [32/119]  27%|██▋       , loss=1.07 [01:25<03:00]\n",
      "Epoch [2/5]: [33/119]  28%|██▊       , loss=1.07 [01:25<03:21]\n",
      "2025-07-15 08:14:34,706 - INFO - Current learning rate: 0.0009519370902547094\n",
      "2025-07-15 08:14:39,536 - INFO - Epoch: 2/5, Iter: 34/119 -- train_loss: 1.0681\n",
      "Epoch [2/5]: [33/119]  28%|██▊       , loss=1.07 [01:30<03:21]\n",
      "Epoch [2/5]: [33/119]  28%|██▊       , loss=1.07 [01:30<03:21]\n",
      "Epoch [2/5]: [34/119]  29%|██▊       , loss=1.07 [01:30<04:22]\n",
      "2025-07-15 08:14:39,537 - INFO - Current learning rate: 0.0009555746777260853\n",
      "2025-07-15 08:14:47,690 - INFO - Epoch: 2/5, Iter: 35/119 -- train_loss: 1.0673\n",
      "Epoch [2/5]: [34/119]  29%|██▊       , loss=1.07 [01:38<04:22]\n",
      "Epoch [2/5]: [34/119]  29%|██▊       , loss=1.07 [01:38<04:22]\n",
      "Epoch [2/5]: [35/119]  29%|██▉       , loss=1.07 [01:38<06:27]\n",
      "2025-07-15 08:14:47,692 - INFO - Current learning rate: 0.0009590758212368541\n",
      "2025-07-15 08:14:48,944 - INFO - Epoch: 2/5, Iter: 36/119 -- train_loss: 1.0687\n",
      "Epoch [2/5]: [35/119]  29%|██▉       , loss=1.07 [01:39<06:27]\n",
      "Epoch [2/5]: [35/119]  29%|██▉       , loss=1.07 [01:39<06:27]\n",
      "Epoch [2/5]: [36/119]  30%|███       , loss=1.07 [01:39<04:58]\n",
      "2025-07-15 08:14:48,946 - INFO - Current learning rate: 0.0009624394240522616\n",
      "2025-07-15 08:14:50,207 - INFO - Epoch: 2/5, Iter: 37/119 -- train_loss: 1.0590\n",
      "Epoch [2/5]: [36/119]  30%|███       , loss=1.07 [01:41<04:58]\n",
      "Epoch [2/5]: [36/119]  30%|███       , loss=1.06 [01:41<04:58]\n",
      "Epoch [2/5]: [37/119]  31%|███       , loss=1.06 [01:41<03:57]\n",
      "2025-07-15 08:14:50,208 - INFO - Current learning rate: 0.0009656644325222369\n",
      "2025-07-15 08:14:51,523 - INFO - Epoch: 2/5, Iter: 38/119 -- train_loss: 1.0667\n",
      "Epoch [2/5]: [37/119]  31%|███       , loss=1.06 [01:42<03:57]\n",
      "Epoch [2/5]: [37/119]  31%|███       , loss=1.07 [01:42<03:57]\n",
      "Epoch [2/5]: [38/119]  32%|███▏      , loss=1.07 [01:42<03:16]\n",
      "2025-07-15 08:14:51,524 - INFO - Current learning rate: 0.0009687498364114489\n",
      "2025-07-15 08:14:57,270 - INFO - Epoch: 2/5, Iter: 39/119 -- train_loss: 0.9750\n",
      "Epoch [2/5]: [38/119]  32%|███▏      , loss=1.07 [01:48<03:16]\n",
      "Epoch [2/5]: [38/119]  32%|███▏      , loss=0.975 [01:48<03:16]\n",
      "Epoch [2/5]: [39/119]  33%|███▎      , loss=0.975 [01:48<04:33]\n",
      "2025-07-15 08:14:57,272 - INFO - Current learning rate: 0.0009716946692157638\n",
      "2025-07-15 08:14:58,489 - INFO - Epoch: 2/5, Iter: 40/119 -- train_loss: 1.0586\n",
      "Epoch [2/5]: [39/119]  33%|███▎      , loss=0.975 [01:49<04:33]\n",
      "Epoch [2/5]: [39/119]  33%|███▎      , loss=1.06 [01:49<04:33]\n",
      "Epoch [2/5]: [40/119]  34%|███▎      , loss=1.06 [01:49<03:38]\n",
      "2025-07-15 08:14:58,490 - INFO - Current learning rate: 0.0009744980084650021\n",
      "2025-07-15 08:14:59,630 - INFO - Epoch: 2/5, Iter: 41/119 -- train_loss: 1.0663\n",
      "Epoch [2/5]: [40/119]  34%|███▎      , loss=1.06 [01:50<03:38]\n",
      "Epoch [2/5]: [40/119]  34%|███▎      , loss=1.07 [01:50<03:38]\n",
      "Epoch [2/5]: [41/119]  34%|███▍      , loss=1.07 [01:50<02:57]\n",
      "2025-07-15 08:14:59,632 - INFO - Current learning rate: 0.0009771589760119031\n",
      "2025-07-15 08:15:00,855 - INFO - Epoch: 2/5, Iter: 42/119 -- train_loss: 1.0665\n",
      "Epoch [2/5]: [41/119]  34%|███▍      , loss=1.07 [01:51<02:57]\n",
      "Epoch [2/5]: [41/119]  34%|███▍      , loss=1.07 [01:51<02:57]\n",
      "Epoch [2/5]: [42/119]  35%|███▌      , loss=1.07 [01:51<02:30]\n",
      "2025-07-15 08:15:00,857 - INFO - Current learning rate: 0.0009796767383072047\n",
      "2025-07-15 08:15:11,570 - INFO - Epoch: 2/5, Iter: 43/119 -- train_loss: 1.0652\n",
      "Epoch [2/5]: [42/119]  35%|███▌      , loss=1.07 [02:02<02:30]\n",
      "Epoch [2/5]: [42/119]  35%|███▌      , loss=1.07 [02:02<02:30]\n",
      "Epoch [2/5]: [43/119]  36%|███▌      , loss=1.07 [02:02<05:48]\n",
      "2025-07-15 08:15:11,571 - INFO - Current learning rate: 0.0009820505066607534\n",
      "2025-07-15 08:15:12,926 - INFO - Epoch: 2/5, Iter: 44/119 -- train_loss: 1.0640\n",
      "Epoch [2/5]: [43/119]  36%|███▌      , loss=1.07 [02:03<05:48]\n",
      "Epoch [2/5]: [43/119]  36%|███▌      , loss=1.06 [02:03<05:48]\n",
      "Epoch [2/5]: [44/119]  37%|███▋      , loss=1.06 [02:03<04:31]\n",
      "2025-07-15 08:15:12,927 - INFO - Current learning rate: 0.000984279537488562\n",
      "2025-07-15 08:15:14,413 - INFO - Epoch: 2/5, Iter: 45/119 -- train_loss: 1.0159\n",
      "Epoch [2/5]: [44/119]  37%|███▋      , loss=1.06 [02:05<04:31]\n",
      "Epoch [2/5]: [44/119]  37%|███▋      , loss=1.02 [02:05<04:31]\n",
      "Epoch [2/5]: [45/119]  38%|███▊      , loss=1.02 [02:05<03:40]2025-07-15 08:15:14,415 - INFO - Current learning rate: 0.0009863631325457364\n",
      "2025-07-15 08:15:15,731 - INFO - Epoch: 2/5, Iter: 46/119 -- train_loss: 1.0573\n",
      "Epoch [2/5]: [45/119]  38%|███▊      , loss=1.02 [02:06<03:40]\n",
      "Epoch [2/5]: [45/119]  38%|███▊      , loss=1.06 [02:06<03:40]\n",
      "Epoch [2/5]: [46/119]  39%|███▊      , loss=1.06 [02:06<03:01]\n",
      "2025-07-15 08:15:15,732 - INFO - Current learning rate: 0.0009883006391452032\n",
      "2025-07-15 08:15:21,706 - INFO - Epoch: 2/5, Iter: 47/119 -- train_loss: 1.0421\n",
      "Epoch [2/5]: [46/119]  39%|███▊      , loss=1.06 [02:12<03:01]\n",
      "Epoch [2/5]: [46/119]  39%|███▊      , loss=1.04 [02:12<03:01]\n",
      "Epoch [2/5]: [47/119]  39%|███▉      , loss=1.04 [02:12<04:14]\n",
      "2025-07-15 08:15:21,707 - INFO - Current learning rate: 0.0009900914503621623\n",
      "2025-07-15 08:15:23,111 - INFO - Epoch: 2/5, Iter: 48/119 -- train_loss: 1.0621\n",
      "Epoch [2/5]: [47/119]  39%|███▉      , loss=1.04 [02:14<04:14]\n",
      "Epoch [2/5]: [47/119]  39%|███▉      , loss=1.06 [02:14<04:14]\n",
      "Epoch [2/5]: [48/119]  40%|████      , loss=1.06 [02:14<03:25]\n",
      "2025-07-15 08:15:23,112 - INFO - Current learning rate: 0.0009917350052242072\n",
      "2025-07-15 08:15:24,456 - INFO - Epoch: 2/5, Iter: 49/119 -- train_loss: 1.0631\n",
      "Epoch [2/5]: [48/119]  40%|████      , loss=1.06 [02:15<03:25]\n",
      "Epoch [2/5]: [48/119]  40%|████      , loss=1.06 [02:15<03:25]\n",
      "Epoch [2/5]: [49/119]  41%|████      , loss=1.06 [02:15<02:49]\n",
      "2025-07-15 08:15:24,457 - INFO - Current learning rate: 0.0009932307888870498\n",
      "2025-07-15 08:15:25,663 - INFO - Epoch: 2/5, Iter: 50/119 -- train_loss: 1.0589\n",
      "Epoch [2/5]: [49/119]  41%|████      , loss=1.06 [02:16<02:49]\n",
      "Epoch [2/5]: [49/119]  41%|████      , loss=1.06 [02:16<02:49]\n",
      "Epoch [2/5]: [50/119]  42%|████▏     , loss=1.06 [02:16<02:22]\n",
      "2025-07-15 08:15:25,665 - INFO - Current learning rate: 0.0009945783327957958\n",
      "2025-07-15 08:15:33,782 - INFO - Epoch: 2/5, Iter: 51/119 -- train_loss: 1.0539\n",
      "Epoch [2/5]: [50/119]  42%|████▏     , loss=1.06 [02:24<02:22]\n",
      "Epoch [2/5]: [50/119]  42%|████▏     , loss=1.05 [02:24<02:22]\n",
      "Epoch [2/5]: [51/119]  43%|████▎     , loss=1.05 [02:24<04:23]\n",
      "2025-07-15 08:15:33,783 - INFO - Current learning rate: 0.000995777214831719\n",
      "2025-07-15 08:15:34,974 - INFO - Epoch: 2/5, Iter: 52/119 -- train_loss: 1.0600\n",
      "Epoch [2/5]: [51/119]  43%|████▎     , loss=1.05 [02:26<04:23]\n",
      "Epoch [2/5]: [51/119]  43%|████▎     , loss=1.06 [02:26<04:23]\n",
      "Epoch [2/5]: [52/119]  44%|████▎     , loss=1.06 [02:26<03:25]\n",
      "2025-07-15 08:15:34,976 - INFO - Current learning rate: 0.0009968270594444913\n",
      "2025-07-15 08:15:36,403 - INFO - Epoch: 2/5, Iter: 53/119 -- train_loss: 1.0596\n",
      "Epoch [2/5]: [52/119]  44%|████▎     , loss=1.06 [02:27<03:25]\n",
      "Epoch [2/5]: [52/119]  44%|████▎     , loss=1.06 [02:27<03:25]\n",
      "Epoch [2/5]: [53/119]  45%|████▍     , loss=1.06 [02:27<02:50]\n",
      "2025-07-15 08:15:36,405 - INFO - Current learning rate: 0.0009977275377698227\n",
      "2025-07-15 08:15:37,792 - INFO - Epoch: 2/5, Iter: 54/119 -- train_loss: 1.0606\n",
      "Epoch [2/5]: [53/119]  45%|████▍     , loss=1.06 [02:28<02:50]\n",
      "Epoch [2/5]: [53/119]  45%|████▍     , loss=1.06 [02:28<02:50]\n",
      "Epoch [2/5]: [54/119]  45%|████▌     , loss=1.06 [02:28<02:24]\n",
      "2025-07-15 08:15:37,793 - INFO - Current learning rate: 0.0009984783677324794\n",
      "2025-07-15 08:15:44,020 - INFO - Epoch: 2/5, Iter: 55/119 -- train_loss: 1.0262\n",
      "Epoch [2/5]: [54/119]  45%|████▌     , loss=1.06 [02:35<02:24]\n",
      "Epoch [2/5]: [54/119]  45%|████▌     , loss=1.03 [02:35<02:24]\n",
      "Epoch [2/5]: [55/119]  46%|████▌     , loss=1.03 [02:35<03:39]\n",
      "2025-07-15 08:15:44,021 - INFO - Current learning rate: 0.000999079314134643\n",
      "2025-07-15 08:15:45,283 - INFO - Epoch: 2/5, Iter: 56/119 -- train_loss: 1.0605\n",
      "Epoch [2/5]: [55/119]  46%|████▌     , loss=1.03 [02:36<03:39]\n",
      "Epoch [2/5]: [55/119]  46%|████▌     , loss=1.06 [02:36<03:39]\n",
      "Epoch [2/5]: [56/119]  47%|████▋     , loss=1.06 [02:36<02:54]\n",
      "2025-07-15 08:15:45,284 - INFO - Current learning rate: 0.0009995301887295873\n",
      "2025-07-15 08:15:46,726 - INFO - Epoch: 2/5, Iter: 57/119 -- train_loss: 1.0584\n",
      "Epoch [2/5]: [56/119]  47%|████▋     , loss=1.06 [02:37<02:54]\n",
      "Epoch [2/5]: [56/119]  47%|████▋     , loss=1.06 [02:37<02:54]\n",
      "Epoch [2/5]: [57/119]  48%|████▊     , loss=1.06 [02:37<02:27]\n",
      "2025-07-15 08:15:46,728 - INFO - Current learning rate: 0.0009998308502806458\n",
      "2025-07-15 08:15:48,130 - INFO - Epoch: 2/5, Iter: 58/119 -- train_loss: 1.0551\n",
      "Epoch [2/5]: [57/119]  48%|████▊     , loss=1.06 [02:39<02:27]\n",
      "Epoch [2/5]: [57/119]  48%|████▊     , loss=1.06 [02:39<02:27]\n",
      "Epoch [2/5]: [58/119]  49%|████▊     , loss=1.06 [02:39<02:07]\n",
      "2025-07-15 08:15:48,131 - INFO - Current learning rate: 0.0009999812046054544\n",
      "2025-07-15 08:15:57,229 - INFO - Epoch: 2/5, Iter: 59/119 -- train_loss: 1.0580\n",
      "Epoch [2/5]: [58/119]  49%|████▊     , loss=1.06 [02:48<02:07]\n",
      "Epoch [2/5]: [58/119]  49%|████▊     , loss=1.06 [02:48<02:07]\n",
      "Epoch [2/5]: [59/119]  50%|████▉     , loss=1.06 [02:48<04:11]\n",
      "2025-07-15 08:15:57,230 - INFO - Current learning rate: 0.0009999964441167017\n",
      "2025-07-15 08:15:58,595 - INFO - Epoch: 2/5, Iter: 60/119 -- train_loss: 1.0566\n",
      "Epoch [2/5]: [59/119]  50%|████▉     , loss=1.06 [02:49<04:11]\n",
      "Epoch [2/5]: [59/119]  50%|████▉     , loss=1.06 [02:49<04:11]\n",
      "Epoch [2/5]: [60/119]  50%|█████     , loss=1.06 [02:49<03:17]\n",
      "2025-07-15 08:15:58,597 - INFO - Current learning rate: 0.000999967997353778\n",
      "2025-07-15 08:16:00,096 - INFO - Epoch: 2/5, Iter: 61/119 -- train_loss: 1.0579\n",
      "Epoch [2/5]: [60/119]  50%|█████     , loss=1.06 [02:51<03:17]\n",
      "Epoch [2/5]: [60/119]  50%|█████     , loss=1.06 [02:51<03:17]\n",
      "Epoch [2/5]: [61/119]  51%|█████▏    , loss=1.06 [02:51<02:41]2025-07-15 08:16:00,098 - INFO - Current learning rate: 0.0009999111054463852\n",
      "2025-07-15 08:16:01,370 - INFO - Epoch: 2/5, Iter: 62/119 -- train_loss: 1.0565\n",
      "Epoch [2/5]: [61/119]  51%|█████▏    , loss=1.06 [02:52<02:41]\n",
      "Epoch [2/5]: [61/119]  51%|█████▏    , loss=1.06 [02:52<02:41]\n",
      "Epoch [2/5]: [62/119]  52%|█████▏    , loss=1.06 [02:52<02:13]\n",
      "2025-07-15 08:16:01,372 - INFO - Current learning rate: 0.0009998257716313408\n",
      "2025-07-15 08:16:07,824 - INFO - Epoch: 2/5, Iter: 63/119 -- train_loss: 1.0564\n",
      "Epoch [2/5]: [62/119]  52%|█████▏    , loss=1.06 [02:58<02:13]\n",
      "Epoch [2/5]: [62/119]  52%|█████▏    , loss=1.06 [02:58<02:13]\n",
      "Epoch [2/5]: [63/119]  53%|█████▎    , loss=1.06 [02:58<03:19]\n",
      "2025-07-15 08:16:07,825 - INFO - Current learning rate: 0.0009997120007636398\n",
      "2025-07-15 08:16:09,091 - INFO - Epoch: 2/5, Iter: 64/119 -- train_loss: 1.0503\n",
      "Epoch [2/5]: [63/119]  53%|█████▎    , loss=1.06 [03:00<03:19]\n",
      "Epoch [2/5]: [63/119]  53%|█████▎    , loss=1.05 [03:00<03:19]\n",
      "Epoch [2/5]: [64/119]  54%|█████▍    , loss=1.05 [03:00<02:38]\n",
      "2025-07-15 08:16:09,092 - INFO - Current learning rate: 0.0009995697993161804\n",
      "2025-07-15 08:16:10,562 - INFO - Epoch: 2/5, Iter: 65/119 -- train_loss: 1.0560\n",
      "Epoch [2/5]: [64/119]  54%|█████▍    , loss=1.05 [03:01<02:38]\n",
      "Epoch [2/5]: [64/119]  54%|█████▍    , loss=1.06 [03:01<02:38]\n",
      "Epoch [2/5]: [65/119]  55%|█████▍    , loss=1.06 [03:01<02:12]\n",
      "2025-07-15 08:16:10,564 - INFO - Current learning rate: 0.0009993991753793942\n",
      "2025-07-15 08:16:11,976 - INFO - Epoch: 2/5, Iter: 66/119 -- train_loss: 1.0568\n",
      "Epoch [2/5]: [65/119]  55%|█████▍    , loss=1.06 [03:03<02:12]\n",
      "Epoch [2/5]: [65/119]  55%|█████▍    , loss=1.06 [03:03<02:12]\n",
      "Epoch [2/5]: [66/119]  55%|█████▌    , loss=1.06 [03:03<01:53]\n",
      "2025-07-15 08:16:11,978 - INFO - Current learning rate: 0.000999200138660786\n",
      "2025-07-15 08:16:18,268 - INFO - Epoch: 2/5, Iter: 67/119 -- train_loss: 1.0542\n",
      "Epoch [2/5]: [66/119]  55%|█████▌    , loss=1.06 [03:09<01:53]\n",
      "Epoch [2/5]: [66/119]  55%|█████▌    , loss=1.05 [03:09<01:53]\n",
      "Epoch [2/5]: [67/119]  56%|█████▋    , loss=1.05 [03:09<02:56]\n",
      "2025-07-15 08:16:18,269 - INFO - Current learning rate: 0.0009989727004843826\n",
      "2025-07-15 08:16:19,453 - INFO - Epoch: 2/5, Iter: 68/119 -- train_loss: 1.0365\n",
      "Epoch [2/5]: [67/119]  56%|█████▋    , loss=1.05 [03:10<02:56]\n",
      "Epoch [2/5]: [67/119]  56%|█████▋    , loss=1.04 [03:10<02:56]\n",
      "Epoch [2/5]: [68/119]  57%|█████▋    , loss=1.04 [03:10<02:19]\n",
      "2025-07-15 08:16:19,455 - INFO - Current learning rate: 0.0009987168737900878\n",
      "2025-07-15 08:16:20,654 - INFO - Epoch: 2/5, Iter: 69/119 -- train_loss: 1.0551\n",
      "Epoch [2/5]: [68/119]  57%|█████▋    , loss=1.04 [03:11<02:19]\n",
      "Epoch [2/5]: [68/119]  57%|█████▋    , loss=1.06 [03:11<02:19]\n",
      "Epoch [2/5]: [69/119]  58%|█████▊    , loss=1.06 [03:11<01:53]\n",
      "2025-07-15 08:16:20,655 - INFO - Current learning rate: 0.0009984326731329455\n",
      "2025-07-15 08:16:21,824 - INFO - Epoch: 2/5, Iter: 70/119 -- train_loss: 1.0484\n",
      "Epoch [2/5]: [69/119]  58%|█████▊    , loss=1.06 [03:12<01:53]\n",
      "Epoch [2/5]: [69/119]  58%|█████▊    , loss=1.05 [03:12<01:53]\n",
      "Epoch [2/5]: [70/119]  59%|█████▉    , loss=1.05 [03:12<01:35]\n",
      "2025-07-15 08:16:21,825 - INFO - Current learning rate: 0.0009981201146823133\n",
      "2025-07-15 08:16:26,011 - INFO - Epoch: 2/5, Iter: 71/119 -- train_loss: 1.0358\n",
      "Epoch [2/5]: [70/119]  59%|█████▉    , loss=1.05 [03:17<01:35]\n",
      "Epoch [2/5]: [70/119]  59%|█████▉    , loss=1.04 [03:17<01:35]\n",
      "Epoch [2/5]: [71/119]  60%|█████▉    , loss=1.04 [03:17<02:05]\n",
      "2025-07-15 08:16:26,012 - INFO - Current learning rate: 0.0009977792162209405\n",
      "2025-07-15 08:16:27,318 - INFO - Epoch: 2/5, Iter: 72/119 -- train_loss: 1.0409\n",
      "Epoch [2/5]: [71/119]  60%|█████▉    , loss=1.04 [03:18<02:05]\n",
      "Epoch [2/5]: [71/119]  60%|█████▉    , loss=1.04 [03:18<02:05]\n",
      "Epoch [2/5]: [72/119]  61%|██████    , loss=1.04 [03:18<01:44]\n",
      "2025-07-15 08:16:27,320 - INFO - Current learning rate: 0.0009974099971439586\n",
      "2025-07-15 08:16:28,621 - INFO - Epoch: 2/5, Iter: 73/119 -- train_loss: 1.0526\n",
      "Epoch [2/5]: [72/119]  61%|██████    , loss=1.04 [03:19<01:44]\n",
      "Epoch [2/5]: [72/119]  61%|██████    , loss=1.05 [03:19<01:44]\n",
      "Epoch [2/5]: [73/119]  61%|██████▏   , loss=1.05 [03:19<01:29]\n",
      "2025-07-15 08:16:28,623 - INFO - Current learning rate: 0.0009970124784577758\n",
      "2025-07-15 08:16:29,646 - INFO - Epoch: 2/5, Iter: 74/119 -- train_loss: 1.0497\n",
      "Epoch [2/5]: [73/119]  61%|██████▏   , loss=1.05 [03:20<01:29]\n",
      "Epoch [2/5]: [73/119]  61%|██████▏   , loss=1.05 [03:20<01:29]\n",
      "Epoch [2/5]: [74/119]  62%|██████▏   , loss=1.05 [03:20<01:15]\n",
      "2025-07-15 08:16:29,647 - INFO - Current learning rate: 0.0009965866827788832\n",
      "2025-07-15 08:16:33,631 - INFO - Epoch: 2/5, Iter: 75/119 -- train_loss: 1.0430\n",
      "Epoch [2/5]: [74/119]  62%|██████▏   , loss=1.05 [03:24<01:15]\n",
      "Epoch [2/5]: [74/119]  62%|██████▏   , loss=1.04 [03:24<01:15]\n",
      "Epoch [2/5]: [75/119]  63%|██████▎   , loss=1.04 [03:24<01:44]\n",
      "2025-07-15 08:16:33,633 - INFO - Current learning rate: 0.0009961326343325672\n",
      "2025-07-15 08:16:35,123 - INFO - Epoch: 2/5, Iter: 76/119 -- train_loss: 1.0510\n",
      "Epoch [2/5]: [75/119]  63%|██████▎   , loss=1.04 [03:26<01:44]\n",
      "Epoch [2/5]: [75/119]  63%|██████▎   , loss=1.05 [03:26<01:44]\n",
      "Epoch [2/5]: [76/119]  64%|██████▍   , loss=1.05 [03:26<01:30]\n",
      "2025-07-15 08:16:35,124 - INFO - Current learning rate: 0.0009956503589515322\n",
      "2025-07-15 08:16:36,425 - INFO - Epoch: 2/5, Iter: 77/119 -- train_loss: 1.0509\n",
      "Epoch [2/5]: [76/119]  64%|██████▍   , loss=1.05 [03:27<01:30]\n",
      "Epoch [2/5]: [76/119]  64%|██████▍   , loss=1.05 [03:27<01:30]\n",
      "Epoch [2/5]: [77/119]  65%|██████▍   , loss=1.05 [03:27<01:18]\n",
      "2025-07-15 08:16:36,426 - INFO - Current learning rate: 0.0009951398840744297\n",
      "2025-07-15 08:16:37,714 - INFO - Epoch: 2/5, Iter: 78/119 -- train_loss: 1.0324\n",
      "Epoch [2/5]: [77/119]  65%|██████▍   , loss=1.05 [03:28<01:18]\n",
      "Epoch [2/5]: [77/119]  65%|██████▍   , loss=1.03 [03:28<01:18]\n",
      "Epoch [2/5]: [78/119]  66%|██████▌   , loss=1.03 [03:28<01:09]\n",
      "2025-07-15 08:16:37,715 - INFO - Current learning rate: 0.0009946012387442983\n",
      "2025-07-15 08:16:41,538 - INFO - Epoch: 2/5, Iter: 79/119 -- train_loss: 1.0518\n",
      "Epoch [2/5]: [78/119]  66%|██████▌   , loss=1.03 [03:32<01:09]\n",
      "Epoch [2/5]: [78/119]  66%|██████▌   , loss=1.05 [03:32<01:09]\n",
      "Epoch [2/5]: [79/119]  66%|██████▋   , loss=1.05 [03:32<01:33]\n",
      "2025-07-15 08:16:41,539 - INFO - Current learning rate: 0.0009940344536069103\n",
      "2025-07-15 08:16:44,665 - INFO - Epoch: 2/5, Iter: 80/119 -- train_loss: 1.0507\n",
      "Epoch [2/5]: [79/119]  66%|██████▋   , loss=1.05 [03:35<01:33]\n",
      "Epoch [2/5]: [79/119]  66%|██████▋   , loss=1.05 [03:35<01:33]\n",
      "Epoch [2/5]: [80/119]  67%|██████▋   , loss=1.05 [03:35<01:40]\n",
      "2025-07-15 08:16:44,668 - INFO - Current learning rate: 0.0009934395609090287\n",
      "2025-07-15 08:16:45,880 - INFO - Epoch: 2/5, Iter: 81/119 -- train_loss: 1.0510\n",
      "Epoch [2/5]: [80/119]  67%|██████▋   , loss=1.05 [03:36<01:40]\n",
      "Epoch [2/5]: [80/119]  67%|██████▋   , loss=1.05 [03:36<01:40]\n",
      "Epoch [2/5]: [81/119]  68%|██████▊   , loss=1.05 [03:36<01:22]\n",
      "2025-07-15 08:16:45,882 - INFO - Current learning rate: 0.0009928165944965732\n",
      "2025-07-15 08:16:48,386 - INFO - Epoch: 2/5, Iter: 82/119 -- train_loss: 1.0499\n",
      "Epoch [2/5]: [81/119]  68%|██████▊   , loss=1.05 [03:39<01:22]\n",
      "Epoch [2/5]: [81/119]  68%|██████▊   , loss=1.05 [03:39<01:22]\n",
      "Epoch [2/5]: [82/119]  69%|██████▉   , loss=1.05 [03:39<01:23]\n",
      "2025-07-15 08:16:48,388 - INFO - Current learning rate: 0.000992165589812693\n",
      "2025-07-15 08:16:53,295 - INFO - Epoch: 2/5, Iter: 83/119 -- train_loss: 1.0497\n",
      "Epoch [2/5]: [82/119]  69%|██████▉   , loss=1.05 [03:44<01:23]\n",
      "Epoch [2/5]: [82/119]  69%|██████▉   , loss=1.05 [03:44<01:23]\n",
      "Epoch [2/5]: [83/119]  70%|██████▉   , loss=1.05 [03:44<01:50]\n",
      "2025-07-15 08:16:53,297 - INFO - Current learning rate: 0.0009914865838957516\n",
      "2025-07-15 08:16:57,836 - INFO - Epoch: 2/5, Iter: 84/119 -- train_loss: 1.0259\n",
      "Epoch [2/5]: [83/119]  70%|██████▉   , loss=1.05 [03:48<01:50]\n",
      "Epoch [2/5]: [83/119]  70%|██████▉   , loss=1.03 [03:48<01:50]\n",
      "Epoch [2/5]: [84/119]  71%|███████   , loss=1.03 [03:48<02:02]\n",
      "2025-07-15 08:16:57,837 - INFO - Current learning rate: 0.0009907796153772188\n",
      "2025-07-15 08:16:58,957 - INFO - Epoch: 2/5, Iter: 85/119 -- train_loss: 1.0434\n",
      "Epoch [2/5]: [84/119]  71%|███████   , loss=1.03 [03:49<02:02]\n",
      "Epoch [2/5]: [84/119]  71%|███████   , loss=1.04 [03:49<02:02]\n",
      "Epoch [2/5]: [85/119]  71%|███████▏  , loss=1.04 [03:49<01:34]\n",
      "2025-07-15 08:16:58,959 - INFO - Current learning rate: 0.0009900447244794732\n",
      "2025-07-15 08:17:01,194 - INFO - Epoch: 2/5, Iter: 86/119 -- train_loss: 1.0490\n",
      "Epoch [2/5]: [85/119]  71%|███████▏  , loss=1.04 [03:52<01:34]\n",
      "Epoch [2/5]: [85/119]  71%|███████▏  , loss=1.05 [03:52<01:34]\n",
      "Epoch [2/5]: [86/119]  72%|███████▏  , loss=1.05 [03:52<01:26]\n",
      "2025-07-15 08:17:01,196 - INFO - Current learning rate: 0.0009892819530135136\n",
      "2025-07-15 08:17:05,359 - INFO - Epoch: 2/5, Iter: 87/119 -- train_loss: 1.0485\n",
      "Epoch [2/5]: [86/119]  72%|███████▏  , loss=1.05 [03:56<01:26]\n",
      "Epoch [2/5]: [86/119]  72%|███████▏  , loss=1.05 [03:56<01:26]\n",
      "Epoch [2/5]: [87/119]  73%|███████▎  , loss=1.05 [03:56<01:38]\n",
      "2025-07-15 08:17:05,360 - INFO - Current learning rate: 0.000988491344376581\n",
      "2025-07-15 08:17:07,641 - INFO - Epoch: 2/5, Iter: 88/119 -- train_loss: 1.0106\n",
      "Epoch [2/5]: [87/119]  73%|███████▎  , loss=1.05 [03:58<01:38]\n",
      "Epoch [2/5]: [87/119]  73%|███████▎  , loss=1.01 [03:58<01:38]\n",
      "Epoch [2/5]: [88/119]  74%|███████▍  , loss=1.01 [03:58<01:28]\n",
      "2025-07-15 08:17:07,643 - INFO - Current learning rate: 0.0009876729435496874\n",
      "2025-07-15 08:17:08,736 - INFO - Epoch: 2/5, Iter: 89/119 -- train_loss: 1.0364\n",
      "Epoch [2/5]: [88/119]  74%|███████▍  , loss=1.01 [03:59<01:28]\n",
      "Epoch [2/5]: [88/119]  74%|███████▍  , loss=1.04 [03:59<01:28]\n",
      "Epoch [2/5]: [89/119]  75%|███████▍  , loss=1.04 [03:59<01:09]\n",
      "2025-07-15 08:17:08,737 - INFO - Current learning rate: 0.0009868267970950591\n",
      "2025-07-15 08:17:12,125 - INFO - Epoch: 2/5, Iter: 90/119 -- train_loss: 1.0483\n",
      "Epoch [2/5]: [89/119]  75%|███████▍  , loss=1.04 [04:03<01:09]\n",
      "Epoch [2/5]: [89/119]  75%|███████▍  , loss=1.05 [04:03<01:09]\n",
      "Epoch [2/5]: [90/119]  76%|███████▌  , loss=1.05 [04:03<01:16]\n",
      "2025-07-15 08:17:12,127 - INFO - Current learning rate: 0.000985952953153486\n",
      "2025-07-15 08:17:13,564 - INFO - Epoch: 2/5, Iter: 91/119 -- train_loss: 1.0413\n",
      "Epoch [2/5]: [90/119]  76%|███████▌  , loss=1.05 [04:04<01:16]\n",
      "Epoch [2/5]: [90/119]  76%|███████▌  , loss=1.04 [04:04<01:16]\n",
      "Epoch [2/5]: [91/119]  76%|███████▋  , loss=1.04 [04:04<01:03]\n",
      "2025-07-15 08:17:13,566 - INFO - Current learning rate: 0.0009850514614415835\n",
      "2025-07-15 08:17:15,587 - INFO - Epoch: 2/5, Iter: 92/119 -- train_loss: 1.0471\n",
      "Epoch [2/5]: [91/119]  76%|███████▋  , loss=1.04 [04:06<01:03]\n",
      "Epoch [2/5]: [91/119]  76%|███████▋  , loss=1.05 [04:06<01:03]\n",
      "Epoch [2/5]: [92/119]  77%|███████▋  , loss=1.05 [04:06<00:59]\n",
      "2025-07-15 08:17:15,589 - INFO - Current learning rate: 0.0009841223732489639\n",
      "2025-07-15 08:17:16,893 - INFO - Epoch: 2/5, Iter: 93/119 -- train_loss: 1.0372\n",
      "Epoch [2/5]: [92/119]  77%|███████▋  , loss=1.05 [04:07<00:59]\n",
      "Epoch [2/5]: [92/119]  77%|███████▋  , loss=1.04 [04:07<00:59]\n",
      "Epoch [2/5]: [93/119]  78%|███████▊  , loss=1.04 [04:07<00:50]\n",
      "2025-07-15 08:17:16,895 - INFO - Current learning rate: 0.000983165741435317\n",
      "2025-07-15 08:17:25,656 - INFO - Epoch: 2/5, Iter: 94/119 -- train_loss: 1.0462\n",
      "Epoch [2/5]: [93/119]  78%|███████▊  , loss=1.04 [04:16<00:50]\n",
      "Epoch [2/5]: [93/119]  78%|███████▊  , loss=1.05 [04:16<00:50]\n",
      "Epoch [2/5]: [94/119]  79%|███████▉  , loss=1.05 [04:16<01:39]\n",
      "2025-07-15 08:17:25,657 - INFO - Current learning rate: 0.0009821816204274051\n",
      "2025-07-15 08:17:26,824 - INFO - Epoch: 2/5, Iter: 95/119 -- train_loss: 1.0470\n",
      "Epoch [2/5]: [94/119]  79%|███████▉  , loss=1.05 [04:17<01:39]\n",
      "Epoch [2/5]: [94/119]  79%|███████▉  , loss=1.05 [04:17<01:39]\n",
      "Epoch [2/5]: [95/119]  80%|███████▉  , loss=1.05 [04:17<01:15]\n",
      "2025-07-15 08:17:26,825 - INFO - Current learning rate: 0.0009811700662159639\n",
      "2025-07-15 08:17:28,194 - INFO - Epoch: 2/5, Iter: 96/119 -- train_loss: 0.9942\n",
      "Epoch [2/5]: [95/119]  80%|███████▉  , loss=1.05 [04:19<01:15]\n",
      "Epoch [2/5]: [95/119]  80%|███████▉  , loss=0.994 [04:19<01:15]\n",
      "Epoch [2/5]: [96/119]  81%|████████  , loss=0.994 [04:19<00:59]\n",
      "2025-07-15 08:17:28,196 - INFO - Current learning rate: 0.0009801311363525187\n",
      "2025-07-15 08:17:29,358 - INFO - Epoch: 2/5, Iter: 97/119 -- train_loss: 1.0452\n",
      "Epoch [2/5]: [96/119]  81%|████████  , loss=0.994 [04:20<00:59]\n",
      "Epoch [2/5]: [96/119]  81%|████████  , loss=1.05 [04:20<00:59]\n",
      "Epoch [2/5]: [97/119]  82%|████████▏ , loss=1.05 [04:20<00:47]\n",
      "2025-07-15 08:17:29,360 - INFO - Current learning rate: 0.0009790648899461092\n",
      "2025-07-15 08:17:36,433 - INFO - Epoch: 2/5, Iter: 98/119 -- train_loss: 1.0332\n",
      "Epoch [2/5]: [97/119]  82%|████████▏ , loss=1.05 [04:27<00:47]\n",
      "Epoch [2/5]: [97/119]  82%|████████▏ , loss=1.03 [04:27<00:47]\n",
      "Epoch [2/5]: [98/119]  82%|████████▏ , loss=1.03 [04:27<01:16]\n",
      "2025-07-15 08:17:36,435 - INFO - Current learning rate: 0.0009779713876599272\n",
      "2025-07-15 08:17:37,688 - INFO - Epoch: 2/5, Iter: 99/119 -- train_loss: 1.0453\n",
      "Epoch [2/5]: [98/119]  82%|████████▏ , loss=1.03 [04:28<01:16]\n",
      "Epoch [2/5]: [98/119]  82%|████████▏ , loss=1.05 [04:28<01:16]\n",
      "Epoch [2/5]: [99/119]  83%|████████▎ , loss=1.05 [04:28<00:58]\n",
      "2025-07-15 08:17:37,689 - INFO - Current learning rate: 0.0009768506917078646\n",
      "2025-07-15 08:17:43,413 - INFO - Epoch: 2/5, Iter: 100/119 -- train_loss: 1.0451\n",
      "Epoch [2/5]: [99/119]  83%|████████▎ , loss=1.05 [04:34<00:58]\n",
      "Epoch [2/5]: [99/119]  83%|████████▎ , loss=1.05 [04:34<00:58]\n",
      "Epoch [2/5]: [100/119]  84%|████████▍ , loss=1.05 [04:34<01:11]\n",
      "2025-07-15 08:17:43,414 - INFO - Current learning rate: 0.0009757028658509734\n",
      "2025-07-15 08:17:44,763 - INFO - Epoch: 2/5, Iter: 101/119 -- train_loss: 1.0444\n",
      "Epoch [2/5]: [100/119]  84%|████████▍ , loss=1.05 [04:35<01:11]\n",
      "Epoch [2/5]: [100/119]  84%|████████▍ , loss=1.04 [04:35<01:11]\n",
      "Epoch [2/5]: [101/119]  85%|████████▍ , loss=1.04 [04:35<00:54]\n",
      "2025-07-15 08:17:44,765 - INFO - Current learning rate: 0.0009745279753938402\n",
      "2025-07-15 08:17:46,057 - INFO - Epoch: 2/5, Iter: 102/119 -- train_loss: 1.0333\n",
      "Epoch [2/5]: [101/119]  85%|████████▍ , loss=1.04 [04:37<00:54]\n",
      "Epoch [2/5]: [101/119]  85%|████████▍ , loss=1.03 [04:37<00:54]\n",
      "Epoch [2/5]: [102/119]  86%|████████▌ , loss=1.03 [04:37<00:42]\n",
      "2025-07-15 08:17:46,058 - INFO - Current learning rate: 0.000973326087180868\n",
      "2025-07-15 08:17:47,406 - INFO - Epoch: 2/5, Iter: 103/119 -- train_loss: 1.0440\n",
      "Epoch [2/5]: [102/119]  86%|████████▌ , loss=1.03 [04:38<00:42]\n",
      "Epoch [2/5]: [102/119]  86%|████████▌ , loss=1.04 [04:38<00:42]\n",
      "Epoch [2/5]: [103/119]  87%|████████▋ , loss=1.04 [04:38<00:34]\n",
      "2025-07-15 08:17:47,407 - INFO - Current learning rate: 0.0009720972695924745\n",
      "2025-07-15 08:18:03,123 - INFO - Epoch: 2/5, Iter: 104/119 -- train_loss: 1.0436\n",
      "Epoch [2/5]: [103/119]  87%|████████▋ , loss=1.04 [04:54<00:34]\n",
      "Epoch [2/5]: [103/119]  87%|████████▋ , loss=1.04 [04:54<00:34]\n",
      "Epoch [2/5]: [104/119]  87%|████████▋ , loss=1.04 [04:54<01:33]\n",
      "2025-07-15 08:18:03,124 - INFO - Current learning rate: 0.000970841592541202\n",
      "2025-07-15 08:18:04,503 - INFO - Epoch: 2/5, Iter: 105/119 -- train_loss: 1.0437\n",
      "Epoch [2/5]: [104/119]  87%|████████▋ , loss=1.04 [04:55<01:33]\n",
      "Epoch [2/5]: [104/119]  87%|████████▋ , loss=1.04 [04:55<01:33]\n",
      "Epoch [2/5]: [105/119]  88%|████████▊ , loss=1.04 [04:55<01:06]\n",
      "2025-07-15 08:18:04,504 - INFO - Current learning rate: 0.0009695591274677391\n",
      "2025-07-15 08:18:05,658 - INFO - Epoch: 2/5, Iter: 106/119 -- train_loss: 1.0433\n",
      "Epoch [2/5]: [105/119]  88%|████████▊ , loss=1.04 [04:56<01:06]\n",
      "Epoch [2/5]: [105/119]  88%|████████▊ , loss=1.04 [04:56<01:06]\n",
      "Epoch [2/5]: [106/119]  89%|████████▉ , loss=1.04 [04:56<00:47]\n",
      "2025-07-15 08:18:05,659 - INFO - Current learning rate: 0.0009682499473368564\n",
      "2025-07-15 08:18:07,190 - INFO - Epoch: 2/5, Iter: 107/119 -- train_loss: 1.0422\n",
      "Epoch [2/5]: [106/119]  89%|████████▉ , loss=1.04 [04:58<00:47]\n",
      "Epoch [2/5]: [106/119]  89%|████████▉ , loss=1.04 [04:58<00:47]\n",
      "Epoch [2/5]: [107/119]  90%|████████▉ , loss=1.04 [04:58<00:36]\n",
      "2025-07-15 08:18:07,192 - INFO - Current learning rate: 0.000966914126633255\n",
      "2025-07-15 08:18:14,057 - INFO - Epoch: 2/5, Iter: 108/119 -- train_loss: 1.0436\n",
      "Epoch [2/5]: [107/119]  90%|████████▉ , loss=1.04 [05:05<00:36]\n",
      "Epoch [2/5]: [107/119]  90%|████████▉ , loss=1.04 [05:05<00:36]\n",
      "Epoch [2/5]: [108/119]  91%|█████████ , loss=1.04 [05:05<00:46]\n",
      "2025-07-15 08:18:14,058 - INFO - Current learning rate: 0.0009655517413573294\n",
      "2025-07-15 08:18:15,230 - INFO - Epoch: 2/5, Iter: 109/119 -- train_loss: 1.0398\n",
      "Epoch [2/5]: [108/119]  91%|█████████ , loss=1.04 [05:06<00:46]\n",
      "Epoch [2/5]: [108/119]  91%|█████████ , loss=1.04 [05:06<00:46]\n",
      "Epoch [2/5]: [109/119]  92%|█████████▏, loss=1.04 [05:06<00:32]\n",
      "2025-07-15 08:18:15,231 - INFO - Current learning rate: 0.0009641628690208426\n",
      "2025-07-15 08:18:16,372 - INFO - Epoch: 2/5, Iter: 110/119 -- train_loss: 1.0422\n",
      "Epoch [2/5]: [109/119]  92%|█████████▏, loss=1.04 [05:07<00:32]\n",
      "Epoch [2/5]: [109/119]  92%|█████████▏, loss=1.04 [05:07<00:32]\n",
      "Epoch [2/5]: [110/119]  92%|█████████▏, loss=1.04 [05:07<00:23]\n",
      "2025-07-15 08:18:16,374 - INFO - Current learning rate: 0.0009627475886425167\n",
      "2025-07-15 08:18:17,950 - INFO - Epoch: 2/5, Iter: 111/119 -- train_loss: 1.0299\n",
      "Epoch [2/5]: [110/119]  92%|█████████▏, loss=1.04 [05:08<00:23]\n",
      "Epoch [2/5]: [110/119]  92%|█████████▏, loss=1.03 [05:08<00:23]\n",
      "Epoch [2/5]: [111/119]  93%|█████████▎, loss=1.03 [05:08<00:18]\n",
      "2025-07-15 08:18:17,952 - INFO - Current learning rate: 0.0009613059807435375\n",
      "2025-07-15 08:18:30,626 - INFO - Epoch: 2/5, Iter: 112/119 -- train_loss: 1.0417\n",
      "Epoch [2/5]: [111/119]  93%|█████████▎, loss=1.03 [05:21<00:18]\n",
      "Epoch [2/5]: [111/119]  93%|█████████▎, loss=1.04 [05:21<00:18]\n",
      "Epoch [2/5]: [112/119]  94%|█████████▍, loss=1.04 [05:21<00:38]\n",
      "2025-07-15 08:18:30,627 - INFO - Current learning rate: 0.0009598381273429725\n",
      "2025-07-15 08:18:31,460 - INFO - Epoch: 2/5, Iter: 113/119 -- train_loss: 1.0416\n",
      "Epoch [2/5]: [112/119]  94%|█████████▍, loss=1.04 [05:22<00:38]\n",
      "Epoch [2/5]: [112/119]  94%|█████████▍, loss=1.04 [05:22<00:38]\n",
      "Epoch [2/5]: [113/119]  95%|█████████▍, loss=1.04 [05:22<00:24]\n",
      "2025-07-15 08:18:31,461 - INFO - Current learning rate: 0.0009583441119531051\n",
      "2025-07-15 08:18:32,519 - INFO - Epoch: 2/5, Iter: 114/119 -- train_loss: 1.0414\n",
      "Epoch [2/5]: [113/119]  95%|█████████▍, loss=1.04 [05:23<00:24]\n",
      "Epoch [2/5]: [113/119]  95%|█████████▍, loss=1.04 [05:23<00:24]\n",
      "Epoch [2/5]: [114/119]  96%|█████████▌, loss=1.04 [05:23<00:15]\n",
      "2025-07-15 08:18:32,520 - INFO - Current learning rate: 0.0009568240195746829\n",
      "2025-07-15 08:18:33,541 - INFO - Epoch: 2/5, Iter: 115/119 -- train_loss: 1.0296\n",
      "Epoch [2/5]: [114/119]  96%|█████████▌, loss=1.04 [05:24<00:15]\n",
      "Epoch [2/5]: [114/119]  96%|█████████▌, loss=1.03 [05:24<00:15]\n",
      "Epoch [2/5]: [115/119]  97%|█████████▋, loss=1.03 [05:24<00:10]\n",
      "2025-07-15 08:18:33,542 - INFO - Current learning rate: 0.000955277936692082\n",
      "2025-07-15 08:18:41,200 - INFO - Epoch: 2/5, Iter: 116/119 -- train_loss: 1.0268\n",
      "Epoch [2/5]: [115/119]  97%|█████████▋, loss=1.03 [05:32<00:10]\n",
      "Epoch [2/5]: [115/119]  97%|█████████▋, loss=1.03 [05:32<00:10]\n",
      "Epoch [2/5]: [116/119]  97%|█████████▋, loss=1.03 [05:32<00:12]\n",
      "2025-07-15 08:18:41,201 - INFO - Current learning rate: 0.0009537059512683863\n",
      "2025-07-15 08:18:41,854 - INFO - Epoch: 2/5, Iter: 117/119 -- train_loss: 1.0409\n",
      "Epoch [2/5]: [116/119]  97%|█████████▋, loss=1.03 [05:32<00:12]\n",
      "Epoch [2/5]: [116/119]  97%|█████████▋, loss=1.04 [05:32<00:12]\n",
      "Epoch [2/5]: [117/119]  98%|█████████▊, loss=1.04 [05:32<00:06]\n",
      "2025-07-15 08:18:41,855 - INFO - Current learning rate: 0.0009521081527403829\n",
      "2025-07-15 08:18:42,507 - INFO - Epoch: 2/5, Iter: 118/119 -- train_loss: 1.0406\n",
      "Epoch [2/5]: [117/119]  98%|█████████▊, loss=1.04 [05:33<00:06]\n",
      "Epoch [2/5]: [117/119]  98%|█████████▊, loss=1.04 [05:33<00:06]\n",
      "Epoch [2/5]: [118/119]  99%|█████████▉, loss=1.04 [05:33<00:02]\n",
      "2025-07-15 08:18:42,509 - INFO - Current learning rate: 0.0009504846320134736\n",
      "2025-07-15 08:18:43,161 - INFO - Epoch: 2/5, Iter: 119/119 -- train_loss: 1.0398\n",
      "Epoch [2/5]: [118/119]  99%|█████████▉, loss=1.04 [05:34<00:02]\n",
      "Epoch [2/5]: [118/119]  99%|█████████▉, loss=1.04 [05:34<00:02]\n",
      "Epoch [2/5]: [119/119] 100%|██████████, loss=1.04 [05:34<00:00]\n",
      "2025-07-15 08:18:43,162 - INFO - Current learning rate: 0.0009488354814565037\n",
      "2025-07-15 08:18:43,162 - INFO - Engine run resuming from iteration 0, epoch 1 until 2 epochs\n",
      "[1/20]   5%|#033[32m▌         #033[0m [00:00<?]#033[A\n",
      "Epoch [2/2]: [1/20]   5%|#033[32m▌         #033[0m [00:00<?]#033[A\n",
      "Epoch [2/2]: [1/20]   5%|#033[32m▌         #033[0m [00:09<?]#033[A\n",
      "Epoch [2/2]: [2/20]  10%|#033[32m█         #033[0m [00:09<02:42]#033[A\n",
      "Epoch [2/2]: [2/20]  10%|#033[32m█         #033[0m [00:17<02:42]#033[A\n",
      "Epoch [2/2]: [3/20]  15%|#033[32m█▌        #033[0m [00:17<02:30]#033[A\n",
      "Epoch [2/2]: [3/20]  15%|#033[32m█▌        #033[0m [00:24<02:30]#033[A\n",
      "Epoch [2/2]: [4/20]  20%|#033[32m██        #033[0m [00:24<02:02]#033[A\n",
      "Epoch [2/2]: [4/20]  20%|#033[32m██        #033[0m [00:31<02:02]#033[A\n",
      "Epoch [2/2]: [5/20]  25%|#033[32m██▌       #033[0m [00:31<01:50]#033[A\n",
      "Epoch [2/2]: [5/20]  25%|#033[32m██▌       #033[0m [00:39<01:50]#033[A\n",
      "Epoch [2/2]: [6/20]  30%|#033[32m███       #033[0m [00:39<01:48]#033[A\n",
      "Epoch [2/2]: [6/20]  30%|#033[32m███       #033[0m [00:48<01:48]#033[A\n",
      "Epoch [2/2]: [7/20]  35%|#033[32m███▌      #033[0m [00:48<01:44]#033[A\n",
      "Epoch [2/2]: [7/20]  35%|#033[32m███▌      #033[0m [00:56<01:44]#033[A\n",
      "Epoch [2/2]: [8/20]  40%|#033[32m████      #033[0m [00:56<01:38]#033[A\n",
      "Epoch [2/2]: [8/20]  40%|#033[32m████      #033[0m [01:03<01:38]#033[A\n",
      "Epoch [2/2]: [9/20]  45%|#033[32m████▌     #033[0m [01:03<01:24]#033[A\n",
      "Epoch [2/2]: [9/20]  45%|#033[32m████▌     #033[0m [01:15<01:24]#033[A\n",
      "Epoch [2/2]: [10/20]  50%|#033[32m█████     #033[0m [01:15<01:32]#033[A\n",
      "Epoch [2/2]: [10/20]  50%|#033[32m█████     #033[0m [01:27<01:32]#033[A\n",
      "Epoch [2/2]: [11/20]  55%|#033[32m█████▌    #033[0m [01:27<01:29]#033[A\n",
      "Epoch [2/2]: [11/20]  55%|#033[32m█████▌    #033[0m [01:38<01:29]#033[A\n",
      "Epoch [2/2]: [12/20]  60%|#033[32m██████    #033[0m [01:38<01:21]#033[A\n",
      "Epoch [2/2]: [12/20]  60%|#033[32m██████    #033[0m [01:47<01:21]#033[A\n",
      "Epoch [2/2]: [13/20]  65%|#033[32m██████▌   #033[0m [01:47<01:09]#033[A\n",
      "Epoch [2/2]: [13/20]  65%|#033[32m██████▌   #033[0m [01:55<01:09]#033[A\n",
      "Epoch [2/2]: [14/20]  70%|#033[32m███████   #033[0m [01:55<00:56]#033[A\n",
      "Epoch [2/2]: [14/20]  70%|#033[32m███████   #033[0m [02:00<00:56]#033[A\n",
      "Epoch [2/2]: [15/20]  75%|#033[32m███████▌  #033[0m [02:00<00:40]#033[A\n",
      "Epoch [2/2]: [15/20]  75%|#033[32m███████▌  #033[0m [02:06<00:40]#033[A\n",
      "Epoch [2/2]: [16/20]  80%|#033[32m████████  #033[0m [02:06<00:29]#033[A\n",
      "Epoch [2/2]: [16/20]  80%|#033[32m████████  #033[0m [02:13<00:29]#033[A\n",
      "Epoch [2/2]: [17/20]  85%|#033[32m████████▌ #033[0m [02:13<00:22]#033[A\n",
      "Epoch [2/2]: [17/20]  85%|#033[32m████████▌ #033[0m [02:21<00:22]#033[A\n",
      "Epoch [2/2]: [18/20]  90%|#033[32m█████████ #033[0m [02:21<00:14]#033[A\n",
      "Epoch [2/2]: [18/20]  90%|#033[32m█████████ #033[0m [02:28<00:14]#033[A\n",
      "Epoch [2/2]: [19/20]  95%|#033[32m█████████▌#033[0m [02:28<00:07]#033[A\n",
      "Epoch [2/2]: [19/20]  95%|#033[32m█████████▌#033[0m [02:38<00:07]#033[A\n",
      "Epoch [2/2]: [20/20] 100%|#033[32m██████████#033[0m [02:38<00:00]#033[A\n",
      "#033[A\n",
      "2025-07-15 08:21:34,972 - INFO - Epoch[2] Complete. Time taken: 00:02:51.715\n",
      "2025-07-15 08:21:34,972 - INFO - Engine run finished. Time taken: 00:02:51.810\n",
      "2025-07-15 08:21:35,045 - INFO - Epoch[2] Complete. Time taken: 00:08:35.080\n",
      "2025-07-15 08:21:48,422 - INFO - Epoch: 3/5, Iter: 1/119 -- train_loss: 1.0401\n",
      "[1/119]   1%|           [00:00<?]\n",
      "Epoch [3/5]: [1/119]   1%|           [00:00<?]\n",
      "Epoch [3/5]: [1/119]   1%|          , loss=1.04 [00:00<?]\n",
      "2025-07-15 08:21:48,425 - INFO - Current learning rate: 0.000947160794896505\n",
      "2025-07-15 08:21:49,828 - INFO - Epoch: 3/5, Iter: 2/119 -- train_loss: 1.0366\n",
      "Epoch [3/5]: [1/119]   1%|          , loss=1.04 [00:01<?]\n",
      "Epoch [3/5]: [1/119]   1%|          , loss=1.04 [00:01<?]\n",
      "Epoch [3/5]: [2/119]   2%|▏         , loss=1.04 [00:01<02:44]\n",
      "2025-07-15 08:21:49,830 - INFO - Current learning rate: 0.0009454606676133598\n",
      "2025-07-15 08:21:50,985 - INFO - Epoch: 3/5, Iter: 3/119 -- train_loss: 0.9936\n",
      "Epoch [3/5]: [2/119]   2%|▏         , loss=1.04 [00:02<02:44]\n",
      "Epoch [3/5]: [2/119]   2%|▏         , loss=0.994 [00:02<02:44]\n",
      "Epoch [3/5]: [3/119]   3%|▎         , loss=0.994 [00:02<02:26]\n",
      "2025-07-15 08:21:50,987 - INFO - Current learning rate: 0.0009437351963343782\n",
      "2025-07-15 08:21:52,209 - INFO - Epoch: 3/5, Iter: 4/119 -- train_loss: 1.0085\n",
      "Epoch [3/5]: [3/119]   3%|▎         , loss=0.994 [00:03<02:26]\n",
      "Epoch [3/5]: [3/119]   3%|▎         , loss=1.01 [00:03<02:26]\n",
      "Epoch [3/5]: [4/119]   3%|▎         , loss=1.01 [00:03<02:22]\n",
      "2025-07-15 08:21:52,211 - INFO - Current learning rate: 0.000941984479228796\n",
      "2025-07-15 08:21:56,749 - INFO - Epoch: 3/5, Iter: 5/119 -- train_loss: 1.0308\n",
      "Epoch [3/5]: [4/119]   3%|▎         , loss=1.01 [00:08<02:22]\n",
      "Epoch [3/5]: [4/119]   3%|▎         , loss=1.03 [00:08<02:22]\n",
      "Epoch [3/5]: [5/119]   4%|▍         , loss=1.03 [00:08<04:50]\n",
      "2025-07-15 08:21:56,750 - INFO - Current learning rate: 0.0009402086159021886\n",
      "2025-07-15 08:21:58,007 - INFO - Epoch: 3/5, Iter: 6/119 -- train_loss: 1.0390\n",
      "Epoch [3/5]: [5/119]   4%|▍         , loss=1.03 [00:09<04:50]\n",
      "Epoch [3/5]: [5/119]   4%|▍         , loss=1.04 [00:09<04:50]\n",
      "Epoch [3/5]: [6/119]   5%|▌         , loss=1.04 [00:09<03:55]\n",
      "2025-07-15 08:21:58,008 - INFO - Current learning rate: 0.0009384077073908053\n",
      "2025-07-15 08:21:59,298 - INFO - Epoch: 3/5, Iter: 7/119 -- train_loss: 1.0386\n",
      "Epoch [3/5]: [6/119]   5%|▌         , loss=1.04 [00:10<03:55]\n",
      "Epoch [3/5]: [6/119]   5%|▌         , loss=1.04 [00:10<03:55]\n",
      "Epoch [3/5]: [7/119]   6%|▌         , loss=1.04 [00:10<03:22]\n",
      "2025-07-15 08:21:59,299 - INFO - Current learning rate: 0.0009365818561558189\n",
      "2025-07-15 08:22:03,146 - INFO - Epoch: 3/5, Iter: 8/119 -- train_loss: 1.0025\n",
      "Epoch [3/5]: [7/119]   6%|▌         , loss=1.04 [00:14<03:22]\n",
      "Epoch [3/5]: [7/119]   6%|▌         , loss=1 [00:14<03:22]\n",
      "Epoch [3/5]: [8/119]   7%|▋         , loss=1 [00:14<04:35]\n",
      "2025-07-15 08:22:03,147 - INFO - Current learning rate: 0.0009347311660774986\n",
      "2025-07-15 08:22:06,952 - INFO - Epoch: 3/5, Iter: 9/119 -- train_loss: 1.0301\n",
      "Epoch [3/5]: [8/119]   7%|▋         , loss=1 [00:18<04:35]\n",
      "Epoch [3/5]: [8/119]   7%|▋         , loss=1.03 [00:18<04:35]\n",
      "Epoch [3/5]: [9/119]   8%|▊         , loss=1.03 [00:18<05:19]\n",
      "2025-07-15 08:22:06,954 - INFO - Current learning rate: 0.000932855742449298\n",
      "2025-07-15 08:22:08,361 - INFO - Epoch: 3/5, Iter: 10/119 -- train_loss: 1.0323\n",
      "Epoch [3/5]: [9/119]   8%|▊         , loss=1.03 [00:19<05:19]\n",
      "Epoch [3/5]: [9/119]   8%|▊         , loss=1.03 [00:19<05:19]\n",
      "Epoch [3/5]: [10/119]   8%|▊         , loss=1.03 [00:19<04:25]\n",
      "2025-07-15 08:22:08,362 - INFO - Current learning rate: 0.0009309556919718655\n",
      "2025-07-15 08:22:09,600 - INFO - Epoch: 3/5, Iter: 11/119 -- train_loss: 1.0380\n",
      "Epoch [3/5]: [10/119]   8%|▊         , loss=1.03 [00:21<04:25]\n",
      "Epoch [3/5]: [10/119]   8%|▊         , loss=1.04 [00:21<04:25]\n",
      "Epoch [3/5]: [11/119]   9%|▉         , loss=1.04 [00:21<03:43]\n",
      "2025-07-15 08:22:09,601 - INFO - Current learning rate: 0.0009290311227469733\n",
      "2025-07-15 08:22:12,008 - INFO - Epoch: 3/5, Iter: 12/119 -- train_loss: 1.0296\n",
      "Epoch [3/5]: [11/119]   9%|▉         , loss=1.04 [00:23<03:43]\n",
      "Epoch [3/5]: [11/119]   9%|▉         , loss=1.03 [00:23<03:43]\n",
      "Epoch [3/5]: [12/119]  10%|█         , loss=1.03 [00:23<03:52]\n",
      "2025-07-15 08:22:12,009 - INFO - Current learning rate: 0.0009270821442713667\n",
      "2025-07-15 08:22:17,826 - INFO - Epoch: 3/5, Iter: 13/119 -- train_loss: 1.0371\n",
      "Epoch [3/5]: [12/119]  10%|█         , loss=1.03 [00:29<03:52]\n",
      "Epoch [3/5]: [12/119]  10%|█         , loss=1.04 [00:29<03:52]\n",
      "Epoch [3/5]: [13/119]  11%|█         , loss=1.04 [00:29<05:47]\n",
      "2025-07-15 08:22:17,827 - INFO - Current learning rate: 0.0009251088674305356\n",
      "2025-07-15 08:22:19,206 - INFO - Epoch: 3/5, Iter: 14/119 -- train_loss: 1.0373\n",
      "Epoch [3/5]: [13/119]  11%|█         , loss=1.04 [00:30<05:47]\n",
      "Epoch [3/5]: [13/119]  11%|█         , loss=1.04 [00:30<05:47]\n",
      "Epoch [3/5]: [14/119]  12%|█▏        , loss=1.04 [00:30<04:43]\n",
      "2025-07-15 08:22:19,207 - INFO - Current learning rate: 0.000923111404492404\n",
      "2025-07-15 08:22:20,615 - INFO - Epoch: 3/5, Iter: 15/119 -- train_loss: 1.0205\n",
      "Epoch [3/5]: [14/119]  12%|█▏        , loss=1.04 [00:32<04:43]\n",
      "Epoch [3/5]: [14/119]  12%|█▏        , loss=1.02 [00:32<04:43]\n",
      "Epoch [3/5]: [15/119]  13%|█▎        , loss=1.02 [00:32<04:00]\n",
      "2025-07-15 08:22:20,616 - INFO - Current learning rate: 0.0009210898691009443\n",
      "2025-07-15 08:22:21,788 - INFO - Epoch: 3/5, Iter: 16/119 -- train_loss: 1.0370\n",
      "Epoch [3/5]: [15/119]  13%|█▎        , loss=1.02 [00:33<04:00]\n",
      "Epoch [3/5]: [15/119]  13%|█▎        , loss=1.04 [00:33<04:00]\n",
      "Epoch [3/5]: [16/119]  13%|█▎        , loss=1.04 [00:33<03:22]\n",
      "2025-07-15 08:22:21,789 - INFO - Current learning rate: 0.0009190443762697104\n",
      "2025-07-15 08:22:27,914 - INFO - Epoch: 3/5, Iter: 17/119 -- train_loss: 1.0164\n",
      "Epoch [3/5]: [16/119]  13%|█▎        , loss=1.04 [00:39<03:22]\n",
      "Epoch [3/5]: [16/119]  13%|█▎        , loss=1.02 [00:39<03:22]\n",
      "Epoch [3/5]: [17/119]  14%|█▍        , loss=1.02 [00:39<05:28]\n",
      "2025-07-15 08:22:27,916 - INFO - Current learning rate: 0.0009169750423752949\n",
      "2025-07-15 08:22:29,340 - INFO - Epoch: 3/5, Iter: 18/119 -- train_loss: 1.0368\n",
      "Epoch [3/5]: [17/119]  14%|█▍        , loss=1.02 [00:40<05:28]\n",
      "Epoch [3/5]: [17/119]  14%|█▍        , loss=1.04 [00:40<05:28]\n",
      "Epoch [3/5]: [18/119]  15%|█▌        , loss=1.04 [00:40<04:30]\n",
      "2025-07-15 08:22:29,342 - INFO - Current learning rate: 0.0009148819851507071\n",
      "2025-07-15 08:22:30,570 - INFO - Epoch: 3/5, Iter: 19/119 -- train_loss: 1.0364\n",
      "Epoch [3/5]: [18/119]  15%|█▌        , loss=1.04 [00:42<04:30]\n",
      "Epoch [3/5]: [18/119]  15%|█▌        , loss=1.04 [00:42<04:30]\n",
      "Epoch [3/5]: [19/119]  16%|█▌        , loss=1.04 [00:42<03:44]\n",
      "2025-07-15 08:22:30,572 - INFO - Current learning rate: 0.0009127653236786758\n",
      "2025-07-15 08:22:31,665 - INFO - Epoch: 3/5, Iter: 20/119 -- train_loss: 1.0349\n",
      "Epoch [3/5]: [19/119]  16%|█▌        , loss=1.04 [00:43<03:44]\n",
      "Epoch [3/5]: [19/119]  16%|█▌        , loss=1.03 [00:43<03:44]\n",
      "Epoch [3/5]: [20/119]  17%|█▋        , loss=1.03 [00:43<03:08]\n",
      "2025-07-15 08:22:31,667 - INFO - Current learning rate: 0.0009106251783848731\n",
      "2025-07-15 08:22:38,084 - INFO - Epoch: 3/5, Iter: 21/119 -- train_loss: 1.0356\n",
      "Epoch [3/5]: [20/119]  17%|█▋        , loss=1.03 [00:49<03:08]\n",
      "Epoch [3/5]: [20/119]  17%|█▋        , loss=1.04 [00:49<03:08]\n",
      "Epoch [3/5]: [21/119]  18%|█▊        , loss=1.04 [00:49<05:19]\n",
      "2025-07-15 08:22:38,086 - INFO - Current learning rate: 0.0009084616710310638\n",
      "2025-07-15 08:22:40,592 - INFO - Epoch: 3/5, Iter: 22/119 -- train_loss: 1.0203\n",
      "Epoch [3/5]: [21/119]  18%|█▊        , loss=1.04 [00:52<05:19]\n",
      "Epoch [3/5]: [21/119]  18%|█▊        , loss=1.02 [00:52<05:19]\n",
      "Epoch [3/5]: [22/119]  18%|█▊        , loss=1.02 [00:52<04:54]\n",
      "2025-07-15 08:22:40,594 - INFO - Current learning rate: 0.0009062749247081771\n",
      "2025-07-15 08:22:41,766 - INFO - Epoch: 3/5, Iter: 23/119 -- train_loss: 1.0185\n",
      "Epoch [3/5]: [22/119]  18%|█▊        , loss=1.02 [00:53<04:54]\n",
      "Epoch [3/5]: [22/119]  18%|█▊        , loss=1.02 [00:53<04:54]\n",
      "Epoch [3/5]: [23/119]  19%|█▉        , loss=1.02 [00:53<03:57]\n",
      "2025-07-15 08:22:41,767 - INFO - Current learning rate: 0.0009040650638293038\n",
      "2025-07-15 08:22:42,884 - INFO - Epoch: 3/5, Iter: 24/119 -- train_loss: 0.9985\n",
      "Epoch [3/5]: [23/119]  19%|█▉        , loss=1.02 [00:54<03:57]\n",
      "Epoch [3/5]: [23/119]  19%|█▉        , loss=0.999 [00:54<03:57]\n",
      "Epoch [3/5]: [24/119]  20%|██        , loss=0.999 [00:54<03:16]\n",
      "2025-07-15 08:22:42,886 - INFO - Current learning rate: 0.0009018322141226183\n",
      "2025-07-15 08:22:51,553 - INFO - Epoch: 3/5, Iter: 25/119 -- train_loss: 1.0350\n",
      "Epoch [3/5]: [24/119]  20%|██        , loss=0.999 [01:03<03:16]\n",
      "Epoch [3/5]: [24/119]  20%|██        , loss=1.04 [01:03<03:16]\n",
      "Epoch [3/5]: [25/119]  21%|██        , loss=1.04 [01:03<06:20]\n",
      "2025-07-15 08:22:51,554 - INFO - Current learning rate: 0.0008995765026242245\n",
      "2025-07-15 08:22:52,720 - INFO - Epoch: 3/5, Iter: 26/119 -- train_loss: 1.0357\n",
      "Epoch [3/5]: [25/119]  21%|██        , loss=1.04 [01:04<06:20]\n",
      "Epoch [3/5]: [25/119]  21%|██        , loss=1.04 [01:04<06:20]\n",
      "Epoch [3/5]: [26/119]  22%|██▏       , loss=1.04 [01:04<04:56]\n",
      "2025-07-15 08:22:52,721 - INFO - Current learning rate: 0.0008972980576709286\n",
      "2025-07-15 08:22:54,151 - INFO - Epoch: 3/5, Iter: 27/119 -- train_loss: 1.0350\n",
      "Epoch [3/5]: [26/119]  22%|██▏       , loss=1.04 [01:05<04:56]\n",
      "Epoch [3/5]: [26/119]  22%|██▏       , loss=1.03 [01:05<04:56]\n",
      "Epoch [3/5]: [27/119]  23%|██▎       , loss=1.03 [01:05<04:04]\n",
      "2025-07-15 08:22:54,152 - INFO - Current learning rate: 0.0008949970088929384\n",
      "2025-07-15 08:22:55,672 - INFO - Epoch: 3/5, Iter: 28/119 -- train_loss: 1.0154\n",
      "Epoch [3/5]: [27/119]  23%|██▎       , loss=1.03 [01:07<04:04]\n",
      "Epoch [3/5]: [27/119]  23%|██▎       , loss=1.02 [01:07<04:04]\n",
      "Epoch [3/5]: [28/119]  24%|██▎       , loss=1.02 [01:07<03:30]\n",
      "2025-07-15 08:22:55,674 - INFO - Current learning rate: 0.0008926734872064864\n",
      "2025-07-15 08:23:05,771 - INFO - Epoch: 3/5, Iter: 29/119 -- train_loss: 1.0289\n",
      "Epoch [3/5]: [28/119]  24%|██▎       , loss=1.02 [01:17<03:30]\n",
      "Epoch [3/5]: [28/119]  24%|██▎       , loss=1.03 [01:17<03:30]\n",
      "Epoch [3/5]: [29/119]  24%|██▍       , loss=1.03 [01:17<06:58]\n",
      "2025-07-15 08:23:05,772 - INFO - Current learning rate: 0.0008903276248063826\n",
      "2025-07-15 08:23:07,123 - INFO - Epoch: 3/5, Iter: 30/119 -- train_loss: 1.0139\n",
      "Epoch [3/5]: [29/119]  24%|██▍       , loss=1.03 [01:18<06:58]\n",
      "Epoch [3/5]: [29/119]  24%|██▍       , loss=1.01 [01:18<06:58]\n",
      "Epoch [3/5]: [30/119]  25%|██▌       , loss=1.01 [01:18<05:25]\n",
      "2025-07-15 08:23:07,125 - INFO - Current learning rate: 0.0008879595551584934\n",
      "2025-07-15 08:23:08,402 - INFO - Epoch: 3/5, Iter: 31/119 -- train_loss: 1.0206\n",
      "Epoch [3/5]: [30/119]  25%|██▌       , loss=1.01 [01:19<05:25]\n",
      "Epoch [3/5]: [30/119]  25%|██▌       , loss=1.02 [01:19<05:25]\n",
      "Epoch [3/5]: [31/119]  26%|██▌       , loss=1.02 [01:19<04:19]\n",
      "2025-07-15 08:23:08,404 - INFO - Current learning rate: 0.0008855694129921473\n",
      "2025-07-15 08:23:09,827 - INFO - Epoch: 3/5, Iter: 32/119 -- train_loss: 1.0341\n",
      "Epoch [3/5]: [31/119]  26%|██▌       , loss=1.02 [01:21<04:19]\n",
      "Epoch [3/5]: [31/119]  26%|██▌       , loss=1.03 [01:21<04:19]\n",
      "Epoch [3/5]: [32/119]  27%|██▋       , loss=1.03 [01:21<03:36]\n",
      "2025-07-15 08:23:09,829 - INFO - Current learning rate: 0.0008831573342924707\n",
      "2025-07-15 08:23:14,737 - INFO - Epoch: 3/5, Iter: 33/119 -- train_loss: 1.0337\n",
      "Epoch [3/5]: [32/119]  27%|██▋       , loss=1.03 [01:26<03:36]\n",
      "Epoch [3/5]: [32/119]  27%|██▋       , loss=1.03 [01:26<03:36]\n",
      "Epoch [3/5]: [33/119]  28%|██▊       , loss=1.03 [01:26<04:36]\n",
      "2025-07-15 08:23:14,738 - INFO - Current learning rate: 0.0008807234562926504\n",
      "2025-07-15 08:23:15,950 - INFO - Epoch: 3/5, Iter: 34/119 -- train_loss: 1.0186\n",
      "Epoch [3/5]: [33/119]  28%|██▊       , loss=1.03 [01:27<04:36]\n",
      "Epoch [3/5]: [33/119]  28%|██▊       , loss=1.02 [01:27<04:36]\n",
      "Epoch [3/5]: [34/119]  29%|██▊       , loss=1.02 [01:27<03:42]2025-07-15 08:23:15,951 - INFO - Current learning rate: 0.0008782679174661258\n",
      "2025-07-15 08:23:17,205 - INFO - Epoch: 3/5, Iter: 35/119 -- train_loss: 1.0177\n",
      "Epoch [3/5]: [34/119]  29%|██▊       , loss=1.02 [01:28<03:42]\n",
      "Epoch [3/5]: [34/119]  29%|██▊       , loss=1.02 [01:28<03:42]\n",
      "Epoch [3/5]: [35/119]  29%|██▉       , loss=1.02 [01:28<03:05]\n",
      "2025-07-15 08:23:17,206 - INFO - Current learning rate: 0.0008757908575187108\n",
      "2025-07-15 08:23:18,408 - INFO - Epoch: 3/5, Iter: 36/119 -- train_loss: 1.0335\n",
      "Epoch [3/5]: [35/119]  29%|██▉       , loss=1.02 [01:29<03:05]\n",
      "Epoch [3/5]: [35/119]  29%|██▉       , loss=1.03 [01:29<03:05]\n",
      "Epoch [3/5]: [36/119]  30%|███       , loss=1.03 [01:29<02:38]\n",
      "2025-07-15 08:23:18,409 - INFO - Current learning rate: 0.0008732924173806459\n",
      "2025-07-15 08:23:22,921 - INFO - Epoch: 3/5, Iter: 37/119 -- train_loss: 1.0332\n",
      "Epoch [3/5]: [36/119]  30%|███       , loss=1.03 [01:34<02:38]\n",
      "Epoch [3/5]: [36/119]  30%|███       , loss=1.03 [01:34<02:38]\n",
      "Epoch [3/5]: [37/119]  31%|███       , loss=1.03 [01:34<03:40]\n",
      "2025-07-15 08:23:22,923 - INFO - Current learning rate: 0.000870772739198579\n",
      "2025-07-15 08:23:24,201 - INFO - Epoch: 3/5, Iter: 38/119 -- train_loss: 1.0334\n",
      "Epoch [3/5]: [37/119]  31%|███       , loss=1.03 [01:35<03:40]\n",
      "Epoch [3/5]: [37/119]  31%|███       , loss=1.03 [01:35<03:40]\n",
      "Epoch [3/5]: [38/119]  32%|███▏      , loss=1.03 [01:35<03:03]\n",
      "2025-07-15 08:23:24,202 - INFO - Current learning rate: 0.0008682319663274788\n",
      "2025-07-15 08:23:25,498 - INFO - Epoch: 3/5, Iter: 39/119 -- train_loss: 1.0333\n",
      "Epoch [3/5]: [38/119]  32%|███▏      , loss=1.03 [01:37<03:03]\n",
      "Epoch [3/5]: [38/119]  32%|███▏      , loss=1.03 [01:37<03:03]\n",
      "Epoch [3/5]: [39/119]  33%|███▎      , loss=1.03 [01:37<02:38]\n",
      "2025-07-15 08:23:25,499 - INFO - Current learning rate: 0.0008656702433224786\n",
      "2025-07-15 08:23:27,185 - INFO - Epoch: 3/5, Iter: 40/119 -- train_loss: 1.0327\n",
      "Epoch [3/5]: [39/119]  33%|███▎      , loss=1.03 [01:38<02:38]\n",
      "Epoch [3/5]: [39/119]  33%|███▎      , loss=1.03 [01:38<02:38]\n",
      "Epoch [3/5]: [40/119]  34%|███▎      , loss=1.03 [01:38<02:29]\n",
      "2025-07-15 08:23:27,186 - INFO - Current learning rate: 0.0008630877159306519\n",
      "2025-07-15 08:23:32,706 - INFO - Epoch: 3/5, Iter: 41/119 -- train_loss: 1.0293\n",
      "Epoch [3/5]: [40/119]  34%|███▎      , loss=1.03 [01:44<02:29]\n",
      "Epoch [3/5]: [40/119]  34%|███▎      , loss=1.03 [01:44<02:29]\n",
      "Epoch [3/5]: [41/119]  34%|███▍      , loss=1.03 [01:44<03:52]\n",
      "2025-07-15 08:23:32,707 - INFO - Current learning rate: 0.0008604845310827203\n",
      "2025-07-15 08:23:33,881 - INFO - Epoch: 3/5, Iter: 42/119 -- train_loss: 1.0330\n",
      "Epoch [3/5]: [41/119]  34%|███▍      , loss=1.03 [01:45<03:52]\n",
      "Epoch [3/5]: [41/119]  34%|███▍      , loss=1.03 [01:45<03:52]\n",
      "Epoch [3/5]: [42/119]  35%|███▌      , loss=1.03 [01:45<03:07]\n",
      "2025-07-15 08:23:33,883 - INFO - Current learning rate: 0.0008578608368846938\n",
      "2025-07-15 08:23:35,039 - INFO - Epoch: 3/5, Iter: 43/119 -- train_loss: 1.0328\n",
      "Epoch [3/5]: [42/119]  35%|███▌      , loss=1.03 [01:46<03:07]\n",
      "Epoch [3/5]: [42/119]  35%|███▌      , loss=1.03 [01:46<03:07]\n",
      "Epoch [3/5]: [43/119]  36%|███▌      , loss=1.03 [01:46<02:36]\n",
      "2025-07-15 08:23:35,041 - INFO - Current learning rate: 0.0008552167826094451\n",
      "2025-07-15 08:23:46,140 - INFO - Epoch: 3/5, Iter: 44/119 -- train_loss: 1.0330\n",
      "Epoch [3/5]: [43/119]  36%|███▌      , loss=1.03 [01:57<02:36]\n",
      "Epoch [3/5]: [43/119]  36%|███▌      , loss=1.03 [01:57<02:36]\n",
      "Epoch [3/5]: [44/119]  37%|███▋      , loss=1.03 [01:57<05:57]\n",
      "2025-07-15 08:23:46,141 - INFO - Current learning rate: 0.0008525525186882157\n",
      "2025-07-15 08:23:47,558 - INFO - Epoch: 3/5, Iter: 45/119 -- train_loss: 1.0325\n",
      "Epoch [3/5]: [44/119]  37%|███▋      , loss=1.03 [01:59<05:57]\n",
      "Epoch [3/5]: [44/119]  37%|███▋      , loss=1.03 [01:59<05:57]\n",
      "Epoch [3/5]: [45/119]  38%|███▊      , loss=1.03 [01:59<04:38]\n",
      "2025-07-15 08:23:47,560 - INFO - Current learning rate: 0.0008498681967020581\n",
      "2025-07-15 08:23:48,694 - INFO - Epoch: 3/5, Iter: 46/119 -- train_loss: 1.0254\n",
      "Epoch [3/5]: [45/119]  38%|███▊      , loss=1.03 [02:00<04:38]\n",
      "Epoch [3/5]: [45/119]  38%|███▊      , loss=1.03 [02:00<04:38]\n",
      "Epoch [3/5]: [46/119]  39%|███▊      , loss=1.03 [02:00<03:37]\n",
      "2025-07-15 08:23:48,696 - INFO - Current learning rate: 0.0008471639693732121\n",
      "2025-07-15 08:23:49,882 - INFO - Epoch: 3/5, Iter: 47/119 -- train_loss: 1.0300\n",
      "Epoch [3/5]: [46/119]  39%|███▊      , loss=1.03 [02:01<03:37]\n",
      "Epoch [3/5]: [46/119]  39%|███▊      , loss=1.03 [02:01<03:37]\n",
      "Epoch [3/5]: [47/119]  39%|███▉      , loss=1.03 [02:01<02:55]\n",
      "2025-07-15 08:23:49,883 - INFO - Current learning rate: 0.0008444399905564142\n",
      "2025-07-15 08:24:01,383 - INFO - Epoch: 3/5, Iter: 48/119 -- train_loss: 1.0318\n",
      "Epoch [3/5]: [47/119]  39%|███▉      , loss=1.03 [02:12<02:55]\n",
      "Epoch [3/5]: [47/119]  39%|███▉      , loss=1.03 [02:12<02:55]\n",
      "Epoch [3/5]: [48/119]  40%|████      , loss=1.03 [02:12<06:06]\n",
      "2025-07-15 08:24:01,384 - INFO - Current learning rate: 0.0008416964152301461\n",
      "2025-07-15 08:24:02,717 - INFO - Epoch: 3/5, Iter: 49/119 -- train_loss: 1.0315\n",
      "Epoch [3/5]: [48/119]  40%|████      , loss=1.03 [02:14<06:06]\n",
      "Epoch [3/5]: [48/119]  40%|████      , loss=1.03 [02:14<06:06]\n",
      "Epoch [3/5]: [49/119]  41%|████      , loss=1.03 [02:14<04:40]\n",
      "2025-07-15 08:24:02,718 - INFO - Current learning rate: 0.0008389333994878158\n",
      "2025-07-15 08:24:04,201 - INFO - Epoch: 3/5, Iter: 50/119 -- train_loss: 1.0313\n",
      "Epoch [3/5]: [49/119]  41%|████      , loss=1.03 [02:15<04:40]\n",
      "Epoch [3/5]: [49/119]  41%|████      , loss=1.03 [02:15<04:40]\n",
      "Epoch [3/5]: [50/119]  42%|████▏     , loss=1.03 [02:15<03:44]\n",
      "2025-07-15 08:24:04,202 - INFO - Current learning rate: 0.0008361511005288778\n",
      "2025-07-15 08:24:05,562 - INFO - Epoch: 3/5, Iter: 51/119 -- train_loss: 1.0292\n",
      "Epoch [3/5]: [50/119]  42%|████▏     , loss=1.03 [02:17<03:44]\n",
      "Epoch [3/5]: [50/119]  42%|████▏     , loss=1.03 [02:17<03:44]\n",
      "Epoch [3/5]: [51/119]  43%|████▎     , loss=1.03 [02:17<03:02]\n",
      "2025-07-15 08:24:05,564 - INFO - Current learning rate: 0.0008333496766498886\n",
      "2025-07-15 08:24:17,062 - INFO - Epoch: 3/5, Iter: 52/119 -- train_loss: 1.0310\n",
      "Epoch [3/5]: [51/119]  43%|████▎     , loss=1.03 [02:28<03:02]\n",
      "Epoch [3/5]: [51/119]  43%|████▎     , loss=1.03 [02:28<03:02]\n",
      "Epoch [3/5]: [52/119]  44%|████▎     , loss=1.03 [02:28<05:57]\n",
      "2025-07-15 08:24:17,064 - INFO - Current learning rate: 0.0008305292872355011\n",
      "2025-07-15 08:24:18,218 - INFO - Epoch: 3/5, Iter: 53/119 -- train_loss: 1.0309\n",
      "Epoch [3/5]: [52/119]  44%|████▎     , loss=1.03 [02:29<05:57]\n",
      "Epoch [3/5]: [52/119]  44%|████▎     , loss=1.03 [02:29<05:57]\n",
      "Epoch [3/5]: [53/119]  45%|████▍     , loss=1.03 [02:29<04:29]\n",
      "2025-07-15 08:24:18,219 - INFO - Current learning rate: 0.0008276900927493964\n",
      "2025-07-15 08:24:19,446 - INFO - Epoch: 3/5, Iter: 54/119 -- train_loss: 1.0279\n",
      "Epoch [3/5]: [53/119]  45%|████▍     , loss=1.03 [02:31<04:29]\n",
      "Epoch [3/5]: [53/119]  45%|████▍     , loss=1.03 [02:31<04:29]\n",
      "Epoch [3/5]: [54/119]  45%|████▌     , loss=1.03 [02:31<03:29]\n",
      "2025-07-15 08:24:19,447 - INFO - Current learning rate: 0.0008248322547251544\n",
      "2025-07-15 08:24:21,025 - INFO - Epoch: 3/5, Iter: 55/119 -- train_loss: 1.0308\n",
      "Epoch [3/5]: [54/119]  45%|████▌     , loss=1.03 [02:32<03:29]\n",
      "Epoch [3/5]: [54/119]  45%|████▌     , loss=1.03 [02:32<03:29]\n",
      "Epoch [3/5]: [55/119]  46%|████▌     , loss=1.03 [02:32<02:54]\n",
      "2025-07-15 08:24:21,027 - INFO - Current learning rate: 0.0008219559357570634\n",
      "2025-07-15 08:24:26,468 - INFO - Epoch: 3/5, Iter: 56/119 -- train_loss: 1.0304\n",
      "Epoch [3/5]: [55/119]  46%|████▌     , loss=1.03 [02:38<02:54]\n",
      "Epoch [3/5]: [55/119]  46%|████▌     , loss=1.03 [02:38<02:54]\n",
      "Epoch [3/5]: [56/119]  47%|████▋     , loss=1.03 [02:38<03:43]\n",
      "2025-07-15 08:24:26,470 - INFO - Current learning rate: 0.0008190612994908691\n",
      "2025-07-15 08:24:27,739 - INFO - Epoch: 3/5, Iter: 57/119 -- train_loss: 1.0299\n",
      "Epoch [3/5]: [56/119]  47%|████▋     , loss=1.03 [02:39<03:43]\n",
      "Epoch [3/5]: [56/119]  47%|████▋     , loss=1.03 [02:39<03:43]\n",
      "Epoch [3/5]: [57/119]  48%|████▊     , loss=1.03 [02:39<02:57]\n",
      "2025-07-15 08:24:27,740 - INFO - Current learning rate: 0.000816148510614465\n",
      "2025-07-15 08:24:29,134 - INFO - Epoch: 3/5, Iter: 58/119 -- train_loss: 1.0211\n",
      "Epoch [3/5]: [57/119]  48%|████▊     , loss=1.03 [02:40<02:57]\n",
      "Epoch [3/5]: [57/119]  48%|████▊     , loss=1.02 [02:40<02:57]\n",
      "Epoch [3/5]: [58/119]  49%|████▊     , loss=1.02 [02:40<02:27]\n",
      "2025-07-15 08:24:29,135 - INFO - Current learning rate: 0.0008132177348485214\n",
      "2025-07-15 08:24:30,476 - INFO - Epoch: 3/5, Iter: 59/119 -- train_loss: 1.0302\n",
      "Epoch [3/5]: [58/119]  49%|████▊     , loss=1.02 [02:42<02:27]\n",
      "Epoch [3/5]: [58/119]  49%|████▊     , loss=1.03 [02:42<02:27]\n",
      "Epoch [3/5]: [59/119]  50%|████▉     , loss=1.03 [02:42<02:05]\n",
      "2025-07-15 08:24:30,477 - INFO - Current learning rate: 0.0008102691389370582\n",
      "2025-07-15 08:24:39,425 - INFO - Epoch: 3/5, Iter: 60/119 -- train_loss: 1.0275\n",
      "Epoch [3/5]: [59/119]  50%|████▉     , loss=1.03 [02:51<02:05]\n",
      "Epoch [3/5]: [59/119]  50%|████▉     , loss=1.03 [02:51<02:05]\n",
      "Epoch [3/5]: [60/119]  50%|█████     , loss=1.03 [02:51<04:05]\n",
      "2025-07-15 08:24:39,426 - INFO - Current learning rate: 0.000807302890637957\n",
      "2025-07-15 08:24:40,648 - INFO - Epoch: 3/5, Iter: 61/119 -- train_loss: 1.0283\n",
      "Epoch [3/5]: [60/119]  50%|█████     , loss=1.03 [02:52<04:05]\n",
      "Epoch [3/5]: [60/119]  50%|█████     , loss=1.03 [02:52<04:05]\n",
      "Epoch [3/5]: [61/119]  51%|█████▏    , loss=1.03 [02:52<03:09]\n",
      "2025-07-15 08:24:40,649 - INFO - Current learning rate: 0.0008043191587134176\n",
      "2025-07-15 08:24:41,992 - INFO - Epoch: 3/5, Iter: 62/119 -- train_loss: 1.0300\n",
      "Epoch [3/5]: [61/119]  51%|█████▏    , loss=1.03 [02:53<03:09]\n",
      "Epoch [3/5]: [61/119]  51%|█████▏    , loss=1.03 [02:53<03:09]#015Epoch [3/5]: [62/119]  52%|█████▏    , loss=1.03 [02:53<02:33]2025-07-15 08:24:41,993 - INFO - Current learning rate: 0.0008013181129203554\n",
      "2025-07-15 08:24:43,425 - INFO - Epoch: 3/5, Iter: 63/119 -- train_loss: 0.9901\n",
      "Epoch [3/5]: [62/119]  52%|█████▏    , loss=1.03 [02:55<02:33]\n",
      "Epoch [3/5]: [62/119]  52%|█████▏    , loss=0.99 [02:55<02:33]\n",
      "Epoch [3/5]: [63/119]  53%|█████▎    , loss=0.99 [02:55<02:09]\n",
      "2025-07-15 08:24:43,427 - INFO - Current learning rate: 0.000798299924000744\n",
      "2025-07-15 08:24:51,502 - INFO - Epoch: 3/5, Iter: 64/119 -- train_loss: 1.0280\n",
      "Epoch [3/5]: [63/119]  53%|█████▎    , loss=0.99 [03:03<02:09]\n",
      "Epoch [3/5]: [63/119]  53%|█████▎    , loss=1.03 [03:03<02:09]\n",
      "Epoch [3/5]: [64/119]  54%|█████▍    , loss=1.03 [03:03<03:42]\n",
      "2025-07-15 08:24:51,503 - INFO - Current learning rate: 0.0007952647636719012\n",
      "2025-07-15 08:24:52,647 - INFO - Epoch: 3/5, Iter: 65/119 -- train_loss: 1.0135\n",
      "Epoch [3/5]: [64/119]  54%|█████▍    , loss=1.03 [03:04<03:42]\n",
      "Epoch [3/5]: [64/119]  54%|█████▍    , loss=1.01 [03:04<03:42]\n",
      "Epoch [3/5]: [65/119]  55%|█████▍    , loss=1.01 [03:04<02:51]\n",
      "2025-07-15 08:24:52,648 - INFO - Current learning rate: 0.000792212804616718\n",
      "2025-07-15 08:24:54,043 - INFO - Epoch: 3/5, Iter: 66/119 -- train_loss: 1.0300\n",
      "Epoch [3/5]: [65/119]  55%|█████▍    , loss=1.01 [03:05<02:51]\n",
      "Epoch [3/5]: [65/119]  55%|█████▍    , loss=1.03 [03:05<02:51]\n",
      "Epoch [3/5]: [66/119]  55%|█████▌    , loss=1.03 [03:05<02:19]\n",
      "2025-07-15 08:24:54,045 - INFO - Current learning rate: 0.0007891442204738353\n",
      "2025-07-15 08:24:55,204 - INFO - Epoch: 3/5, Iter: 67/119 -- train_loss: 1.0213\n",
      "Epoch [3/5]: [66/119]  55%|█████▌    , loss=1.03 [03:06<02:19]\n",
      "Epoch [3/5]: [66/119]  55%|█████▌    , loss=1.02 [03:06<02:19]\n",
      "Epoch [3/5]: [67/119]  56%|█████▋    , loss=1.02 [03:06<01:54]\n",
      "2025-07-15 08:24:55,206 - INFO - Current learning rate: 0.0007860591858277645\n",
      "2025-07-15 08:25:01,534 - INFO - Epoch: 3/5, Iter: 68/119 -- train_loss: 1.0227\n",
      "Epoch [3/5]: [67/119]  56%|█████▋    , loss=1.02 [03:13<01:54]\n",
      "Epoch [3/5]: [67/119]  56%|█████▋    , loss=1.02 [03:13<01:54]\n",
      "Epoch [3/5]: [68/119]  57%|█████▋    , loss=1.02 [03:13<02:55]\n",
      "2025-07-15 08:25:01,535 - INFO - Current learning rate: 0.0007829578761989541\n",
      "2025-07-15 08:25:02,750 - INFO - Epoch: 3/5, Iter: 69/119 -- train_loss: 1.0290\n",
      "Epoch [3/5]: [68/119]  57%|█████▋    , loss=1.02 [03:14<02:55]\n",
      "Epoch [3/5]: [68/119]  57%|█████▋    , loss=1.03 [03:14<02:55]\n",
      "Epoch [3/5]: [69/119]  58%|█████▊    , loss=1.03 [03:14<02:18]\n",
      "2025-07-15 08:25:02,751 - INFO - Current learning rate: 0.0007798404680338045\n",
      "2025-07-15 08:25:04,077 - INFO - Epoch: 3/5, Iter: 70/119 -- train_loss: 1.0037\n",
      "Epoch [3/5]: [69/119]  58%|█████▊    , loss=1.03 [03:15<02:18]\n",
      "Epoch [3/5]: [69/119]  58%|█████▊    , loss=1 [03:15<02:18]\n",
      "Epoch [3/5]: [70/119]  59%|█████▉    , loss=1 [03:15<01:54]\n",
      "2025-07-15 08:25:04,080 - INFO - Current learning rate: 0.0007767071386946281\n",
      "2025-07-15 08:25:05,950 - INFO - Epoch: 3/5, Iter: 71/119 -- train_loss: 1.0288\n",
      "Epoch [3/5]: [70/119]  59%|█████▉    , loss=1 [03:17<01:54]\n",
      "Epoch [3/5]: [70/119]  59%|█████▉    , loss=1.03 [03:17<01:54]\n",
      "Epoch [3/5]: [71/119]  60%|█████▉    , loss=1.03 [03:17<01:45]\n",
      "2025-07-15 08:25:05,951 - INFO - Current learning rate: 0.0007735580664495601\n",
      "2025-07-15 08:25:13,543 - INFO - Epoch: 3/5, Iter: 72/119 -- train_loss: 1.0286\n",
      "Epoch [3/5]: [71/119]  60%|█████▉    , loss=1.03 [03:25<01:45]\n",
      "Epoch [3/5]: [71/119]  60%|█████▉    , loss=1.03 [03:25<01:45]\n",
      "Epoch [3/5]: [72/119]  61%|██████    , loss=1.03 [03:25<02:59]\n",
      "2025-07-15 08:25:13,545 - INFO - Current learning rate: 0.000770393430462414\n",
      "2025-07-15 08:25:14,964 - INFO - Epoch: 3/5, Iter: 73/119 -- train_loss: 1.0257\n",
      "Epoch [3/5]: [72/119]  61%|██████    , loss=1.03 [03:26<02:59]\n",
      "Epoch [3/5]: [72/119]  61%|██████    , loss=1.03 [03:26<02:59]\n",
      "Epoch [3/5]: [73/119]  61%|██████▏   , loss=1.03 [03:26<02:22]\n",
      "2025-07-15 08:25:14,966 - INFO - Current learning rate: 0.00076721341078249\n",
      "2025-07-15 08:25:16,552 - INFO - Epoch: 3/5, Iter: 74/119 -- train_loss: 1.0214\n",
      "Epoch [3/5]: [73/119]  61%|██████▏   , loss=1.03 [03:28<02:22]\n",
      "Epoch [3/5]: [73/119]  61%|██████▏   , loss=1.02 [03:28<02:22]\n",
      "Epoch [3/5]: [74/119]  62%|██████▏   , loss=1.02 [03:28<01:59]\n",
      "2025-07-15 08:25:16,553 - INFO - Current learning rate: 0.0007640181883343305\n",
      "2025-07-15 08:25:18,040 - INFO - Epoch: 3/5, Iter: 75/119 -- train_loss: 1.0278\n",
      "Epoch [3/5]: [74/119]  62%|██████▏   , loss=1.02 [03:29<01:59]\n",
      "Epoch [3/5]: [74/119]  62%|██████▏   , loss=1.03 [03:29<01:59]\n",
      "Epoch [3/5]: [75/119]  63%|██████▎   , loss=1.03 [03:29<01:41]\n",
      "2025-07-15 08:25:18,042 - INFO - Current learning rate: 0.0007608079449074262\n",
      "2025-07-15 08:25:23,270 - INFO - Epoch: 3/5, Iter: 76/119 -- train_loss: 1.0222\n",
      "Epoch [3/5]: [75/119]  63%|██████▎   , loss=1.03 [03:34<01:41]\n",
      "Epoch [3/5]: [75/119]  63%|██████▎   , loss=1.02 [03:34<01:41]\n",
      "Epoch [3/5]: [76/119]  64%|██████▍   , loss=1.02 [03:34<02:16]\n",
      "2025-07-15 08:25:23,271 - INFO - Current learning rate: 0.000757582863145874\n",
      "2025-07-15 08:25:24,392 - INFO - Epoch: 3/5, Iter: 77/119 -- train_loss: 1.0278\n",
      "Epoch [3/5]: [76/119]  64%|██████▍   , loss=1.02 [03:35<02:16]\n",
      "Epoch [3/5]: [76/119]  64%|██████▍   , loss=1.03 [03:35<02:16]\n",
      "Epoch [3/5]: [77/119]  65%|██████▍   , loss=1.03 [03:35<01:47]\n",
      "2025-07-15 08:25:24,393 - INFO - Current learning rate: 0.0007543431265379855\n",
      "2025-07-15 08:25:25,629 - INFO - Epoch: 3/5, Iter: 78/119 -- train_loss: 1.0201\n",
      "Epoch [3/5]: [77/119]  65%|██████▍   , loss=1.03 [03:37<01:47]\n",
      "Epoch [3/5]: [77/119]  65%|██████▍   , loss=1.02 [03:37<01:47]\n",
      "Epoch [3/5]: [78/119]  66%|██████▌   , loss=1.02 [03:37<01:28]\n",
      "2025-07-15 08:25:25,631 - INFO - Current learning rate: 0.0007510889194058472\n",
      "2025-07-15 08:25:27,113 - INFO - Epoch: 3/5, Iter: 79/119 -- train_loss: 1.0075\n",
      "Epoch [3/5]: [78/119]  66%|██████▌   , loss=1.02 [03:38<01:28]\n",
      "Epoch [3/5]: [78/119]  66%|██████▌   , loss=1.01 [03:38<01:28]\n",
      "Epoch [3/5]: [79/119]  66%|██████▋   , loss=1.01 [03:38<01:18]\n",
      "2025-07-15 08:25:27,115 - INFO - Current learning rate: 0.0007478204268948339\n",
      "2025-07-15 08:25:30,530 - INFO - Epoch: 3/5, Iter: 80/119 -- train_loss: 1.0244\n",
      "Epoch [3/5]: [79/119]  66%|██████▋   , loss=1.01 [03:42<01:18]\n",
      "Epoch [3/5]: [79/119]  66%|██████▋   , loss=1.02 [03:42<01:18]\n",
      "Epoch [3/5]: [80/119]  67%|██████▋   , loss=1.02 [03:42<01:33]\n",
      "2025-07-15 08:25:30,532 - INFO - Current learning rate: 0.0007445378349630751\n",
      "2025-07-15 08:25:32,920 - INFO - Epoch: 3/5, Iter: 81/119 -- train_loss: 1.0272\n",
      "Epoch [3/5]: [80/119]  67%|██████▋   , loss=1.02 [03:44<01:33]\n",
      "Epoch [3/5]: [80/119]  67%|██████▋   , loss=1.03 [03:44<01:33]\n",
      "Epoch [3/5]: [81/119]  68%|██████▊   , loss=1.03 [03:44<01:31]\n",
      "2025-07-15 08:25:32,922 - INFO - Current learning rate: 0.0007412413303708751\n",
      "2025-07-15 08:25:33,967 - INFO - Epoch: 3/5, Iter: 82/119 -- train_loss: 1.0273\n",
      "Epoch [3/5]: [81/119]  68%|██████▊   , loss=1.03 [03:45<01:31]\n",
      "Epoch [3/5]: [81/119]  68%|██████▊   , loss=1.03 [03:45<01:31]\n",
      "Epoch [3/5]: [82/119]  69%|██████▉   , loss=1.03 [03:45<01:13]\n",
      "2025-07-15 08:25:33,968 - INFO - Current learning rate: 0.000737931100670087\n",
      "2025-07-15 08:25:35,299 - INFO - Epoch: 3/5, Iter: 83/119 -- train_loss: 1.0278\n",
      "Epoch [3/5]: [82/119]  69%|██████▉   , loss=1.03 [03:46<01:13]\n",
      "Epoch [3/5]: [82/119]  69%|██████▉   , loss=1.03 [03:46<01:13]\n",
      "Epoch [3/5]: [83/119]  70%|██████▉   , loss=1.03 [03:46<01:04]\n",
      "2025-07-15 08:25:35,301 - INFO - Current learning rate: 0.0007346073341934426\n",
      "2025-07-15 08:25:38,582 - INFO - Epoch: 3/5, Iter: 84/119 -- train_loss: 1.0170\n",
      "Epoch [3/5]: [83/119]  70%|██████▉   , loss=1.03 [03:50<01:04]\n",
      "Epoch [3/5]: [83/119]  70%|██████▉   , loss=1.02 [03:50<01:04]\n",
      "Epoch [3/5]: [84/119]  71%|███████   , loss=1.02 [03:50<01:18]\n",
      "2025-07-15 08:25:38,584 - INFO - Current learning rate: 0.0007312702200438372\n",
      "2025-07-15 08:25:44,156 - INFO - Epoch: 3/5, Iter: 85/119 -- train_loss: 1.0269\n",
      "Epoch [3/5]: [84/119]  71%|███████   , loss=1.02 [03:55<01:18]\n",
      "Epoch [3/5]: [84/119]  71%|███████   , loss=1.03 [03:55<01:18]\n",
      "Epoch [3/5]: [85/119]  71%|███████▏  , loss=1.03 [03:55<01:50]\n",
      "2025-07-15 08:25:44,157 - INFO - Current learning rate: 0.0007279199480835705\n",
      "2025-07-15 08:25:45,354 - INFO - Epoch: 3/5, Iter: 86/119 -- train_loss: 1.0270\n",
      "Epoch [3/5]: [85/119]  71%|███████▏  , loss=1.03 [03:56<01:50]\n",
      "Epoch [3/5]: [85/119]  71%|███████▏  , loss=1.03 [03:56<01:50]\n",
      "Epoch [3/5]: [86/119]  72%|███████▏  , loss=1.03 [03:56<01:26]\n",
      "2025-07-15 08:25:45,356 - INFO - Current learning rate: 0.0007245567089235448\n",
      "2025-07-15 08:25:46,606 - INFO - Epoch: 3/5, Iter: 87/119 -- train_loss: 1.0130\n",
      "Epoch [3/5]: [86/119]  72%|███████▏  , loss=1.03 [03:58<01:26]\n",
      "Epoch [3/5]: [86/119]  72%|███████▏  , loss=1.01 [03:58<01:26]\n",
      "Epoch [3/5]: [87/119]  73%|███████▎  , loss=1.01 [03:58<01:10]\n",
      "2025-07-15 08:25:46,607 - INFO - Current learning rate: 0.0007211806939124207\n",
      "2025-07-15 08:25:50,878 - INFO - Epoch: 3/5, Iter: 88/119 -- train_loss: 1.0101\n",
      "Epoch [3/5]: [87/119]  73%|███████▎  , loss=1.01 [04:02<01:10]\n",
      "Epoch [3/5]: [87/119]  73%|███████▎  , loss=1.01 [04:02<01:10]\n",
      "Epoch [3/5]: [88/119]  74%|███████▍  , loss=1.01 [04:02<01:27]\n",
      "2025-07-15 08:25:50,880 - INFO - Current learning rate: 0.0007177920951257296\n",
      "2025-07-15 08:26:06,588 - INFO - Epoch: 3/5, Iter: 89/119 -- train_loss: 1.0265\n",
      "Epoch [3/5]: [88/119]  74%|███████▍  , loss=1.01 [04:18<01:27]\n",
      "Epoch [3/5]: [88/119]  74%|███████▍  , loss=1.03 [04:18<01:27]\n",
      "Epoch [3/5]: [89/119]  75%|███████▍  , loss=1.03 [04:18<03:20]\n",
      "2025-07-15 08:26:06,590 - INFO - Current learning rate: 0.0007143911053549464\n",
      "2025-07-15 08:26:07,767 - INFO - Epoch: 3/5, Iter: 90/119 -- train_loss: 0.9891\n",
      "Epoch [3/5]: [89/119]  75%|███████▍  , loss=1.03 [04:19<03:20]\n",
      "Epoch [3/5]: [89/119]  75%|███████▍  , loss=0.989 [04:19<03:20]\n",
      "Epoch [3/5]: [90/119]  76%|███████▌  , loss=0.989 [04:19<02:26]\n",
      "2025-07-15 08:26:07,769 - INFO - Current learning rate: 0.0007109779180965208\n",
      "2025-07-15 08:26:09,210 - INFO - Epoch: 3/5, Iter: 91/119 -- train_loss: 1.0266\n",
      "Epoch [3/5]: [90/119]  76%|███████▌  , loss=0.989 [04:20<02:26]\n",
      "Epoch [3/5]: [90/119]  76%|███████▌  , loss=1.03 [04:20<02:26]\n",
      "Epoch [3/5]: [91/119]  76%|███████▋  , loss=1.03 [04:20<01:50]\n",
      "2025-07-15 08:26:09,212 - INFO - Current learning rate: 0.0007075527275408679\n",
      "2025-07-15 08:26:10,913 - INFO - Epoch: 3/5, Iter: 92/119 -- train_loss: 1.0262\n",
      "Epoch [3/5]: [91/119]  76%|███████▋  , loss=1.03 [04:22<01:50]\n",
      "Epoch [3/5]: [91/119]  76%|███████▋  , loss=1.03 [04:22<01:50]\n",
      "Epoch [3/5]: [92/119]  77%|███████▋  , loss=1.03 [04:22<01:28]\n",
      "2025-07-15 08:26:10,915 - INFO - Current learning rate: 0.0007041157285613206\n",
      "2025-07-15 08:26:18,735 - INFO - Epoch: 3/5, Iter: 93/119 -- train_loss: 1.0262\n",
      "Epoch [3/5]: [92/119]  77%|███████▋  , loss=1.03 [04:30<01:28]\n",
      "Epoch [3/5]: [92/119]  77%|███████▋  , loss=1.03 [04:30<01:28]\n",
      "Epoch [3/5]: [93/119]  78%|███████▊  , loss=1.03 [04:30<02:00]\n",
      "2025-07-15 08:26:18,737 - INFO - Current learning rate: 0.000700667116703042\n",
      "2025-07-15 08:26:20,052 - INFO - Epoch: 3/5, Iter: 94/119 -- train_loss: 1.0263\n",
      "Epoch [3/5]: [93/119]  78%|███████▊  , loss=1.03 [04:31<02:00]\n",
      "Epoch [3/5]: [93/119]  78%|███████▊  , loss=1.03 [04:31<02:00]\n",
      "Epoch [3/5]: [94/119]  79%|███████▉  , loss=1.03 [04:31<01:31]\n",
      "2025-07-15 08:26:20,054 - INFO - Current learning rate: 0.0006972070881719002\n",
      "2025-07-15 08:26:21,421 - INFO - Epoch: 3/5, Iter: 95/119 -- train_loss: 1.0260\n",
      "Epoch [3/5]: [94/119]  79%|███████▉  , loss=1.03 [04:32<01:31]\n",
      "Epoch [3/5]: [94/119]  79%|███████▉  , loss=1.03 [04:32<01:31]\n",
      "Epoch [3/5]: [95/119]  80%|███████▉  , loss=1.03 [04:32<01:11]\n",
      "2025-07-15 08:26:21,422 - INFO - Current learning rate: 0.0006937358398233053\n",
      "2025-07-15 08:26:22,933 - INFO - Epoch: 3/5, Iter: 96/119 -- train_loss: 1.0261\n",
      "Epoch [3/5]: [95/119]  80%|███████▉  , loss=1.03 [04:34<01:11]\n",
      "Epoch [3/5]: [95/119]  80%|███████▉  , loss=1.03 [04:34<01:11]\n",
      "Epoch [3/5]: [96/119]  81%|████████  , loss=1.03 [04:34<00:58]\n",
      "2025-07-15 08:26:22,934 - INFO - Current learning rate: 0.0006902535691510095\n",
      "2025-07-15 08:26:27,767 - INFO - Epoch: 3/5, Iter: 97/119 -- train_loss: 1.0256\n",
      "Epoch [3/5]: [96/119]  81%|████████  , loss=1.03 [04:39<00:58]\n",
      "Epoch [3/5]: [96/119]  81%|████████  , loss=1.03 [04:39<00:58]\n",
      "Epoch [3/5]: [97/119]  82%|████████▏ , loss=1.03 [04:39<01:10]\n",
      "2025-07-15 08:26:27,768 - INFO - Current learning rate: 0.0006867604742758704\n",
      "2025-07-15 08:26:28,893 - INFO - Epoch: 3/5, Iter: 98/119 -- train_loss: 1.0259\n",
      "Epoch [3/5]: [97/119]  82%|████████▏ , loss=1.03 [04:40<01:10]\n",
      "Epoch [3/5]: [97/119]  82%|████████▏ , loss=1.03 [04:40<01:10]\n",
      "Epoch [3/5]: [98/119]  82%|████████▏ , loss=1.03 [04:40<00:54]\n",
      "2025-07-15 08:26:28,895 - INFO - Current learning rate: 0.0006832567539345805\n",
      "2025-07-15 08:26:30,560 - INFO - Epoch: 3/5, Iter: 99/119 -- train_loss: 1.0149\n",
      "Epoch [3/5]: [98/119]  82%|████████▏ , loss=1.03 [04:42<00:54]\n",
      "Epoch [3/5]: [98/119]  82%|████████▏ , loss=1.01 [04:42<00:54]\n",
      "Epoch [3/5]: [99/119]  83%|████████▎ , loss=1.01 [04:42<00:46]\n",
      "2025-07-15 08:26:30,561 - INFO - Current learning rate: 0.0006797426074683582\n",
      "2025-07-15 08:26:31,867 - INFO - Epoch: 3/5, Iter: 100/119 -- train_loss: 1.0259\n",
      "Epoch [3/5]: [99/119]  83%|████████▎ , loss=1.01 [04:43<00:46]\n",
      "Epoch [3/5]: [99/119]  83%|████████▎ , loss=1.03 [04:43<00:46]\n",
      "Epoch [3/5]: [100/119]  84%|████████▍ , loss=1.03 [04:43<00:38]\n",
      "2025-07-15 08:26:31,868 - INFO - Current learning rate: 0.0006762182348116081\n",
      "2025-07-15 08:26:38,078 - INFO - Epoch: 3/5, Iter: 101/119 -- train_loss: 1.0146\n",
      "Epoch [3/5]: [100/119]  84%|████████▍ , loss=1.03 [04:49<00:38]\n",
      "Epoch [3/5]: [100/119]  84%|████████▍ , loss=1.01 [04:49<00:38]\n",
      "Epoch [3/5]: [101/119]  85%|████████▍ , loss=1.01 [04:49<00:58]\n",
      "2025-07-15 08:26:38,079 - INFO - Current learning rate: 0.0006726838364805449\n",
      "2025-07-15 08:26:39,263 - INFO - Epoch: 3/5, Iter: 102/119 -- train_loss: 1.0199\n",
      "Epoch [3/5]: [101/119]  85%|████████▍ , loss=1.01 [04:50<00:58]\n",
      "Epoch [3/5]: [101/119]  85%|████████▍ , loss=1.02 [04:50<00:58]\n",
      "Epoch [3/5]: [102/119]  86%|████████▌ , loss=1.02 [04:50<00:44]\n",
      "2025-07-15 08:26:39,264 - INFO - Current learning rate: 0.0006691396135617861\n",
      "2025-07-15 08:26:40,478 - INFO - Epoch: 3/5, Iter: 103/119 -- train_loss: 1.0179\n",
      "Epoch [3/5]: [102/119]  86%|████████▌ , loss=1.02 [04:52<00:44]\n",
      "Epoch [3/5]: [102/119]  86%|████████▌ , loss=1.02 [04:52<00:44]\n",
      "Epoch [3/5]: [103/119]  87%|████████▋ , loss=1.02 [04:52<00:35]\n",
      "2025-07-15 08:26:40,480 - INFO - Current learning rate: 0.0006655857677009106\n",
      "2025-07-15 08:26:41,922 - INFO - Epoch: 3/5, Iter: 104/119 -- train_loss: 1.0261\n",
      "Epoch [3/5]: [103/119]  87%|████████▋ , loss=1.02 [04:53<00:35]\n",
      "Epoch [3/5]: [103/119]  87%|████████▋ , loss=1.03 [04:53<00:35]\n",
      "Epoch [3/5]: [104/119]  87%|████████▋ , loss=1.03 [04:53<00:29]\n",
      "2025-07-15 08:26:41,923 - INFO - Current learning rate: 0.0006620225010909862\n",
      "2025-07-15 08:26:47,668 - INFO - Epoch: 3/5, Iter: 105/119 -- train_loss: 1.0250\n",
      "Epoch [3/5]: [104/119]  87%|████████▋ , loss=1.03 [04:59<00:29]\n",
      "Epoch [3/5]: [104/119]  87%|████████▋ , loss=1.02 [04:59<00:29]\n",
      "Epoch [3/5]: [105/119]  88%|████████▊ , loss=1.02 [04:59<00:43]\n",
      "2025-07-15 08:26:47,671 - INFO - Current learning rate: 0.0006584500164610668\n",
      "2025-07-15 08:26:48,927 - INFO - Epoch: 3/5, Iter: 106/119 -- train_loss: 1.0250\n",
      "Epoch [3/5]: [105/119]  88%|████████▊ , loss=1.02 [05:00<00:43]\n",
      "Epoch [3/5]: [105/119]  88%|████████▊ , loss=1.02 [05:00<00:43]\n",
      "Epoch [3/5]: [106/119]  89%|████████▉ , loss=1.02 [05:00<00:33]\n",
      "2025-07-15 08:26:48,929 - INFO - Current learning rate: 0.0006548685170646574\n",
      "2025-07-15 08:26:50,136 - INFO - Epoch: 3/5, Iter: 107/119 -- train_loss: 1.0251\n",
      "Epoch [3/5]: [106/119]  89%|████████▉ , loss=1.02 [05:01<00:33]\n",
      "Epoch [3/5]: [106/119]  89%|████████▉ , loss=1.03 [05:01<00:33]\n",
      "Epoch [3/5]: [107/119]  90%|████████▉ , loss=1.03 [05:01<00:25]\n",
      "2025-07-15 08:26:50,138 - INFO - Current learning rate: 0.0006512782066681507\n",
      "2025-07-15 08:26:51,203 - INFO - Epoch: 3/5, Iter: 108/119 -- train_loss: 1.0243\n",
      "Epoch [3/5]: [107/119]  90%|████████▉ , loss=1.03 [05:02<00:25]\n",
      "Epoch [3/5]: [107/119]  90%|████████▉ , loss=1.02 [05:02<00:25]\n",
      "Epoch [3/5]: [108/119]  91%|█████████ , loss=1.02 [05:02<00:20]\n",
      "2025-07-15 08:26:51,205 - INFO - Current learning rate: 0.0006476792895392338\n",
      "2025-07-15 08:26:55,040 - INFO - Epoch: 3/5, Iter: 109/119 -- train_loss: 1.0149\n",
      "Epoch [3/5]: [108/119]  91%|█████████ , loss=1.02 [05:06<00:20]\n",
      "Epoch [3/5]: [108/119]  91%|█████████ , loss=1.01 [05:06<00:20]\n",
      "Epoch [3/5]: [109/119]  92%|█████████▏, loss=1.01 [05:06<00:24]\n",
      "2025-07-15 08:26:55,042 - INFO - Current learning rate: 0.0006440719704352667\n",
      "2025-07-15 08:26:58,990 - INFO - Epoch: 3/5, Iter: 110/119 -- train_loss: 1.0203\n",
      "Epoch [3/5]: [109/119]  92%|█████████▏, loss=1.01 [05:10<00:24]\n",
      "Epoch [3/5]: [109/119]  92%|█████████▏, loss=1.02 [05:10<00:24]\n",
      "Epoch [3/5]: [110/119]  92%|█████████▏, loss=1.02 [05:10<00:25]\n",
      "2025-07-15 08:26:58,992 - INFO - Current learning rate: 0.0006404564545916326\n",
      "2025-07-15 08:27:00,417 - INFO - Epoch: 3/5, Iter: 111/119 -- train_loss: 1.0240\n",
      "Epoch [3/5]: [110/119]  92%|█████████▏, loss=1.02 [05:11<00:25]\n",
      "Epoch [3/5]: [110/119]  92%|█████████▏, loss=1.02 [05:11<00:25]\n",
      "Epoch [3/5]: [111/119]  93%|█████████▎, loss=1.02 [05:11<00:19]\n",
      "2025-07-15 08:27:00,419 - INFO - Current learning rate: 0.0006368329477100613\n",
      "2025-07-15 08:27:01,716 - INFO - Epoch: 3/5, Iter: 112/119 -- train_loss: 1.0246\n",
      "Epoch [3/5]: [111/119]  93%|█████████▎, loss=1.02 [05:13<00:19]\n",
      "Epoch [3/5]: [111/119]  93%|█████████▎, loss=1.02 [05:13<00:19]\n",
      "Epoch [3/5]: [112/119]  94%|█████████▍, loss=1.02 [05:13<00:14]\n",
      "2025-07-15 08:27:01,717 - INFO - Current learning rate: 0.0006332016559469259\n",
      "2025-07-15 08:27:06,540 - INFO - Epoch: 3/5, Iter: 113/119 -- train_loss: 1.0243\n",
      "Epoch [3/5]: [112/119]  94%|█████████▍, loss=1.02 [05:18<00:14]\n",
      "Epoch [3/5]: [112/119]  94%|█████████▍, loss=1.02 [05:18<00:14]\n",
      "Epoch [3/5]: [113/119]  95%|█████████▍, loss=1.02 [05:18<00:17]\n",
      "2025-07-15 08:27:06,541 - INFO - Current learning rate: 0.000629562785901514\n",
      "2025-07-15 08:27:11,327 - INFO - Epoch: 3/5, Iter: 114/119 -- train_loss: 1.0182\n",
      "Epoch [3/5]: [113/119]  95%|█████████▍, loss=1.02 [05:22<00:17]\n",
      "Epoch [3/5]: [113/119]  95%|█████████▍, loss=1.02 [05:22<00:17]\n",
      "Epoch [3/5]: [114/119]  96%|█████████▌, loss=1.02 [05:22<00:17]\n",
      "2025-07-15 08:27:11,328 - INFO - Current learning rate: 0.0006259165446042728\n",
      "2025-07-15 08:27:12,445 - INFO - Epoch: 3/5, Iter: 115/119 -- train_loss: 1.0243\n",
      "Epoch [3/5]: [114/119]  96%|█████████▌, loss=1.02 [05:24<00:17]\n",
      "Epoch [3/5]: [114/119]  96%|█████████▌, loss=1.02 [05:24<00:17]\n",
      "Epoch [3/5]: [115/119]  97%|█████████▋, loss=1.02 [05:24<00:11]\n",
      "2025-07-15 08:27:12,447 - INFO - Current learning rate: 0.0006222631395050313\n",
      "2025-07-15 08:27:13,624 - INFO - Epoch: 3/5, Iter: 116/119 -- train_loss: 1.0200\n",
      "Epoch [3/5]: [115/119]  97%|█████████▋, loss=1.02 [05:25<00:11]\n",
      "Epoch [3/5]: [115/119]  97%|█████████▋, loss=1.02 [05:25<00:11]\n",
      "Epoch [3/5]: [116/119]  97%|█████████▋, loss=1.02 [05:25<00:06]\n",
      "2025-07-15 08:27:13,626 - INFO - Current learning rate: 0.0006186027784611962\n",
      "2025-07-15 08:27:15,016 - INFO - Epoch: 3/5, Iter: 117/119 -- train_loss: 1.0069\n",
      "Epoch [3/5]: [116/119]  97%|█████████▋, loss=1.02 [05:26<00:06]\n",
      "Epoch [3/5]: [116/119]  97%|█████████▋, loss=1.01 [05:26<00:06]\n",
      "Epoch [3/5]: [117/119]  98%|█████████▊, loss=1.01 [05:26<00:04]\n",
      "2025-07-15 08:27:15,018 - INFO - Current learning rate: 0.0006149356697259276\n",
      "2025-07-15 08:27:19,829 - INFO - Epoch: 3/5, Iter: 118/119 -- train_loss: 1.0237\n",
      "Epoch [3/5]: [117/119]  98%|█████████▊, loss=1.01 [05:31<00:04]\n",
      "Epoch [3/5]: [117/119]  98%|█████████▊, loss=1.02 [05:31<00:04]\n",
      "Epoch [3/5]: [118/119]  99%|█████████▉, loss=1.02 [05:31<00:02]\n",
      "2025-07-15 08:27:19,831 - INFO - Current learning rate: 0.0006112620219362892\n",
      "2025-07-15 08:27:20,471 - INFO - Epoch: 3/5, Iter: 119/119 -- train_loss: 1.0237\n",
      "Epoch [3/5]: [118/119]  99%|█████████▉, loss=1.02 [05:32<00:02]\n",
      "Epoch [3/5]: [118/119]  99%|█████████▉, loss=1.02 [05:32<00:02]\n",
      "Epoch [3/5]: [119/119] 100%|██████████, loss=1.02 [05:32<00:00]\n",
      "2025-07-15 08:27:20,472 - INFO - Current learning rate: 0.0006075820441013791\n",
      "2025-07-15 08:27:20,472 - INFO - Engine run resuming from iteration 0, epoch 2 until 3 epochs\n",
      "[1/20]   5%|#033[32m▌         #033[0m [00:00<?]#033[A\n",
      "Epoch [3/3]: [1/20]   5%|#033[32m▌         #033[0m [00:00<?]#033[A\n",
      "Epoch [3/3]: [1/20]   5%|#033[32m▌         #033[0m [00:08<?]#033[A\n",
      "Epoch [3/3]: [2/20]  10%|#033[32m█         #033[0m [00:08<02:33]\n",
      "#033[A\n",
      "Epoch [3/3]: [2/20]  10%|#033[32m█         #033[0m [00:17<02:33]#033[A\n",
      "Epoch [3/3]: [3/20]  15%|#033[32m█▌        #033[0m [00:17<02:28]#033[A\n",
      "Epoch [3/3]: [3/20]  15%|#033[32m█▌        #033[0m [00:24<02:28]#033[A\n",
      "Epoch [3/3]: [4/20]  20%|#033[32m██        #033[0m [00:24<02:04]#033[A\n",
      "Epoch [3/3]: [4/20]  20%|#033[32m██        #033[0m [00:30<02:04]#033[A\n",
      "Epoch [3/3]: [5/20]  25%|#033[32m██▌       #033[0m [00:30<01:50]#033[A\n",
      "Epoch [3/3]: [5/20]  25%|#033[32m██▌       #033[0m [00:39<01:50]#033[A\n",
      "Epoch [3/3]: [6/20]  30%|#033[32m███       #033[0m [00:39<01:48]#033[A\n",
      "Epoch [3/3]: [6/20]  30%|#033[32m███       #033[0m [00:48<01:48]#033[A\n",
      "Epoch [3/3]: [7/20]  35%|#033[32m███▌      #033[0m [00:48<01:45]#033[A\n",
      "Epoch [3/3]: [7/20]  35%|#033[32m███▌      #033[0m [00:56<01:45]#033[A\n",
      "Epoch [3/3]: [8/20]  40%|#033[32m████      #033[0m [00:56<01:39]#033[A\n",
      "Epoch [3/3]: [8/20]  40%|#033[32m████      #033[0m [01:03<01:39]#033[A\n",
      "Epoch [3/3]: [9/20]  45%|#033[32m████▌     #033[0m [01:03<01:25]#033[A\n",
      "Epoch [3/3]: [9/20]  45%|#033[32m████▌     #033[0m [01:16<01:25]#033[A\n",
      "Epoch [3/3]: [10/20]  50%|#033[32m█████     #033[0m [01:16<01:33]#033[A\n",
      "Epoch [3/3]: [10/20]  50%|#033[32m█████     #033[0m [01:27<01:33]#033[A\n",
      "Epoch [3/3]: [11/20]  55%|#033[32m█████▌    #033[0m [01:27<01:28]#033[A\n",
      "Epoch [3/3]: [11/20]  55%|#033[32m█████▌    #033[0m [01:38<01:28]#033[A\n",
      "Epoch [3/3]: [12/20]  60%|#033[32m██████    #033[0m [01:38<01:21]#033[A\n",
      "Epoch [3/3]: [12/20]  60%|#033[32m██████    #033[0m [01:47<01:21]#033[A\n",
      "Epoch [3/3]: [13/20]  65%|#033[32m██████▌   #033[0m [01:47<01:09]#033[A\n",
      "Epoch [3/3]: [13/20]  65%|#033[32m██████▌   #033[0m [01:55<01:09]#033[A\n",
      "Epoch [3/3]: [14/20]  70%|#033[32m███████   #033[0m [01:55<00:55]#033[A\n",
      "Epoch [3/3]: [14/20]  70%|#033[32m███████   #033[0m [02:00<00:55]#033[A\n",
      "Epoch [3/3]: [15/20]  75%|#033[32m███████▌  #033[0m [02:00<00:40]#033[A\n",
      "Epoch [3/3]: [15/20]  75%|#033[32m███████▌  #033[0m [02:05<00:40]#033[A\n",
      "Epoch [3/3]: [16/20]  80%|#033[32m████████  #033[0m [02:05<00:28]#033[A\n",
      "Epoch [3/3]: [16/20]  80%|#033[32m████████  #033[0m [02:13<00:28]#033[A\n",
      "Epoch [3/3]: [17/20]  85%|#033[32m████████▌ #033[0m [02:13<00:22]#033[A\n",
      "Epoch [3/3]: [17/20]  85%|#033[32m████████▌ #033[0m [02:20<00:22]#033[A\n",
      "Epoch [3/3]: [18/20]  90%|#033[32m█████████ #033[0m [02:20<00:14]#033[A\n",
      "Epoch [3/3]: [18/20]  90%|#033[32m█████████ #033[0m [02:28<00:14]#033[A\n",
      "Epoch [3/3]: [19/20]  95%|#033[32m█████████▌#033[0m [02:28<00:07]#033[A\n",
      "Epoch [3/3]: [19/20]  95%|#033[32m█████████▌#033[0m [02:37<00:07]#033[A\n",
      "Epoch [3/3]: [20/20] 100%|#033[32m██████████#033[0m [02:37<00:00]#033[A\n",
      "#033[A\n",
      "2025-07-15 08:30:11,665 - INFO - Epoch[3] Complete. Time taken: 00:02:51.094\n",
      "2025-07-15 08:30:11,666 - INFO - Engine run finished. Time taken: 00:02:51.193\n",
      "2025-07-15 08:30:11,768 - INFO - Epoch[3] Complete. Time taken: 00:08:36.722\n",
      "2025-07-15 08:30:23,379 - INFO - Epoch: 4/5, Iter: 1/119 -- train_loss: 1.0236\n",
      "[1/119]   1%|           [00:00<?]\n",
      "Epoch [4/5]: [1/119]   1%|           [00:00<?]\n",
      "Epoch [4/5]: [1/119]   1%|          , loss=1.02 [00:00<?]\n",
      "2025-07-15 08:30:23,381 - INFO - Current learning rate: 0.0006038959455904377\n",
      "2025-07-15 08:30:24,788 - INFO - Epoch: 4/5, Iter: 2/119 -- train_loss: 1.0239\n",
      "Epoch [4/5]: [1/119]   1%|          , loss=1.02 [00:01<?]\n",
      "Epoch [4/5]: [1/119]   1%|          , loss=1.02 [00:01<?]\n",
      "Epoch [4/5]: [2/119]   2%|▏         , loss=1.02 [00:01<02:44]\n",
      "2025-07-15 08:30:24,790 - INFO - Current learning rate: 0.0006002039361209363\n",
      "2025-07-15 08:30:27,125 - INFO - Epoch: 4/5, Iter: 3/119 -- train_loss: 1.0234\n",
      "Epoch [4/5]: [2/119]   2%|▏         , loss=1.02 [00:03<02:44]\n",
      "Epoch [4/5]: [2/119]   2%|▏         , loss=1.02 [00:03<02:44]\n",
      "Epoch [4/5]: [3/119]   3%|▎         , loss=1.02 [00:03<03:46]\n",
      "2025-07-15 08:30:27,126 - INFO - Current learning rate: 0.0005965062257466451\n",
      "2025-07-15 08:30:28,624 - INFO - Epoch: 4/5, Iter: 4/119 -- train_loss: 1.0247\n",
      "Epoch [4/5]: [3/119]   3%|▎         , loss=1.02 [00:05<03:46]\n",
      "Epoch [4/5]: [3/119]   3%|▎         , loss=1.02 [00:05<03:46]\n",
      "Epoch [4/5]: [4/119]   3%|▎         , loss=1.02 [00:05<03:20]\n",
      "2025-07-15 08:30:28,626 - INFO - Current learning rate: 0.0005928030248456825\n",
      "2025-07-15 08:30:33,540 - INFO - Epoch: 4/5, Iter: 5/119 -- train_loss: 1.0230\n",
      "Epoch [4/5]: [4/119]   3%|▎         , loss=1.02 [00:10<03:20]\n",
      "Epoch [4/5]: [4/119]   3%|▎         , loss=1.02 [00:10<03:20]\n",
      "Epoch [4/5]: [5/119]   4%|▍         , loss=1.02 [00:10<05:41]\n",
      "2025-07-15 08:30:33,541 - INFO - Current learning rate: 0.000589094544108546\n",
      "2025-07-15 08:30:35,371 - INFO - Epoch: 4/5, Iter: 6/119 -- train_loss: 1.0126\n",
      "Epoch [4/5]: [5/119]   4%|▍         , loss=1.02 [00:11<05:41]\n",
      "Epoch [4/5]: [5/119]   4%|▍         , loss=1.01 [00:11<05:41]\n",
      "Epoch [4/5]: [6/119]   5%|▌         , loss=1.01 [00:11<04:51]\n",
      "2025-07-15 08:30:35,373 - INFO - Current learning rate: 0.0005853809945261245\n",
      "2025-07-15 08:30:37,080 - INFO - Epoch: 4/5, Iter: 7/119 -- train_loss: 1.0178\n",
      "Epoch [4/5]: [6/119]   5%|▌         , loss=1.01 [00:13<04:51]\n",
      "Epoch [4/5]: [6/119]   5%|▌         , loss=1.02 [00:13<04:51]#015Epoch [4/5]: [7/119]   6%|▌         , loss=1.02 [00:13<04:15]2025-07-15 08:30:37,082 - INFO - Current learning rate: 0.000581662587377695\n",
      "2025-07-15 08:30:38,638 - INFO - Epoch: 4/5, Iter: 8/119 -- train_loss: 1.0230\n",
      "Epoch [4/5]: [7/119]   6%|▌         , loss=1.02 [00:15<04:15]\n",
      "Epoch [4/5]: [7/119]   6%|▌         , loss=1.02 [00:15<04:15]\n",
      "Epoch [4/5]: [8/119]   7%|▋         , loss=1.02 [00:15<03:47]\n",
      "2025-07-15 08:30:38,639 - INFO - Current learning rate: 0.0005779395342189016\n",
      "2025-07-15 08:30:42,814 - INFO - Epoch: 4/5, Iter: 9/119 -- train_loss: 1.0210\n",
      "Epoch [4/5]: [8/119]   7%|▋         , loss=1.02 [00:19<03:47]\n",
      "Epoch [4/5]: [8/119]   7%|▋         , loss=1.02 [00:19<03:47]\n",
      "Epoch [4/5]: [9/119]   8%|▊         , loss=1.02 [00:19<04:59]\n",
      "2025-07-15 08:30:42,816 - INFO - Current learning rate: 0.0005742120468697188\n",
      "2025-07-15 08:30:44,208 - INFO - Epoch: 4/5, Iter: 10/119 -- train_loss: 1.0235\n",
      "Epoch [4/5]: [9/119]   8%|▊         , loss=1.02 [00:20<04:59]\n",
      "Epoch [4/5]: [9/119]   8%|▊         , loss=1.02 [00:20<04:59]\n",
      "Epoch [4/5]: [10/119]   8%|▊         , loss=1.02 [00:20<04:11]\n",
      "2025-07-15 08:30:44,210 - INFO - Current learning rate: 0.0005704803374024011\n",
      "2025-07-15 08:30:45,677 - INFO - Epoch: 4/5, Iter: 11/119 -- train_loss: 1.0122\n",
      "Epoch [4/5]: [10/119]   8%|▊         , loss=1.02 [00:22<04:11]\n",
      "Epoch [4/5]: [10/119]   8%|▊         , loss=1.01 [00:22<04:11]\n",
      "Epoch [4/5]: [11/119]   9%|▉         , loss=1.01 [00:22<03:41]\n",
      "2025-07-15 08:30:45,678 - INFO - Current learning rate: 0.0005667446181294167\n",
      "2025-07-15 08:30:47,193 - INFO - Epoch: 4/5, Iter: 12/119 -- train_loss: 1.0225\n",
      "Epoch [4/5]: [11/119]   9%|▉         , loss=1.01 [00:23<03:41]\n",
      "Epoch [4/5]: [11/119]   9%|▉         , loss=1.02 [00:23<03:41]\n",
      "Epoch [4/5]: [12/119]  10%|█         , loss=1.02 [00:23<03:21]\n",
      "2025-07-15 08:30:47,194 - INFO - Current learning rate: 0.0005630051015913684\n",
      "2025-07-15 08:30:58,042 - INFO - Epoch: 4/5, Iter: 13/119 -- train_loss: 1.0136\n",
      "Epoch [4/5]: [12/119]  10%|█         , loss=1.02 [00:34<03:21]\n",
      "Epoch [4/5]: [12/119]  10%|█         , loss=1.01 [00:34<03:21]\n",
      "Epoch [4/5]: [13/119]  11%|█         , loss=1.01 [00:34<08:08]\n",
      "2025-07-15 08:30:58,043 - INFO - Current learning rate: 0.000559262000544901\n",
      "2025-07-15 08:30:59,487 - INFO - Epoch: 4/5, Iter: 14/119 -- train_loss: 1.0220\n",
      "Epoch [4/5]: [13/119]  11%|█         , loss=1.01 [00:36<08:08]\n",
      "Epoch [4/5]: [13/119]  11%|█         , loss=1.02 [00:36<08:08]\n",
      "Epoch [4/5]: [14/119]  12%|█▏        , loss=1.02 [00:36<06:23]\n",
      "2025-07-15 08:30:59,499 - INFO - Current learning rate: 0.000555515527950597\n",
      "2025-07-15 08:31:00,925 - INFO - Epoch: 4/5, Iter: 15/119 -- train_loss: 1.0224\n",
      "Epoch [4/5]: [14/119]  12%|█▏        , loss=1.02 [00:37<06:23]\n",
      "Epoch [4/5]: [14/119]  12%|█▏        , loss=1.02 [00:37<06:23]\n",
      "Epoch [4/5]: [15/119]  13%|█▎        , loss=1.02 [00:37<05:10]\n",
      "2025-07-15 08:31:00,926 - INFO - Current learning rate: 0.0005517658969608601\n",
      "2025-07-15 08:31:02,606 - INFO - Epoch: 4/5, Iter: 16/119 -- train_loss: 1.0084\n",
      "Epoch [4/5]: [15/119]  13%|█▎        , loss=1.02 [00:39<05:10]\n",
      "Epoch [4/5]: [15/119]  13%|█▎        , loss=1.01 [00:39<05:10]\n",
      "Epoch [4/5]: [16/119]  13%|█▎        , loss=1.01 [00:39<04:26]\n",
      "2025-07-15 08:31:02,608 - INFO - Current learning rate: 0.000548013320907789\n",
      "2025-07-15 08:31:12,822 - INFO - Epoch: 4/5, Iter: 17/119 -- train_loss: 1.0224\n",
      "Epoch [4/5]: [16/119]  13%|█▎        , loss=1.01 [00:49<04:26]\n",
      "Epoch [4/5]: [16/119]  13%|█▎        , loss=1.02 [00:49<04:26]\n",
      "Epoch [4/5]: [17/119]  14%|█▍        , loss=1.02 [00:49<08:18]\n",
      "2025-07-15 08:31:12,823 - INFO - Current learning rate: 0.0005442580132910382\n",
      "2025-07-15 08:31:14,018 - INFO - Epoch: 4/5, Iter: 18/119 -- train_loss: 1.0173\n",
      "Epoch [4/5]: [17/119]  14%|█▍        , loss=1.02 [00:50<08:18]\n",
      "Epoch [4/5]: [17/119]  14%|█▍        , loss=1.02 [00:50<08:18]\n",
      "Epoch [4/5]: [18/119]  15%|█▌        , loss=1.02 [00:50<06:21]\n",
      "2025-07-15 08:31:14,019 - INFO - Current learning rate: 0.000540500187765673\n",
      "2025-07-15 08:31:15,126 - INFO - Epoch: 4/5, Iter: 19/119 -- train_loss: 1.0116\n",
      "Epoch [4/5]: [18/119]  15%|█▌        , loss=1.02 [00:51<06:21]\n",
      "Epoch [4/5]: [18/119]  15%|█▌        , loss=1.01 [00:51<06:21]\n",
      "Epoch [4/5]: [19/119]  16%|█▌        , loss=1.01 [00:51<04:57]\n",
      "2025-07-15 08:31:15,128 - INFO - Current learning rate: 0.0005367400581300126\n",
      "2025-07-15 08:31:16,583 - INFO - Epoch: 4/5, Iter: 20/119 -- train_loss: 1.0056\n",
      "Epoch [4/5]: [19/119]  16%|█▌        , loss=1.01 [00:53<04:57]\n",
      "Epoch [4/5]: [19/119]  16%|█▌        , loss=1.01 [00:53<04:57]\n",
      "Epoch [4/5]: [20/119]  17%|█▋        , loss=1.01 [00:53<04:09]\n",
      "2025-07-15 08:31:16,585 - INFO - Current learning rate: 0.0005329778383134667\n",
      "2025-07-15 08:31:23,865 - INFO - Epoch: 4/5, Iter: 21/119 -- train_loss: 1.0154\n",
      "Epoch [4/5]: [20/119]  17%|█▋        , loss=1.01 [01:00<04:09]\n",
      "Epoch [4/5]: [20/119]  17%|█▋        , loss=1.02 [01:00<04:09]\n",
      "Epoch [4/5]: [21/119]  18%|█▊        , loss=1.02 [01:00<06:26]\n",
      "2025-07-15 08:31:23,866 - INFO - Current learning rate: 0.0005292137423643644\n",
      "2025-07-15 08:31:25,137 - INFO - Epoch: 4/5, Iter: 22/119 -- train_loss: 1.0224\n",
      "Epoch [4/5]: [21/119]  18%|█▊        , loss=1.02 [01:01<06:26]\n",
      "Epoch [4/5]: [21/119]  18%|█▊        , loss=1.02 [01:01<06:26]\n",
      "Epoch [4/5]: [22/119]  18%|█▊        , loss=1.02 [01:01<05:05]\n",
      "2025-07-15 08:31:25,138 - INFO - Current learning rate: 0.0005254479844377753\n",
      "2025-07-15 08:31:26,367 - INFO - Epoch: 4/5, Iter: 23/119 -- train_loss: 1.0164\n",
      "Epoch [4/5]: [22/119]  18%|█▊        , loss=1.02 [01:02<05:05]\n",
      "Epoch [4/5]: [22/119]  18%|█▊        , loss=1.02 [01:02<05:05]\n",
      "Epoch [4/5]: [23/119]  19%|█▉        , loss=1.02 [01:02<04:06]\n",
      "2025-07-15 08:31:26,369 - INFO - Current learning rate: 0.000521680778783326\n",
      "2025-07-15 08:31:28,035 - INFO - Epoch: 4/5, Iter: 24/119 -- train_loss: 1.0221\n",
      "Epoch [4/5]: [23/119]  19%|█▉        , loss=1.02 [01:04<04:06]\n",
      "Epoch [4/5]: [23/119]  19%|█▉        , loss=1.02 [01:04<04:06]\n",
      "Epoch [4/5]: [24/119]  20%|██        , loss=1.02 [01:04<03:38]\n",
      "2025-07-15 08:31:28,037 - INFO - Current learning rate: 0.0005179123397330104\n",
      "2025-07-15 08:31:35,997 - INFO - Epoch: 4/5, Iter: 25/119 -- train_loss: 1.0221\n",
      "Epoch [4/5]: [24/119]  20%|██        , loss=1.02 [01:12<03:38]\n",
      "Epoch [4/5]: [24/119]  20%|██        , loss=1.02 [01:12<03:38]\n",
      "Epoch [4/5]: [25/119]  21%|██        , loss=1.02 [01:12<06:15]\n",
      "2025-07-15 08:31:35,999 - INFO - Current learning rate: 0.0005141428816889956\n",
      "2025-07-15 08:31:37,219 - INFO - Epoch: 4/5, Iter: 26/119 -- train_loss: 1.0216\n",
      "Epoch [4/5]: [25/119]  21%|██        , loss=1.02 [01:13<06:15]\n",
      "Epoch [4/5]: [25/119]  21%|██        , loss=1.02 [01:13<06:15]\n",
      "Epoch [4/5]: [26/119]  22%|██▏       , loss=1.02 [01:13<04:54]\n",
      "2025-07-15 08:31:37,220 - INFO - Current learning rate: 0.000510372619111423\n",
      "2025-07-15 08:31:38,513 - INFO - Epoch: 4/5, Iter: 27/119 -- train_loss: 1.0122\n",
      "Epoch [4/5]: [26/119]  22%|██▏       , loss=1.02 [01:15<04:54]\n",
      "Epoch [4/5]: [26/119]  22%|██▏       , loss=1.01 [01:15<04:54]\n",
      "Epoch [4/5]: [27/119]  23%|██▎       , loss=1.01 [01:15<03:59]\n",
      "2025-07-15 08:31:38,514 - INFO - Current learning rate: 0.0005066017665062078\n",
      "2025-07-15 08:31:40,105 - INFO - Epoch: 4/5, Iter: 28/119 -- train_loss: 1.0037\n",
      "Epoch [4/5]: [27/119]  23%|██▎       , loss=1.01 [01:16<03:59]\n",
      "Epoch [4/5]: [27/119]  23%|██▎       , loss=1 [01:16<03:59]\n",
      "Epoch [4/5]: [28/119]  24%|██▎       , loss=1 [01:16<03:29]\n",
      "2025-07-15 08:31:40,107 - INFO - Current learning rate: 0.0005028305384128339\n",
      "2025-07-15 08:31:43,692 - INFO - Epoch: 4/5, Iter: 29/119 -- train_loss: 1.0084\n",
      "Epoch [4/5]: [28/119]  24%|██▎       , loss=1 [01:20<03:29]\n",
      "Epoch [4/5]: [28/119]  24%|██▎       , loss=1.01 [01:20<03:29]\n",
      "Epoch [4/5]: [29/119]  24%|██▍       , loss=1.01 [01:20<04:01]\n",
      "2025-07-15 08:31:43,694 - INFO - Current learning rate: 0.0004990591493921485\n",
      "2025-07-15 08:31:45,119 - INFO - Epoch: 4/5, Iter: 30/119 -- train_loss: 1.0217\n",
      "Epoch [4/5]: [29/119]  24%|██▍       , loss=1.01 [01:21<04:01]\n",
      "Epoch [4/5]: [29/119]  24%|██▍       , loss=1.02 [01:21<04:01]\n",
      "Epoch [4/5]: [30/119]  25%|██▌       , loss=1.02 [01:21<03:25]\n",
      "2025-07-15 08:31:45,120 - INFO - Current learning rate: 0.0004952878140141545\n",
      "2025-07-15 08:31:46,480 - INFO - Epoch: 4/5, Iter: 31/119 -- train_loss: 1.0214\n",
      "Epoch [4/5]: [30/119]  25%|██▌       , loss=1.02 [01:23<03:25]\n",
      "Epoch [4/5]: [30/119]  25%|██▌       , loss=1.02 [01:23<03:25]\n",
      "Epoch [4/5]: [31/119]  26%|██▌       , loss=1.02 [01:23<02:58]\n",
      "2025-07-15 08:31:46,481 - INFO - Current learning rate: 0.0004915167468458031\n",
      "2025-07-15 08:31:47,752 - INFO - Epoch: 4/5, Iter: 32/119 -- train_loss: 1.0213\n",
      "Epoch [4/5]: [31/119]  26%|██▌       , loss=1.02 [01:24<02:58]\n",
      "Epoch [4/5]: [31/119]  26%|██▌       , loss=1.02 [01:24<02:58]\n",
      "Epoch [4/5]: [32/119]  27%|██▋       , loss=1.02 [01:24<02:36]\n",
      "2025-07-15 08:31:47,754 - INFO - Current learning rate: 0.0004877461624387859\n",
      "2025-07-15 08:31:53,335 - INFO - Epoch: 4/5, Iter: 33/119 -- train_loss: 1.0184\n",
      "Epoch [4/5]: [32/119]  27%|██▋       , loss=1.02 [01:29<02:36]\n",
      "Epoch [4/5]: [32/119]  27%|██▋       , loss=1.02 [01:29<02:36]\n",
      "Epoch [4/5]: [33/119]  28%|██▊       , loss=1.02 [01:29<04:12]\n",
      "2025-07-15 08:31:53,336 - INFO - Current learning rate: 0.00048397627531732753\n",
      "2025-07-15 08:31:54,910 - INFO - Epoch: 4/5, Iter: 34/119 -- train_loss: 1.0166\n",
      "Epoch [4/5]: [33/119]  28%|██▊       , loss=1.02 [01:31<04:12]\n",
      "Epoch [4/5]: [33/119]  28%|██▊       , loss=1.02 [01:31<04:12]\n",
      "Epoch [4/5]: [34/119]  29%|██▊       , loss=1.02 [01:31<03:34]\n",
      "2025-07-15 08:31:54,912 - INFO - Current learning rate: 0.00048020729996598225\n",
      "2025-07-15 08:31:56,364 - INFO - Epoch: 4/5, Iter: 35/119 -- train_loss: 1.0211\n",
      "Epoch [4/5]: [34/119]  29%|██▊       , loss=1.02 [01:32<03:34]\n",
      "Epoch [4/5]: [34/119]  29%|██▊       , loss=1.02 [01:32<03:34]\n",
      "Epoch [4/5]: [35/119]  29%|██▉       , loss=1.02 [01:32<03:05]\n",
      "2025-07-15 08:31:56,366 - INFO - Current learning rate: 0.0004764394508174288\n",
      "2025-07-15 08:31:57,506 - INFO - Epoch: 4/5, Iter: 36/119 -- train_loss: 1.0112\n",
      "Epoch [4/5]: [35/119]  29%|██▉       , loss=1.02 [01:34<03:05]\n",
      "Epoch [4/5]: [35/119]  29%|██▉       , loss=1.01 [01:34<03:05]\n",
      "Epoch [4/5]: [36/119]  30%|███       , loss=1.01 [01:34<02:36]\n",
      "2025-07-15 08:31:57,508 - INFO - Current learning rate: 0.0004726729422402727\n",
      "2025-07-15 08:32:05,628 - INFO - Epoch: 4/5, Iter: 37/119 -- train_loss: 1.0213\n",
      "Epoch [4/5]: [36/119]  30%|███       , loss=1.01 [01:42<02:36]\n",
      "Epoch [4/5]: [36/119]  30%|███       , loss=1.02 [01:42<02:36]\n",
      "Epoch [4/5]: [37/119]  31%|███       , loss=1.02 [01:42<05:08]\n",
      "2025-07-15 08:32:05,629 - INFO - Current learning rate: 0.0004689079885268477\n",
      "2025-07-15 08:32:07,147 - INFO - Epoch: 4/5, Iter: 38/119 -- train_loss: 1.0066\n",
      "Epoch [4/5]: [37/119]  31%|███       , loss=1.02 [01:43<05:08]\n",
      "Epoch [4/5]: [37/119]  31%|███       , loss=1.01 [01:43<05:08]\n",
      "Epoch [4/5]: [38/119]  32%|███▏      , loss=1.01 [01:43<04:09]\n",
      "2025-07-15 08:32:07,149 - INFO - Current learning rate: 0.0004651448038810258\n",
      "2025-07-15 08:32:08,821 - INFO - Epoch: 4/5, Iter: 39/119 -- train_loss: 1.0218\n",
      "Epoch [4/5]: [38/119]  32%|███▏      , loss=1.01 [01:45<04:09]\n",
      "Epoch [4/5]: [38/119]  32%|███▏      , loss=1.02 [01:45<04:09]\n",
      "Epoch [4/5]: [39/119]  33%|███▎      , loss=1.02 [01:45<03:32]\n",
      "2025-07-15 08:32:08,823 - INFO - Current learning rate: 0.00046138360240602854\n",
      "2025-07-15 08:32:10,092 - INFO - Epoch: 4/5, Iter: 40/119 -- train_loss: 1.0211\n",
      "Epoch [4/5]: [39/119]  33%|███▎      , loss=1.02 [01:46<03:32]\n",
      "Epoch [4/5]: [39/119]  33%|███▎      , loss=1.02 [01:46<03:32]\n",
      "Epoch [4/5]: [40/119]  34%|███▎      , loss=1.02 [01:46<02:57]\n",
      "2025-07-15 08:32:10,094 - INFO - Current learning rate: 0.0004576245980922476\n",
      "2025-07-15 08:32:15,361 - INFO - Epoch: 4/5, Iter: 41/119 -- train_loss: 1.0212\n",
      "Epoch [4/5]: [40/119]  34%|███▎      , loss=1.02 [01:51<02:57]\n",
      "Epoch [4/5]: [40/119]  34%|███▎      , loss=1.02 [01:51<02:57]\n",
      "Epoch [4/5]: [41/119]  34%|███▍      , loss=1.02 [01:51<04:05]\n",
      "2025-07-15 08:32:15,363 - INFO - Current learning rate: 0.000453868004805068\n",
      "2025-07-15 08:32:17,008 - INFO - Epoch: 4/5, Iter: 42/119 -- train_loss: 1.0177\n",
      "Epoch [4/5]: [41/119]  34%|███▍      , loss=1.02 [01:53<04:05]\n",
      "Epoch [4/5]: [41/119]  34%|███▍      , loss=1.02 [01:53<04:05]\n",
      "Epoch [4/5]: [42/119]  35%|███▌      , loss=1.02 [01:53<03:27]\n",
      "2025-07-15 08:32:17,010 - INFO - Current learning rate: 0.0004501140362727022\n",
      "2025-07-15 08:32:18,607 - INFO - Epoch: 4/5, Iter: 43/119 -- train_loss: 0.9681\n",
      "Epoch [4/5]: [42/119]  35%|███▌      , loss=1.02 [01:55<03:27]\n",
      "Epoch [4/5]: [42/119]  35%|███▌      , loss=0.968 [01:55<03:27]\n",
      "Epoch [4/5]: [43/119]  36%|███▌      , loss=0.968 [01:55<03:00]\n",
      "2025-07-15 08:32:18,609 - INFO - Current learning rate: 0.00044636290607402907\n",
      "2025-07-15 08:32:19,797 - INFO - Epoch: 4/5, Iter: 44/119 -- train_loss: 1.0141\n",
      "Epoch [4/5]: [43/119]  36%|███▌      , loss=0.968 [01:56<03:00]\n",
      "Epoch [4/5]: [43/119]  36%|███▌      , loss=1.01 [01:56<03:00]\n",
      "Epoch [4/5]: [44/119]  37%|███▋      , loss=1.01 [01:56<02:31]\n",
      "2025-07-15 08:32:19,799 - INFO - Current learning rate: 0.00044261482762644305\n",
      "2025-07-15 08:32:29,586 - INFO - Epoch: 4/5, Iter: 45/119 -- train_loss: 1.0201\n",
      "Epoch [4/5]: [44/119]  37%|███▋      , loss=1.01 [02:06<02:31]\n",
      "Epoch [4/5]: [44/119]  37%|███▋      , loss=1.02 [02:06<02:31]\n",
      "Epoch [4/5]: [45/119]  38%|███▊      , loss=1.02 [02:06<05:21]\n",
      "2025-07-15 08:32:29,587 - INFO - Current learning rate: 0.0004388700141737114\n",
      "2025-07-15 08:32:30,705 - INFO - Epoch: 4/5, Iter: 46/119 -- train_loss: 1.0085\n",
      "Epoch [4/5]: [45/119]  38%|███▊      , loss=1.02 [02:07<05:21]\n",
      "Epoch [4/5]: [45/119]  38%|███▊      , loss=1.01 [02:07<05:21]\n",
      "Epoch [4/5]: [46/119]  39%|███▊      , loss=1.01 [02:07<04:06]\n",
      "2025-07-15 08:32:30,707 - INFO - Current learning rate: 0.0004351286787738431\n",
      "2025-07-15 08:32:32,806 - INFO - Epoch: 4/5, Iter: 47/119 -- train_loss: 1.0205\n",
      "Epoch [4/5]: [46/119]  39%|███▊      , loss=1.01 [02:09<04:06]\n",
      "Epoch [4/5]: [46/119]  39%|███▊      , loss=1.02 [02:09<04:06]\n",
      "Epoch [4/5]: [47/119]  39%|███▉      , loss=1.02 [02:09<03:35]\n",
      "2025-07-15 08:32:32,807 - INFO - Current learning rate: 0.0004313910342869651\n",
      "2025-07-15 08:32:34,030 - INFO - Epoch: 4/5, Iter: 48/119 -- train_loss: 1.0203\n",
      "Epoch [4/5]: [47/119]  39%|███▉      , loss=1.02 [02:10<03:35]\n",
      "Epoch [4/5]: [47/119]  39%|███▉      , loss=1.02 [02:10<03:35]\n",
      "Epoch [4/5]: [48/119]  40%|████      , loss=1.02 [02:10<02:54]\n",
      "2025-07-15 08:32:34,031 - INFO - Current learning rate: 0.000427657293363214\n",
      "2025-07-15 08:32:44,217 - INFO - Epoch: 4/5, Iter: 49/119 -- train_loss: 0.9913\n",
      "Epoch [4/5]: [48/119]  40%|████      , loss=1.02 [02:20<02:54]\n",
      "Epoch [4/5]: [48/119]  40%|████      , loss=0.991 [02:20<02:54]\n",
      "Epoch [4/5]: [49/119]  41%|████      , loss=0.991 [02:20<05:34]\n",
      "2025-07-15 08:32:44,219 - INFO - Current learning rate: 0.00042392766843063554\n",
      "2025-07-15 08:32:45,371 - INFO - Epoch: 4/5, Iter: 50/119 -- train_loss: 1.0205\n",
      "Epoch [4/5]: [49/119]  41%|████      , loss=0.991 [02:21<05:34]\n",
      "Epoch [4/5]: [49/119]  41%|████      , loss=1.02 [02:21<05:34]\n",
      "Epoch [4/5]: [50/119]  42%|████▏     , loss=1.02 [02:21<04:14]\n",
      "2025-07-15 08:32:45,373 - INFO - Current learning rate: 0.00042020237168310053\n",
      "2025-07-15 08:32:56,717 - INFO - Epoch: 4/5, Iter: 51/119 -- train_loss: 1.0202\n",
      "Epoch [4/5]: [50/119]  42%|████▏     , loss=1.02 [02:33<04:14]\n",
      "Epoch [4/5]: [50/119]  42%|████▏     , loss=1.02 [02:33<04:14]\n",
      "Epoch [4/5]: [51/119]  43%|████▎     , loss=1.02 [02:33<06:47]\n",
      "2025-07-15 08:32:56,719 - INFO - Current learning rate: 0.0004164816150682305\n",
      "2025-07-15 08:32:58,065 - INFO - Epoch: 4/5, Iter: 52/119 -- train_loss: 1.0207\n",
      "Epoch [4/5]: [51/119]  43%|████▎     , loss=1.02 [02:34<06:47]\n",
      "Epoch [4/5]: [51/119]  43%|████▎     , loss=1.02 [02:34<06:47]\n",
      "Epoch [4/5]: [52/119]  44%|████▎     , loss=1.02 [02:34<05:07]\n",
      "2025-07-15 08:32:58,067 - INFO - Current learning rate: 0.0004127656102753405\n",
      "2025-07-15 08:32:59,571 - INFO - Epoch: 4/5, Iter: 53/119 -- train_loss: 1.0204\n",
      "Epoch [4/5]: [52/119]  44%|████▎     , loss=1.02 [02:36<05:07]\n",
      "Epoch [4/5]: [52/119]  44%|████▎     , loss=1.02 [02:36<05:07]\n",
      "Epoch [4/5]: [53/119]  45%|████▍     , loss=1.02 [02:36<04:02]\n",
      "2025-07-15 08:32:59,574 - INFO - Current learning rate: 0.0004090545687233947\n",
      "2025-07-15 08:33:00,995 - INFO - Epoch: 4/5, Iter: 54/119 -- train_loss: 1.0199\n",
      "Epoch [4/5]: [53/119]  45%|████▍     , loss=1.02 [02:37<04:02]\n",
      "Epoch [4/5]: [53/119]  45%|████▍     , loss=1.02 [02:37<04:02]\n",
      "Epoch [4/5]: [54/119]  45%|████▌     , loss=1.02 [02:37<03:14]\n",
      "2025-07-15 08:33:00,997 - INFO - Current learning rate: 0.0004053487015489772\n",
      "2025-07-15 08:33:10,303 - INFO - Epoch: 4/5, Iter: 55/119 -- train_loss: 0.9921\n",
      "Epoch [4/5]: [54/119]  45%|████▌     , loss=1.02 [02:46<03:14]\n",
      "Epoch [4/5]: [54/119]  45%|████▌     , loss=0.992 [02:46<03:14]\n",
      "Epoch [4/5]: [55/119]  46%|████▌     , loss=0.992 [02:46<05:12]\n",
      "2025-07-15 08:33:10,304 - INFO - Current learning rate: 0.0004016482195942813\n",
      "2025-07-15 08:33:11,450 - INFO - Epoch: 4/5, Iter: 56/119 -- train_loss: 1.0202\n",
      "Epoch [4/5]: [55/119]  46%|████▌     , loss=0.992 [02:48<05:12]\n",
      "Epoch [4/5]: [55/119]  46%|████▌     , loss=1.02 [02:48<05:12]\n",
      "Epoch [4/5]: [56/119]  47%|████▋     , loss=1.02 [02:48<03:57]\n",
      "2025-07-15 08:33:11,451 - INFO - Current learning rate: 0.00039795333339511153\n",
      "2025-07-15 08:33:13,047 - INFO - Epoch: 4/5, Iter: 57/119 -- train_loss: 1.0200\n",
      "Epoch [4/5]: [56/119]  47%|████▋     , loss=1.02 [02:49<03:57]\n",
      "Epoch [4/5]: [56/119]  47%|████▋     , loss=1.02 [02:49<03:57]\n",
      "Epoch [4/5]: [57/119]  48%|████▊     , loss=1.02 [02:49<03:13]\n",
      "2025-07-15 08:33:13,048 - INFO - Current learning rate: 0.00039426425316890773\n",
      "2025-07-15 08:33:14,227 - INFO - Epoch: 4/5, Iter: 58/119 -- train_loss: 1.0201\n",
      "Epoch [4/5]: [57/119]  48%|████▊     , loss=1.02 [02:50<03:13]\n",
      "Epoch [4/5]: [57/119]  48%|████▊     , loss=1.02 [02:50<03:13]\n",
      "Epoch [4/5]: [58/119]  49%|████▊     , loss=1.02 [02:50<02:34]\n",
      "2025-07-15 08:33:14,228 - INFO - Current learning rate: 0.00039058118880278296\n",
      "2025-07-15 08:33:18,150 - INFO - Epoch: 4/5, Iter: 59/119 -- train_loss: 1.0179\n",
      "Epoch [4/5]: [58/119]  49%|████▊     , loss=1.02 [02:54<02:34]\n",
      "Epoch [4/5]: [58/119]  49%|████▊     , loss=1.02 [02:54<02:34]\n",
      "Epoch [4/5]: [59/119]  50%|████▉     , loss=1.02 [02:54<02:57]\n",
      "2025-07-15 08:33:18,152 - INFO - Current learning rate: 0.00038690434984158374\n",
      "2025-07-15 08:33:19,364 - INFO - Epoch: 4/5, Iter: 60/119 -- train_loss: 1.0199\n",
      "Epoch [4/5]: [59/119]  50%|████▉     , loss=1.02 [02:55<02:57]\n",
      "Epoch [4/5]: [59/119]  50%|████▉     , loss=1.02 [02:55<02:57]\n",
      "Epoch [4/5]: [60/119]  50%|█████     , loss=1.02 [02:55<02:23]\n",
      "2025-07-15 08:33:19,365 - INFO - Current learning rate: 0.00038323394547596653\n",
      "2025-07-15 08:33:20,491 - INFO - Epoch: 4/5, Iter: 61/119 -- train_loss: 0.9996\n",
      "Epoch [4/5]: [60/119]  50%|█████     , loss=1.02 [02:57<02:23]\n",
      "Epoch [4/5]: [60/119]  50%|█████     , loss=1 [02:57<02:23]\n",
      "Epoch [4/5]: [61/119]  51%|█████▏    , loss=1 [02:57<01:58]\n",
      "2025-07-15 08:33:20,492 - INFO - Current learning rate: 0.0003795701845304978\n",
      "2025-07-15 08:33:21,949 - INFO - Epoch: 4/5, Iter: 62/119 -- train_loss: 1.0192\n",
      "Epoch [4/5]: [61/119]  51%|█████▏    , loss=1 [02:58<01:58]\n",
      "Epoch [4/5]: [61/119]  51%|█████▏    , loss=1.02 [02:58<01:58]\n",
      "Epoch [4/5]: [62/119]  52%|█████▏    , loss=1.02 [02:58<01:46]\n",
      "2025-07-15 08:33:21,951 - INFO - Current learning rate: 0.00037591327545177176\n",
      "2025-07-15 08:33:29,071 - INFO - Epoch: 4/5, Iter: 63/119 -- train_loss: 1.0156\n",
      "Epoch [4/5]: [62/119]  52%|█████▏    , loss=1.02 [03:05<01:46]\n",
      "Epoch [4/5]: [62/119]  52%|█████▏    , loss=1.02 [03:05<01:46]\n",
      "Epoch [4/5]: [63/119]  53%|█████▎    , loss=1.02 [03:05<03:12]\n",
      "2025-07-15 08:33:29,072 - INFO - Current learning rate: 0.00037226342629655124\n",
      "2025-07-15 08:33:30,369 - INFO - Epoch: 4/5, Iter: 64/119 -- train_loss: 1.0200\n",
      "Epoch [4/5]: [63/119]  53%|█████▎    , loss=1.02 [03:06<03:12]\n",
      "Epoch [4/5]: [63/119]  53%|█████▎    , loss=1.02 [03:06<03:12]\n",
      "Epoch [4/5]: [64/119]  54%|█████▍    , loss=1.02 [03:06<02:33]\n",
      "2025-07-15 08:33:30,370 - INFO - Current learning rate: 0.000368620844719931\n",
      "2025-07-15 08:33:31,562 - INFO - Epoch: 4/5, Iter: 65/119 -- train_loss: 1.0087\n",
      "Epoch [4/5]: [64/119]  54%|█████▍    , loss=1.02 [03:08<02:33]\n",
      "Epoch [4/5]: [64/119]  54%|█████▍    , loss=1.01 [03:08<02:33]\n",
      "Epoch [4/5]: [65/119]  55%|█████▍    , loss=1.01 [03:08<02:05]\n",
      "2025-07-15 08:33:31,563 - INFO - Current learning rate: 0.00036498573796352326\n",
      "2025-07-15 08:33:35,068 - INFO - Epoch: 4/5, Iter: 66/119 -- train_loss: 1.0188\n",
      "Epoch [4/5]: [65/119]  55%|█████▍    , loss=1.01 [03:11<02:05]\n",
      "Epoch [4/5]: [65/119]  55%|█████▍    , loss=1.02 [03:11<02:05]\n",
      "Epoch [4/5]: [66/119]  55%|█████▌    , loss=1.02 [03:11<02:21]\n",
      "2025-07-15 08:33:35,069 - INFO - Current learning rate: 0.0003613583128436658\n",
      "2025-07-15 08:33:37,293 - INFO - Epoch: 4/5, Iter: 67/119 -- train_loss: 1.0081\n",
      "Epoch [4/5]: [66/119]  55%|█████▌    , loss=1.02 [03:13<02:21]\n",
      "Epoch [4/5]: [66/119]  55%|█████▌    , loss=1.01 [03:13<02:21]\n",
      "Epoch [4/5]: [67/119]  56%|█████▋    , loss=1.01 [03:13<02:12]\n",
      "2025-07-15 08:33:37,294 - INFO - Current learning rate: 0.00035773877573965705\n",
      "2025-07-15 08:33:38,633 - INFO - Epoch: 4/5, Iter: 68/119 -- train_loss: 1.0136\n",
      "Epoch [4/5]: [67/119]  56%|█████▋    , loss=1.01 [03:15<02:12]\n",
      "Epoch [4/5]: [67/119]  56%|█████▋    , loss=1.01 [03:15<02:12]\n",
      "Epoch [4/5]: [68/119]  57%|█████▋    , loss=1.01 [03:15<01:51]\n",
      "2025-07-15 08:33:38,634 - INFO - Current learning rate: 0.00035412733258201285\n",
      "2025-07-15 08:33:45,369 - INFO - Epoch: 4/5, Iter: 69/119 -- train_loss: 0.9908\n",
      "Epoch [4/5]: [68/119]  57%|█████▋    , loss=1.01 [03:21<01:51]\n",
      "Epoch [4/5]: [68/119]  57%|█████▋    , loss=0.991 [03:21<01:51]\n",
      "Epoch [4/5]: [69/119]  58%|█████▊    , loss=0.991 [03:21<02:57]\n",
      "2025-07-15 08:33:45,370 - INFO - Current learning rate: 0.00035052418884075115\n",
      "2025-07-15 08:33:47,264 - INFO - Epoch: 4/5, Iter: 70/119 -- train_loss: 1.0038\n",
      "Epoch [4/5]: [69/119]  58%|█████▊    , loss=0.991 [03:23<02:57]\n",
      "Epoch [4/5]: [69/119]  58%|█████▊    , loss=1 [03:23<02:57]\n",
      "Epoch [4/5]: [70/119]  59%|█████▉    , loss=1 [03:23<02:29]\n",
      "2025-07-15 08:33:47,266 - INFO - Current learning rate: 0.0003469295495137012\n",
      "2025-07-15 08:33:48,674 - INFO - Epoch: 4/5, Iter: 71/119 -- train_loss: 1.0083\n",
      "Epoch [4/5]: [70/119]  59%|█████▉    , loss=1 [03:25<02:29]\n",
      "Epoch [4/5]: [70/119]  59%|█████▉    , loss=1.01 [03:25<02:29]\n",
      "Epoch [4/5]: [71/119]  60%|█████▉    , loss=1.01 [03:25<02:02]\n",
      "2025-07-15 08:33:48,676 - INFO - Current learning rate: 0.0003433436191148412\n",
      "2025-07-15 08:33:49,974 - INFO - Epoch: 4/5, Iter: 72/119 -- train_loss: 1.0192\n",
      "Epoch [4/5]: [71/119]  60%|█████▉    , loss=1.01 [03:26<02:02]\n",
      "Epoch [4/5]: [71/119]  60%|█████▉    , loss=1.02 [03:26<02:02]\n",
      "Epoch [4/5]: [72/119]  61%|██████    , loss=1.02 [03:26<01:42]\n",
      "2025-07-15 08:33:49,976 - INFO - Current learning rate: 0.00033976660166266167\n",
      "2025-07-15 08:33:54,071 - INFO - Epoch: 4/5, Iter: 73/119 -- train_loss: 0.9897\n",
      "Epoch [4/5]: [72/119]  61%|██████    , loss=1.02 [03:30<01:42]\n",
      "Epoch [4/5]: [72/119]  61%|██████    , loss=0.99 [03:30<01:42]\n",
      "Epoch [4/5]: [73/119]  61%|██████▏   , loss=0.99 [03:30<02:06]\n",
      "2025-07-15 08:33:54,073 - INFO - Current learning rate: 0.00033619870066855926\n",
      "2025-07-15 08:34:11,148 - INFO - Epoch: 4/5, Iter: 74/119 -- train_loss: 1.0192\n",
      "Epoch [4/5]: [73/119]  61%|██████▏   , loss=0.99 [03:47<02:06]\n",
      "Epoch [4/5]: [73/119]  61%|██████▏   , loss=1.02 [03:47<02:06]\n",
      "Epoch [4/5]: [74/119]  62%|██████▏   , loss=1.02 [03:47<05:17]\n",
      "2025-07-15 08:34:11,149 - INFO - Current learning rate: 0.00033264011912525663\n",
      "2025-07-15 08:34:12,668 - INFO - Epoch: 4/5, Iter: 75/119 -- train_loss: 0.9852\n",
      "Epoch [4/5]: [74/119]  62%|██████▏   , loss=1.02 [03:49<05:17]\n",
      "Epoch [4/5]: [74/119]  62%|██████▏   , loss=0.985 [03:49<05:17]\n",
      "Epoch [4/5]: [75/119]  63%|██████▎   , loss=0.985 [03:49<03:57]\n",
      "2025-07-15 08:34:12,670 - INFO - Current learning rate: 0.00032909105949525475\n",
      "2025-07-15 08:34:14,059 - INFO - Epoch: 4/5, Iter: 76/119 -- train_loss: 1.0193\n",
      "Epoch [4/5]: [75/119]  63%|██████▎   , loss=0.985 [03:50<03:57]\n",
      "Epoch [4/5]: [75/119]  63%|██████▎   , loss=1.02 [03:50<03:57]\n",
      "Epoch [4/5]: [76/119]  64%|██████▍   , loss=1.02 [03:50<03:00]\n",
      "2025-07-15 08:34:14,060 - INFO - Current learning rate: 0.0003255517236993131\n",
      "2025-07-15 08:34:15,676 - INFO - Epoch: 4/5, Iter: 77/119 -- train_loss: 1.0066\n",
      "Epoch [4/5]: [76/119]  64%|██████▍   , loss=1.02 [03:52<03:00]\n",
      "Epoch [4/5]: [76/119]  64%|██████▍   , loss=1.01 [03:52<03:00]\n",
      "Epoch [4/5]: [77/119]  65%|██████▍   , loss=1.01 [03:52<02:23]\n",
      "2025-07-15 08:34:15,678 - INFO - Current learning rate: 0.0003220223131049617\n",
      "2025-07-15 08:34:26,065 - INFO - Epoch: 4/5, Iter: 78/119 -- train_loss: 1.0195\n",
      "Epoch [4/5]: [77/119]  65%|██████▍   , loss=1.01 [04:02<02:23]\n",
      "Epoch [4/5]: [77/119]  65%|██████▍   , loss=1.02 [04:02<02:23]\n",
      "Epoch [4/5]: [78/119]  66%|██████▌   , loss=1.02 [04:02<03:45]\n",
      "2025-07-15 08:34:26,066 - INFO - Current learning rate: 0.00031850302851504464\n",
      "2025-07-15 08:34:27,142 - INFO - Epoch: 4/5, Iter: 79/119 -- train_loss: 1.0191\n",
      "Epoch [4/5]: [78/119]  66%|██████▌   , loss=1.02 [04:03<03:45]\n",
      "Epoch [4/5]: [78/119]  66%|██████▌   , loss=1.02 [04:03<03:45]\n",
      "Epoch [4/5]: [79/119]  66%|██████▋   , loss=1.02 [04:03<02:47]\n",
      "2025-07-15 08:34:27,143 - INFO - Current learning rate: 0.0003149940701562959\n",
      "2025-07-15 08:34:28,705 - INFO - Epoch: 4/5, Iter: 80/119 -- train_loss: 1.0191\n",
      "Epoch [4/5]: [79/119]  66%|██████▋   , loss=1.02 [04:05<02:47]\n",
      "Epoch [4/5]: [79/119]  66%|██████▋   , loss=1.02 [04:05<02:47]\n",
      "Epoch [4/5]: [80/119]  67%|██████▋   , loss=1.02 [04:05<02:12]\n",
      "2025-07-15 08:34:28,706 - INFO - Current learning rate: 0.00031149563766794656\n",
      "2025-07-15 08:34:30,448 - INFO - Epoch: 4/5, Iter: 81/119 -- train_loss: 1.0036\n",
      "Epoch [4/5]: [80/119]  67%|██████▋   , loss=1.02 [04:07<02:12]\n",
      "Epoch [4/5]: [80/119]  67%|██████▋   , loss=1 [04:07<02:12]\n",
      "Epoch [4/5]: [81/119]  68%|██████▊   , loss=1 [04:07<01:50]\n",
      "2025-07-15 08:34:30,450 - INFO - Current learning rate: 0.0003080079300903678\n",
      "2025-07-15 08:34:46,131 - INFO - Epoch: 4/5, Iter: 82/119 -- train_loss: 1.0189\n",
      "Epoch [4/5]: [81/119]  68%|██████▊   , loss=1 [04:22<01:50]\n",
      "Epoch [4/5]: [81/119]  68%|██████▊   , loss=1.02 [04:22<01:50]\n",
      "Epoch [4/5]: [82/119]  69%|██████▉   , loss=1.02 [04:22<04:09]\n",
      "2025-07-15 08:34:46,132 - INFO - Current learning rate: 0.0003045311458537454\n",
      "2025-07-15 08:34:47,437 - INFO - Epoch: 4/5, Iter: 83/119 -- train_loss: 1.0195\n",
      "Epoch [4/5]: [82/119]  69%|██████▉   , loss=1.02 [04:24<04:09]\n",
      "Epoch [4/5]: [82/119]  69%|██████▉   , loss=1.02 [04:24<04:09]\n",
      "Epoch [4/5]: [83/119]  70%|██████▉   , loss=1.02 [04:24<03:03]\n",
      "2025-07-15 08:34:47,438 - INFO - Current learning rate: 0.00030106548276679154\n",
      "2025-07-15 08:34:48,749 - INFO - Epoch: 4/5, Iter: 84/119 -- train_loss: 1.0145\n",
      "Epoch [4/5]: [83/119]  70%|██████▉   , loss=1.02 [04:25<03:03]\n",
      "Epoch [4/5]: [83/119]  70%|██████▉   , loss=1.01 [04:25<03:03]\n",
      "Epoch [4/5]: [84/119]  71%|███████   , loss=1.01 [04:25<02:18]\n",
      "2025-07-15 08:34:48,750 - INFO - Current learning rate: 0.00029761113800548934\n",
      "2025-07-15 08:34:50,198 - INFO - Epoch: 4/5, Iter: 85/119 -- train_loss: 1.0190\n",
      "Epoch [4/5]: [84/119]  71%|███████   , loss=1.01 [04:26<02:18]\n",
      "Epoch [4/5]: [84/119]  71%|███████   , loss=1.02 [04:26<02:18]\n",
      "Epoch [4/5]: [85/119]  71%|███████▏  , loss=1.02 [04:26<01:49]\n",
      "2025-07-15 08:34:50,199 - INFO - Current learning rate: 0.0002941683081018758\n",
      "2025-07-15 08:34:53,418 - INFO - Epoch: 4/5, Iter: 86/119 -- train_loss: 1.0114\n",
      "Epoch [4/5]: [85/119]  71%|███████▏  , loss=1.02 [04:30<01:49]\n",
      "Epoch [4/5]: [85/119]  71%|███████▏  , loss=1.01 [04:30<01:49]\n",
      "Epoch [4/5]: [86/119]  72%|███████▏  , loss=1.01 [04:30<01:46]\n",
      "2025-07-15 08:34:53,419 - INFO - Current learning rate: 0.0002907371889328593\n",
      "2025-07-15 08:34:54,542 - INFO - Epoch: 4/5, Iter: 87/119 -- train_loss: 1.0192\n",
      "Epoch [4/5]: [86/119]  72%|███████▏  , loss=1.01 [04:31<01:46]\n",
      "Epoch [4/5]: [86/119]  72%|███████▏  , loss=1.02 [04:31<01:46]\n",
      "Epoch [4/5]: [87/119]  73%|███████▎  , loss=1.02 [04:31<01:22]\n",
      "2025-07-15 08:34:54,543 - INFO - Current learning rate: 0.0002873179757090765\n",
      "2025-07-15 08:34:55,799 - INFO - Epoch: 4/5, Iter: 88/119 -- train_loss: 0.9828\n",
      "Epoch [4/5]: [87/119]  73%|███████▎  , loss=1.02 [04:32<01:22]\n",
      "Epoch [4/5]: [87/119]  73%|███████▎  , loss=0.983 [04:32<01:22]\n",
      "Epoch [4/5]: [88/119]  74%|███████▍  , loss=0.983 [04:32<01:07]\n",
      "2025-07-15 08:34:55,801 - INFO - Current learning rate: 0.0002839108629637845\n",
      "2025-07-15 08:34:57,286 - INFO - Epoch: 4/5, Iter: 89/119 -- train_loss: 1.0155\n",
      "Epoch [4/5]: [88/119]  74%|███████▍  , loss=0.983 [04:33<01:07]\n",
      "Epoch [4/5]: [88/119]  74%|███████▍  , loss=1.02 [04:33<01:07]\n",
      "Epoch [4/5]: [89/119]  75%|███████▍  , loss=1.02 [04:33<00:59]\n",
      "2025-07-15 08:34:57,287 - INFO - Current learning rate: 0.00028051604454179465\n",
      "2025-07-15 08:35:03,159 - INFO - Epoch: 4/5, Iter: 90/119 -- train_loss: 1.0156\n",
      "Epoch [4/5]: [89/119]  75%|███████▍  , loss=1.02 [04:39<00:59]\n",
      "Epoch [4/5]: [89/119]  75%|███████▍  , loss=1.02 [04:39<00:59]\n",
      "Epoch [4/5]: [90/119]  76%|███████▌  , loss=1.02 [04:39<01:31]\n",
      "2025-07-15 08:35:03,160 - INFO - Current learning rate: 0.00027713371358844245\n",
      "2025-07-15 08:35:04,484 - INFO - Epoch: 4/5, Iter: 91/119 -- train_loss: 1.0208\n",
      "Epoch [4/5]: [90/119]  76%|███████▌  , loss=1.02 [04:41<01:31]\n",
      "Epoch [4/5]: [90/119]  76%|███████▌  , loss=1.02 [04:41<01:31]\n",
      "Epoch [4/5]: [91/119]  76%|███████▋  , loss=1.02 [04:41<01:12]\n",
      "2025-07-15 08:35:04,486 - INFO - Current learning rate: 0.0002737640625386001\n",
      "2025-07-15 08:35:05,796 - INFO - Epoch: 4/5, Iter: 92/119 -- train_loss: 0.9511\n",
      "Epoch [4/5]: [91/119]  76%|███████▋  , loss=1.02 [04:42<01:12]\n",
      "Epoch [4/5]: [91/119]  76%|███████▋  , loss=0.951 [04:42<01:12]\n",
      "Epoch [4/5]: [92/119]  77%|███████▋  , loss=0.951 [04:42<00:59]\n",
      "2025-07-15 08:35:05,798 - INFO - Current learning rate: 0.00027040728310572654\n",
      "2025-07-15 08:35:07,030 - INFO - Epoch: 4/5, Iter: 93/119 -- train_loss: 0.9630\n",
      "Epoch [4/5]: [92/119]  77%|███████▋  , loss=0.951 [04:43<00:59]\n",
      "Epoch [4/5]: [92/119]  77%|███████▋  , loss=0.963 [04:43<00:59]\n",
      "Epoch [4/5]: [93/119]  78%|███████▊  , loss=0.963 [04:43<00:49]\n",
      "2025-07-15 08:35:07,032 - INFO - Current learning rate: 0.00026706356627096165\n",
      "2025-07-15 08:35:19,786 - INFO - Epoch: 4/5, Iter: 94/119 -- train_loss: 1.0190\n",
      "Epoch [4/5]: [93/119]  78%|███████▊  , loss=0.963 [04:56<00:49]\n",
      "Epoch [4/5]: [93/119]  78%|███████▊  , loss=1.02 [04:56<00:49]\n",
      "Epoch [4/5]: [94/119]  79%|███████▉  , loss=1.02 [04:56<02:09]\n",
      "2025-07-15 08:35:19,787 - INFO - Current learning rate: 0.0002637331022722593\n",
      "2025-07-15 08:35:21,290 - INFO - Epoch: 4/5, Iter: 95/119 -- train_loss: 1.0162\n",
      "Epoch [4/5]: [94/119]  79%|███████▉  , loss=1.02 [04:57<02:09]\n",
      "Epoch [4/5]: [94/119]  79%|███████▉  , loss=1.02 [04:57<02:09]\n",
      "Epoch [4/5]: [95/119]  80%|███████▉  , loss=1.02 [04:57<01:37]\n",
      "2025-07-15 08:35:21,292 - INFO - Current learning rate: 0.00026041608059356483\n",
      "2025-07-15 08:35:22,728 - INFO - Epoch: 4/5, Iter: 96/119 -- train_loss: 1.0185\n",
      "Epoch [4/5]: [95/119]  80%|███████▉  , loss=1.02 [04:59<01:37]\n",
      "Epoch [4/5]: [95/119]  80%|███████▉  , loss=1.02 [04:59<01:37]\n",
      "Epoch [4/5]: [96/119]  81%|████████  , loss=1.02 [04:59<01:15]\n",
      "2025-07-15 08:35:22,729 - INFO - Current learning rate: 0.0002571126899540335\n",
      "2025-07-15 08:35:24,160 - INFO - Epoch: 4/5, Iter: 97/119 -- train_loss: 1.0187\n",
      "Epoch [4/5]: [96/119]  81%|████████  , loss=1.02 [05:00<01:15]\n",
      "Epoch [4/5]: [96/119]  81%|████████  , loss=1.02 [05:00<01:15]\n",
      "Epoch [4/5]: [97/119]  82%|████████▏ , loss=1.02 [05:00<00:59]\n",
      "2025-07-15 08:35:24,162 - INFO - Current learning rate: 0.00025382311829729457\n",
      "2025-07-15 08:35:29,382 - INFO - Epoch: 4/5, Iter: 98/119 -- train_loss: 1.0087\n",
      "Epoch [4/5]: [97/119]  82%|████████▏ , loss=1.02 [05:06<00:59]\n",
      "Epoch [4/5]: [97/119]  82%|████████▏ , loss=1.01 [05:06<00:59]\n",
      "Epoch [4/5]: [98/119]  82%|████████▏ , loss=1.01 [05:06<01:12]\n",
      "2025-07-15 08:35:29,384 - INFO - Current learning rate: 0.000250547552780758\n",
      "2025-07-15 08:35:30,698 - INFO - Epoch: 4/5, Iter: 99/119 -- train_loss: 1.0190\n",
      "Epoch [4/5]: [98/119]  82%|████████▏ , loss=1.01 [05:07<01:12]\n",
      "Epoch [4/5]: [98/119]  82%|████████▏ , loss=1.02 [05:07<01:12]\n",
      "Epoch [4/5]: [99/119]  83%|████████▎ , loss=1.02 [05:07<00:56]\n",
      "2025-07-15 08:35:30,700 - INFO - Current learning rate: 0.0002472861797649657\n",
      "2025-07-15 08:35:31,951 - INFO - Epoch: 4/5, Iter: 100/119 -- train_loss: 1.0177\n",
      "Epoch [4/5]: [99/119]  83%|████████▎ , loss=1.02 [05:08<00:56]\n",
      "Epoch [4/5]: [99/119]  83%|████████▎ , loss=1.02 [05:08<00:56]\n",
      "Epoch [4/5]: [100/119]  84%|████████▍ , loss=1.02 [05:08<00:44]\n",
      "2025-07-15 08:35:31,952 - INFO - Current learning rate: 0.00024403918480298947\n",
      "2025-07-15 08:35:33,093 - INFO - Epoch: 4/5, Iter: 101/119 -- train_loss: 1.0187\n",
      "Epoch [4/5]: [100/119]  84%|████████▍ , loss=1.02 [05:09<00:44]\n",
      "Epoch [4/5]: [100/119]  84%|████████▍ , loss=1.02 [05:09<00:44]\n",
      "Epoch [4/5]: [101/119]  85%|████████▍ , loss=1.02 [05:09<00:35]\n",
      "2025-07-15 08:35:33,094 - INFO - Current learning rate: 0.0002408067526298741\n",
      "2025-07-15 08:35:42,367 - INFO - Epoch: 4/5, Iter: 102/119 -- train_loss: 0.9653\n",
      "Epoch [4/5]: [101/119]  85%|████████▍ , loss=1.02 [05:18<00:35]\n",
      "Epoch [4/5]: [101/119]  85%|████████▍ , loss=0.965 [05:18<00:35]\n",
      "Epoch [4/5]: [102/119]  86%|████████▌ , loss=0.965 [05:18<01:10]\n",
      "2025-07-15 08:35:42,369 - INFO - Current learning rate: 0.00023758906715212614\n",
      "2025-07-15 08:35:43,625 - INFO - Epoch: 4/5, Iter: 103/119 -- train_loss: 1.0144\n",
      "Epoch [4/5]: [102/119]  86%|████████▌ , loss=0.965 [05:20<01:10]\n",
      "Epoch [4/5]: [102/119]  86%|████████▌ , loss=1.01 [05:20<01:10]\n",
      "Epoch [4/5]: [103/119]  87%|████████▋ , loss=1.01 [05:20<00:52]\n",
      "2025-07-15 08:35:43,627 - INFO - Current learning rate: 0.00023438631143725205\n",
      "2025-07-15 08:35:44,883 - INFO - Epoch: 4/5, Iter: 104/119 -- train_loss: 1.0160\n",
      "Epoch [4/5]: [103/119]  87%|████████▋ , loss=1.01 [05:21<00:52]\n",
      "Epoch [4/5]: [103/119]  87%|████████▋ , loss=1.02 [05:21<00:52]\n",
      "Epoch [4/5]: [104/119]  87%|████████▋ , loss=1.02 [05:21<00:40]\n",
      "2025-07-15 08:35:44,885 - INFO - Current learning rate: 0.0002311986677033413\n",
      "2025-07-15 08:35:45,978 - INFO - Epoch: 4/5, Iter: 105/119 -- train_loss: 1.0188\n",
      "Epoch [4/5]: [104/119]  87%|████████▋ , loss=1.02 [05:22<00:40]\n",
      "Epoch [4/5]: [104/119]  87%|████████▋ , loss=1.02 [05:22<00:40]\n",
      "Epoch [4/5]: [105/119]  88%|████████▊ , loss=1.02 [05:22<00:30]\n",
      "2025-07-15 08:35:45,979 - INFO - Current learning rate: 0.00022802631730870062\n",
      "2025-07-15 08:35:52,880 - INFO - Epoch: 4/5, Iter: 106/119 -- train_loss: 1.0165\n",
      "Epoch [4/5]: [105/119]  88%|████████▊ , loss=1.02 [05:29<00:30]\n",
      "Epoch [4/5]: [105/119]  88%|████████▊ , loss=1.02 [05:29<00:30]\n",
      "Epoch [4/5]: [106/119]  89%|████████▉ , loss=1.02 [05:29<00:47]\n",
      "2025-07-15 08:35:52,881 - INFO - Current learning rate: 0.00022486944074153432\n",
      "2025-07-15 08:35:54,025 - INFO - Epoch: 4/5, Iter: 107/119 -- train_loss: 1.0178\n",
      "Epoch [4/5]: [106/119]  89%|████████▉ , loss=1.02 [05:30<00:47]\n",
      "Epoch [4/5]: [106/119]  89%|████████▉ , loss=1.02 [05:30<00:47]\n",
      "Epoch [4/5]: [107/119]  90%|████████▉ , loss=1.02 [05:30<00:34]\n",
      "2025-07-15 08:35:54,026 - INFO - Current learning rate: 0.00022172821760967716\n",
      "2025-07-15 08:35:55,720 - INFO - Epoch: 4/5, Iter: 108/119 -- train_loss: 0.9908\n",
      "Epoch [4/5]: [107/119]  90%|████████▉ , loss=1.02 [05:32<00:34]\n",
      "Epoch [4/5]: [107/119]  90%|████████▉ , loss=0.991 [05:32<00:34]\n",
      "Epoch [4/5]: [108/119]  91%|█████████ , loss=0.991 [05:32<00:27]\n",
      "2025-07-15 08:35:55,722 - INFO - Current learning rate: 0.00021860282663037402\n",
      "2025-07-15 08:35:56,852 - INFO - Epoch: 4/5, Iter: 109/119 -- train_loss: 1.0061\n",
      "Epoch [4/5]: [108/119]  91%|█████████ , loss=0.991 [05:33<00:27]\n",
      "Epoch [4/5]: [108/119]  91%|█████████ , loss=1.01 [05:33<00:27]\n",
      "Epoch [4/5]: [109/119]  92%|█████████▏, loss=1.01 [05:33<00:21]\n",
      "2025-07-15 08:35:56,854 - INFO - Current learning rate: 0.00021549344562011365\n",
      "2025-07-15 08:36:04,004 - INFO - Epoch: 4/5, Iter: 110/119 -- train_loss: 1.0082\n",
      "Epoch [4/5]: [109/119]  92%|█████████▏, loss=1.01 [05:40<00:21]\n",
      "Epoch [4/5]: [109/119]  92%|█████████▏, loss=1.01 [05:40<00:21]\n",
      "Epoch [4/5]: [110/119]  92%|█████████▏, loss=1.01 [05:40<00:32]\n",
      "2025-07-15 08:36:04,005 - INFO - Current learning rate: 0.00021240025148451026\n",
      "2025-07-15 08:36:05,410 - INFO - Epoch: 4/5, Iter: 111/119 -- train_loss: 1.0170\n",
      "Epoch [4/5]: [110/119]  92%|█████████▏, loss=1.01 [05:42<00:32]\n",
      "Epoch [4/5]: [110/119]  92%|█████████▏, loss=1.02 [05:42<00:32]\n",
      "Epoch [4/5]: [111/119]  93%|█████████▎, loss=1.02 [05:42<00:23]\n",
      "2025-07-15 08:36:05,411 - INFO - Current learning rate: 0.00020932342020824017\n",
      "2025-07-15 08:36:07,111 - INFO - Epoch: 4/5, Iter: 112/119 -- train_loss: 1.0161\n",
      "Epoch [4/5]: [111/119]  93%|█████████▎, loss=1.02 [05:43<00:23]\n",
      "Epoch [4/5]: [111/119]  93%|█████████▎, loss=1.02 [05:43<00:23]\n",
      "Epoch [4/5]: [112/119]  94%|█████████▍, loss=1.02 [05:43<00:18]\n",
      "2025-07-15 08:36:07,113 - INFO - Current learning rate: 0.00020626312684502798\n",
      "2025-07-15 08:36:08,556 - INFO - Epoch: 4/5, Iter: 113/119 -- train_loss: 1.0175\n",
      "Epoch [4/5]: [112/119]  94%|█████████▍, loss=1.02 [05:45<00:18]\n",
      "Epoch [4/5]: [112/119]  94%|█████████▍, loss=1.02 [05:45<00:18]\n",
      "Epoch [4/5]: [113/119]  95%|█████████▍, loss=1.02 [05:45<00:13]\n",
      "2025-07-15 08:36:08,558 - INFO - Current learning rate: 0.00020321954550768837\n",
      "2025-07-15 08:36:18,865 - INFO - Epoch: 4/5, Iter: 114/119 -- train_loss: 1.0184\n",
      "Epoch [4/5]: [113/119]  95%|█████████▍, loss=1.02 [05:55<00:13]\n",
      "Epoch [4/5]: [113/119]  95%|█████████▍, loss=1.02 [05:55<00:13]\n",
      "Epoch [4/5]: [114/119]  96%|█████████▌, loss=1.02 [05:55<00:23]\n",
      "2025-07-15 08:36:18,866 - INFO - Current learning rate: 0.00020019284935821852\n",
      "2025-07-15 08:36:19,933 - INFO - Epoch: 4/5, Iter: 115/119 -- train_loss: 0.9810\n",
      "Epoch [4/5]: [114/119]  96%|█████████▌, loss=1.02 [05:56<00:23]\n",
      "Epoch [4/5]: [114/119]  96%|█████████▌, loss=0.981 [05:56<00:23]\n",
      "Epoch [4/5]: [115/119]  97%|█████████▋, loss=0.981 [05:56<00:14]\n",
      "2025-07-15 08:36:19,935 - INFO - Current learning rate: 0.00019718321059794783\n",
      "2025-07-15 08:36:20,990 - INFO - Epoch: 4/5, Iter: 116/119 -- train_loss: 1.0171\n",
      "Epoch [4/5]: [115/119]  97%|█████████▋, loss=0.981 [05:57<00:14]\n",
      "Epoch [4/5]: [115/119]  97%|█████████▋, loss=1.02 [05:57<00:14]\n",
      "Epoch [4/5]: [116/119]  97%|█████████▋, loss=1.02 [05:57<00:08]\n",
      "2025-07-15 08:36:20,991 - INFO - Current learning rate: 0.0001941908004577393\n",
      "2025-07-15 08:36:22,050 - INFO - Epoch: 4/5, Iter: 117/119 -- train_loss: 1.0156\n",
      "Epoch [4/5]: [116/119]  97%|█████████▋, loss=1.02 [05:58<00:08]\n",
      "Epoch [4/5]: [116/119]  97%|█████████▋, loss=1.02 [05:58<00:08]\n",
      "Epoch [4/5]: [117/119]  98%|█████████▊, loss=1.02 [05:58<00:04]\n",
      "2025-07-15 08:36:22,052 - INFO - Current learning rate: 0.00019121578918824866\n",
      "2025-07-15 08:36:26,681 - INFO - Epoch: 4/5, Iter: 118/119 -- train_loss: 1.0205\n",
      "Epoch [4/5]: [117/119]  98%|█████████▊, loss=1.02 [06:03<00:04]\n",
      "Epoch [4/5]: [117/119]  98%|█████████▊, loss=1.02 [06:03<00:04]\n",
      "Epoch [4/5]: [118/119]  99%|█████████▉, loss=1.02 [06:03<00:02]\n",
      "2025-07-15 08:36:26,683 - INFO - Current learning rate: 0.00018825834605023698\n",
      "2025-07-15 08:36:27,336 - INFO - Epoch: 4/5, Iter: 119/119 -- train_loss: 1.0159\n",
      "Epoch [4/5]: [118/119]  99%|█████████▉, loss=1.02 [06:03<00:02]\n",
      "Epoch [4/5]: [118/119]  99%|█████████▉, loss=1.02 [06:03<00:02]\n",
      "Epoch [4/5]: [119/119] 100%|██████████, loss=1.02 [06:03<00:00]\n",
      "2025-07-15 08:36:27,337 - INFO - Current learning rate: 0.00018531863930494187\n",
      "2025-07-15 08:36:27,338 - INFO - Engine run resuming from iteration 0, epoch 3 until 4 epochs\n",
      "[1/20]   5%|#033[32m▌         #033[0m [00:00<?]#033[A\n",
      "Epoch [4/4]: [1/20]   5%|#033[32m▌         #033[0m [00:00<?]#033[A\n",
      "Epoch [4/4]: [1/20]   5%|#033[32m▌         #033[0m [00:08<?]#033[A\n",
      "Epoch [4/4]: [2/20]  10%|#033[32m█         #033[0m [00:08<02:39]#033[A\n",
      "Epoch [4/4]: [2/20]  10%|#033[32m█         #033[0m [00:17<02:39]#033[A\n",
      "Epoch [4/4]: [3/20]  15%|#033[32m█▌        #033[0m [00:17<02:28]#033[A\n",
      "Epoch [4/4]: [3/20]  15%|#033[32m█▌        #033[0m [00:23<02:28]#033[A\n",
      "Epoch [4/4]: [4/20]  20%|#033[32m██        #033[0m [00:23<02:00]#033[A\n",
      "Epoch [4/4]: [4/20]  20%|#033[32m██        #033[0m [00:30<02:00]#033[A\n",
      "Epoch [4/4]: [5/20]  25%|#033[32m██▌       #033[0m [00:30<01:49]#033[A\n",
      "Epoch [4/4]: [5/20]  25%|#033[32m██▌       #033[0m [00:38<01:49]#033[A\n",
      "Epoch [4/4]: [6/20]  30%|#033[32m███       #033[0m [00:38<01:42]#033[A\n",
      "Epoch [4/4]: [6/20]  30%|#033[32m███       #033[0m [00:46<01:42]#033[A\n",
      "Epoch [4/4]: [7/20]  35%|#033[32m███▌      #033[0m [00:46<01:40]#033[A\n",
      "Epoch [4/4]: [7/20]  35%|#033[32m███▌      #033[0m [00:54<01:40]#033[A\n",
      "Epoch [4/4]: [8/20]  40%|#033[32m████      #033[0m [00:54<01:34]#033[A\n",
      "Epoch [4/4]: [8/20]  40%|#033[32m████      #033[0m [01:01<01:34]#033[A\n",
      "Epoch [4/4]: [9/20]  45%|#033[32m████▌     #033[0m [01:01<01:21]#033[A\n",
      "Epoch [4/4]: [9/20]  45%|#033[32m████▌     #033[0m [01:13<01:21]#033[A\n",
      "Epoch [4/4]: [10/20]  50%|#033[32m█████     #033[0m [01:13<01:30]#033[A\n",
      "Epoch [4/4]: [10/20]  50%|#033[32m█████     #033[0m [01:24<01:30]#033[A\n",
      "Epoch [4/4]: [11/20]  55%|#033[32m█████▌    #033[0m [01:24<01:26]#033[A\n",
      "Epoch [4/4]: [11/20]  55%|#033[32m█████▌    #033[0m [01:35<01:26]#033[A\n",
      "Epoch [4/4]: [12/20]  60%|#033[32m██████    #033[0m [01:35<01:19]#033[A\n",
      "Epoch [4/4]: [12/20]  60%|#033[32m██████    #033[0m [01:44<01:19]#033[A\n",
      "Epoch [4/4]: [13/20]  65%|#033[32m██████▌   #033[0m [01:44<01:08]#033[A\n",
      "Epoch [4/4]: [13/20]  65%|#033[32m██████▌   #033[0m [01:51<01:08]#033[A\n",
      "Epoch [4/4]: [14/20]  70%|#033[32m███████   #033[0m [01:51<00:53]#033[A\n",
      "Epoch [4/4]: [14/20]  70%|#033[32m███████   #033[0m [01:56<00:53]#033[A\n",
      "Epoch [4/4]: [15/20]  75%|#033[32m███████▌  #033[0m [01:56<00:38]#033[A\n",
      "Epoch [4/4]: [15/20]  75%|#033[32m███████▌  #033[0m [02:01<00:38]#033[A\n",
      "Epoch [4/4]: [16/20]  80%|#033[32m████████  #033[0m [02:01<00:28]#033[A\n",
      "Epoch [4/4]: [16/20]  80%|#033[32m████████  #033[0m [02:09<00:28]#033[A\n",
      "Epoch [4/4]: [17/20]  85%|#033[32m████████▌ #033[0m [02:09<00:21]#033[A\n",
      "Epoch [4/4]: [17/20]  85%|#033[32m████████▌ #033[0m [02:16<00:21]#033[A\n",
      "Epoch [4/4]: [18/20]  90%|#033[32m█████████ #033[0m [02:16<00:14]#033[A\n",
      "Epoch [4/4]: [18/20]  90%|#033[32m█████████ #033[0m [02:24<00:14]#033[A\n",
      "Epoch [4/4]: [19/20]  95%|#033[32m█████████▌#033[0m [02:24<00:07]#033[A\n",
      "Epoch [4/4]: [19/20]  95%|#033[32m█████████▌#033[0m [02:33<00:07]#033[A\n",
      "Epoch [4/4]: [20/20] 100%|#033[32m██████████#033[0m [02:33<00:00]#033[A\n",
      "#033[A\n",
      "2025-07-15 08:39:13,586 - INFO - Epoch[4] Complete. Time taken: 00:02:46.148\n",
      "2025-07-15 08:39:13,586 - INFO - Engine run finished. Time taken: 00:02:46.249\n",
      "2025-07-15 08:39:13,678 - INFO - Epoch[4] Complete. Time taken: 00:09:01.910\n",
      "2025-07-15 08:39:28,412 - INFO - Epoch: 5/5, Iter: 1/119 -- train_loss: 1.0180\n",
      "[1/119]   1%|           [00:00<?]\n",
      "Epoch [5/5]: [1/119]   1%|           [00:00<?]\n",
      "Epoch [5/5]: [1/119]   1%|          , loss=1.02 [00:00<?]\n",
      "2025-07-15 08:39:28,415 - INFO - Current learning rate: 0.00018239683620450308\n",
      "2025-07-15 08:39:29,854 - INFO - Epoch: 5/5, Iter: 2/119 -- train_loss: 1.0186\n",
      "Epoch [5/5]: [1/119]   1%|          , loss=1.02 [00:01<?]\n",
      "Epoch [5/5]: [1/119]   1%|          , loss=1.02 [00:01<?]\n",
      "Epoch [5/5]: [2/119]   2%|▏         , loss=1.02 [00:01<02:48]\n",
      "2025-07-15 08:39:29,856 - INFO - Current learning rate: 0.00017949310298244839\n",
      "2025-07-15 08:39:31,064 - INFO - Epoch: 5/5, Iter: 3/119 -- train_loss: 1.0154\n",
      "Epoch [5/5]: [2/119]   2%|▏         , loss=1.02 [00:02<02:48]\n",
      "Epoch [5/5]: [2/119]   2%|▏         , loss=1.02 [00:02<02:48]\n",
      "Epoch [5/5]: [3/119]   3%|▎         , loss=1.02 [00:02<02:31]\n",
      "2025-07-15 08:39:31,065 - INFO - Current learning rate: 0.00017660760484423425\n",
      "2025-07-15 08:39:40,279 - INFO - Epoch: 5/5, Iter: 4/119 -- train_loss: 1.0179\n",
      "Epoch [5/5]: [3/119]   3%|▎         , loss=1.02 [00:11<02:31]\n",
      "Epoch [5/5]: [3/119]   3%|▎         , loss=1.02 [00:11<02:31]\n",
      "Epoch [5/5]: [4/119]   3%|▎         , loss=1.02 [00:11<09:25]\n",
      "2025-07-15 08:39:40,282 - INFO - Current learning rate: 0.00017374050595784816\n",
      "2025-07-15 08:39:41,820 - INFO - Epoch: 5/5, Iter: 5/119 -- train_loss: 1.0113\n",
      "Epoch [5/5]: [4/119]   3%|▎         , loss=1.02 [00:13<09:25]\n",
      "Epoch [5/5]: [4/119]   3%|▎         , loss=1.01 [00:13<09:25]\n",
      "Epoch [5/5]: [5/119]   4%|▍         , loss=1.01 [00:13<06:48]\n",
      "2025-07-15 08:39:41,821 - INFO - Current learning rate: 0.00017089196944446698\n",
      "2025-07-15 08:39:43,587 - INFO - Epoch: 5/5, Iter: 6/119 -- train_loss: 1.0179\n",
      "Epoch [5/5]: [5/119]   4%|▍         , loss=1.01 [00:15<06:48]\n",
      "Epoch [5/5]: [5/119]   4%|▍         , loss=1.02 [00:15<06:48]\n",
      "Epoch [5/5]: [6/119]   5%|▌         , loss=1.02 [00:15<05:30]\n",
      "2025-07-15 08:39:43,588 - INFO - Current learning rate: 0.0001680621573691775\n",
      "2025-07-15 08:39:44,818 - INFO - Epoch: 5/5, Iter: 7/119 -- train_loss: 0.9977\n",
      "Epoch [5/5]: [6/119]   5%|▌         , loss=1.02 [00:16<05:30]\n",
      "Epoch [5/5]: [6/119]   5%|▌         , loss=0.998 [00:16<05:30]\n",
      "Epoch [5/5]: [7/119]   6%|▌         , loss=0.998 [00:16<04:23]\n",
      "2025-07-15 08:39:44,820 - INFO - Current learning rate: 0.00016525123073175493\n",
      "2025-07-15 08:39:48,962 - INFO - Epoch: 5/5, Iter: 8/119 -- train_loss: 1.0142\n",
      "Epoch [5/5]: [7/119]   6%|▌         , loss=0.998 [00:20<04:23]\n",
      "Epoch [5/5]: [7/119]   6%|▌         , loss=1.01 [00:20<04:23]\n",
      "Epoch [5/5]: [8/119]   7%|▋         , loss=1.01 [00:20<05:26]\n",
      "2025-07-15 08:39:48,963 - INFO - Current learning rate: 0.00016245934945750378\n",
      "2025-07-15 08:39:50,148 - INFO - Epoch: 5/5, Iter: 9/119 -- train_loss: 1.0187\n",
      "Epoch [5/5]: [8/119]   7%|▋         , loss=1.01 [00:21<05:26]\n",
      "Epoch [5/5]: [8/119]   7%|▋         , loss=1.02 [00:21<05:26]\n",
      "Epoch [5/5]: [9/119]   8%|▊         , loss=1.02 [00:21<04:21]\n",
      "2025-07-15 08:39:50,149 - INFO - Current learning rate: 0.0001596866723881587\n",
      "2025-07-15 08:39:53,626 - INFO - Epoch: 5/5, Iter: 10/119 -- train_loss: 0.9960\n",
      "Epoch [5/5]: [9/119]   8%|▊         , loss=1.02 [00:25<04:21]\n",
      "Epoch [5/5]: [9/119]   8%|▊         , loss=0.996 [00:25<04:21]\n",
      "Epoch [5/5]: [10/119]   8%|▊         , loss=0.996 [00:25<04:56]\n",
      "2025-07-15 08:39:53,628 - INFO - Current learning rate: 0.00015693335727284658\n",
      "2025-07-15 08:39:56,779 - INFO - Epoch: 5/5, Iter: 11/119 -- train_loss: 1.0177\n",
      "Epoch [5/5]: [10/119]   8%|▊         , loss=0.996 [00:28<04:56]\n",
      "Epoch [5/5]: [10/119]   8%|▊         , loss=1.02 [00:28<04:56]\n",
      "Epoch [5/5]: [11/119]   9%|▉         , loss=1.02 [00:28<05:08]\n",
      "2025-07-15 08:39:56,781 - INFO - Current learning rate: 0.00015419956075911302\n",
      "2025-07-15 08:39:58,188 - INFO - Epoch: 5/5, Iter: 12/119 -- train_loss: 1.0182\n",
      "Epoch [5/5]: [11/119]   9%|▉         , loss=1.02 [00:29<05:08]\n",
      "Epoch [5/5]: [11/119]   9%|▉         , loss=1.02 [00:29<05:08]\n",
      "Epoch [5/5]: [12/119]  10%|█         , loss=1.02 [00:29<04:18]\n",
      "2025-07-15 08:39:58,189 - INFO - Current learning rate: 0.0001514854383840091\n",
      "2025-07-15 08:39:59,302 - INFO - Epoch: 5/5, Iter: 13/119 -- train_loss: 1.0178\n",
      "Epoch [5/5]: [12/119]  10%|█         , loss=1.02 [00:30<04:18]\n",
      "Epoch [5/5]: [12/119]  10%|█         , loss=1.02 [00:30<04:18]\n",
      "Epoch [5/5]: [13/119]  11%|█         , loss=1.02 [00:30<03:33]\n",
      "2025-07-15 08:39:59,303 - INFO - Current learning rate: 0.0001487911445652419\n",
      "2025-07-15 08:40:06,011 - INFO - Epoch: 5/5, Iter: 14/119 -- train_loss: 1.0175\n",
      "Epoch [5/5]: [13/119]  11%|█         , loss=1.02 [00:37<03:33]\n",
      "Epoch [5/5]: [13/119]  11%|█         , loss=1.02 [00:37<03:33]\n",
      "Epoch [5/5]: [14/119]  12%|█▏        , loss=1.02 [00:37<06:01]\n",
      "2025-07-15 08:40:06,013 - INFO - Current learning rate: 0.0001461168325923899\n",
      "2025-07-15 08:40:07,766 - INFO - Epoch: 5/5, Iter: 15/119 -- train_loss: 1.0196\n",
      "Epoch [5/5]: [14/119]  12%|█▏        , loss=1.02 [00:39<06:01]\n",
      "Epoch [5/5]: [14/119]  12%|█▏        , loss=1.02 [00:39<06:01]\n",
      "Epoch [5/5]: [15/119]  13%|█▎        , loss=1.02 [00:39<05:04]\n",
      "2025-07-15 08:40:07,768 - INFO - Current learning rate: 0.0001434626546181809\n",
      "2025-07-15 08:40:12,466 - INFO - Epoch: 5/5, Iter: 16/119 -- train_loss: 1.0180\n",
      "Epoch [5/5]: [15/119]  13%|█▎        , loss=1.02 [00:44<05:04]\n",
      "Epoch [5/5]: [15/119]  13%|█▎        , loss=1.02 [00:44<05:04]\n",
      "Epoch [5/5]: [16/119]  13%|█▎        , loss=1.02 [00:44<05:56]\n",
      "2025-07-15 08:40:12,468 - INFO - Current learning rate: 0.00014082876164983645\n",
      "2025-07-15 08:40:13,896 - INFO - Epoch: 5/5, Iter: 17/119 -- train_loss: 1.0058\n",
      "Epoch [5/5]: [16/119]  13%|█▎        , loss=1.02 [00:45<05:56]\n",
      "Epoch [5/5]: [16/119]  13%|█▎        , loss=1.01 [00:45<05:56]\n",
      "Epoch [5/5]: [17/119]  14%|█▍        , loss=1.01 [00:45<04:50]\n",
      "2025-07-15 08:40:13,898 - INFO - Current learning rate: 0.00013821530354047896\n",
      "2025-07-15 08:40:15,168 - INFO - Epoch: 5/5, Iter: 18/119 -- train_loss: 1.0175\n",
      "Epoch [5/5]: [17/119]  14%|█▍        , loss=1.01 [00:46<04:50]\n",
      "Epoch [5/5]: [17/119]  14%|█▍        , loss=1.02 [00:46<04:50]\n",
      "Epoch [5/5]: [18/119]  15%|█▌        , loss=1.02 [00:46<04:00]\n",
      "2025-07-15 08:40:15,169 - INFO - Current learning rate: 0.00013562242898060745\n",
      "2025-07-15 08:40:17,975 - INFO - Epoch: 5/5, Iter: 19/119 -- train_loss: 0.9848\n",
      "Epoch [5/5]: [18/119]  15%|█▌        , loss=1.02 [00:49<04:00]\n",
      "Epoch [5/5]: [18/119]  15%|█▌        , loss=0.985 [00:49<04:00]\n",
      "Epoch [5/5]: [19/119]  16%|█▌        , loss=0.985 [00:49<04:10]\n",
      "2025-07-15 08:40:17,977 - INFO - Current learning rate: 0.00013305028548963658\n",
      "2025-07-15 08:40:21,806 - INFO - Epoch: 5/5, Iter: 20/119 -- train_loss: 0.9688\n",
      "Epoch [5/5]: [19/119]  16%|█▌        , loss=0.985 [00:53<04:10]\n",
      "Epoch [5/5]: [19/119]  16%|█▌        , loss=0.969 [00:53<04:10]\n",
      "Epoch [5/5]: [20/119]  17%|█▋        , loss=0.969 [00:53<04:47]\n",
      "2025-07-15 08:40:21,808 - INFO - Current learning rate: 0.00013049901940750497\n",
      "2025-07-15 08:40:23,107 - INFO - Epoch: 5/5, Iter: 21/119 -- train_loss: 1.0197\n",
      "Epoch [5/5]: [20/119]  17%|█▋        , loss=0.969 [00:54<04:47]\n",
      "Epoch [5/5]: [20/119]  17%|█▋        , loss=1.02 [00:54<04:47]\n",
      "Epoch [5/5]: [21/119]  18%|█▊        , loss=1.02 [00:54<03:57]\n",
      "2025-07-15 08:40:23,109 - INFO - Current learning rate: 0.00012796877588634801\n",
      "2025-07-15 08:40:24,428 - INFO - Epoch: 5/5, Iter: 22/119 -- train_loss: 1.0176\n",
      "Epoch [5/5]: [21/119]  18%|█▊        , loss=1.02 [00:56<03:57]\n",
      "Epoch [5/5]: [21/119]  18%|█▊        , loss=1.02 [00:56<03:57]\n",
      "Epoch [5/5]: [22/119]  18%|█▊        , loss=1.02 [00:56<03:22]\n",
      "2025-07-15 08:40:24,430 - INFO - Current learning rate: 0.00012545969888224073\n",
      "2025-07-15 08:40:26,856 - INFO - Epoch: 5/5, Iter: 23/119 -- train_loss: 1.0182\n",
      "Epoch [5/5]: [22/119]  18%|█▊        , loss=1.02 [00:58<03:22]\n",
      "Epoch [5/5]: [22/119]  18%|█▊        , loss=1.02 [00:58<03:22]\n",
      "Epoch [5/5]: [23/119]  19%|█▉        , loss=1.02 [00:58<03:30]\n",
      "2025-07-15 08:40:26,857 - INFO - Current learning rate: 0.00012297193114700652\n",
      "2025-07-15 08:40:33,775 - INFO - Epoch: 5/5, Iter: 24/119 -- train_loss: 1.0182\n",
      "Epoch [5/5]: [23/119]  19%|█▉        , loss=1.02 [01:05<03:30]\n",
      "Epoch [5/5]: [23/119]  19%|█▉        , loss=1.02 [01:05<03:30]\n",
      "Epoch [5/5]: [24/119]  20%|██        , loss=1.02 [01:05<05:43]\n",
      "2025-07-15 08:40:33,777 - INFO - Current learning rate: 0.00012050561422009632\n",
      "2025-07-15 08:40:35,315 - INFO - Epoch: 5/5, Iter: 25/119 -- train_loss: 1.0122\n",
      "Epoch [5/5]: [24/119]  20%|██        , loss=1.02 [01:06<05:43]\n",
      "Epoch [5/5]: [24/119]  20%|██        , loss=1.01 [01:06<05:43]\n",
      "Epoch [5/5]: [25/119]  21%|██        , loss=1.01 [01:06<04:41]\n",
      "2025-07-15 08:40:35,317 - INFO - Current learning rate: 0.00011806088842053486\n",
      "2025-07-15 08:40:36,784 - INFO - Epoch: 5/5, Iter: 26/119 -- train_loss: 1.0107\n",
      "Epoch [5/5]: [25/119]  21%|██        , loss=1.01 [01:08<04:41]\n",
      "Epoch [5/5]: [25/119]  21%|██        , loss=1.01 [01:08<04:41]\n",
      "Epoch [5/5]: [26/119]  22%|██▏       , loss=1.01 [01:08<03:55]\n",
      "2025-07-15 08:40:36,787 - INFO - Current learning rate: 0.00011563789283893834\n",
      "2025-07-15 08:40:38,283 - INFO - Epoch: 5/5, Iter: 27/119 -- train_loss: 0.9970\n",
      "Epoch [5/5]: [26/119]  22%|██▏       , loss=1.01 [01:09<03:55]\n",
      "Epoch [5/5]: [26/119]  22%|██▏       , loss=0.997 [01:09<03:55]\n",
      "Epoch [5/5]: [27/119]  23%|██▎       , loss=0.997 [01:09<03:24]\n",
      "2025-07-15 08:40:38,285 - INFO - Current learning rate: 0.00011323676532960039\n",
      "2025-07-15 08:40:43,474 - INFO - Epoch: 5/5, Iter: 28/119 -- train_loss: 1.0155\n",
      "Epoch [5/5]: [27/119]  23%|██▎       , loss=0.997 [01:15<03:24]\n",
      "Epoch [5/5]: [27/119]  23%|██▎       , loss=1.02 [01:15<03:24]\n",
      "Epoch [5/5]: [28/119]  24%|██▎       , loss=1.02 [01:15<04:43]\n",
      "2025-07-15 08:40:43,475 - INFO - Current learning rate: 0.0001108576425026488\n",
      "2025-07-15 08:40:44,611 - INFO - Epoch: 5/5, Iter: 29/119 -- train_loss: 1.0177\n",
      "Epoch [5/5]: [28/119]  24%|██▎       , loss=1.02 [01:16<04:43]\n",
      "Epoch [5/5]: [28/119]  24%|██▎       , loss=1.02 [01:16<04:43]\n",
      "Epoch [5/5]: [29/119]  24%|██▍       , loss=1.02 [01:16<03:46]\n",
      "2025-07-15 08:40:44,612 - INFO - Current learning rate: 0.00010850065971627389\n",
      "2025-07-15 08:40:48,105 - INFO - Epoch: 5/5, Iter: 30/119 -- train_loss: 1.0188\n",
      "Epoch [5/5]: [29/119]  24%|██▍       , loss=1.02 [01:19<03:46]\n",
      "Epoch [5/5]: [29/119]  24%|██▍       , loss=1.02 [01:19<03:46]\n",
      "Epoch [5/5]: [30/119]  25%|██▌       , loss=1.02 [01:19<04:10]\n",
      "2025-07-15 08:40:48,106 - INFO - Current learning rate: 0.0001061659510690266\n",
      "2025-07-15 08:40:50,907 - INFO - Epoch: 5/5, Iter: 31/119 -- train_loss: 1.0171\n",
      "Epoch [5/5]: [30/119]  25%|██▌       , loss=1.02 [01:22<04:10]\n",
      "Epoch [5/5]: [30/119]  25%|██▌       , loss=1.02 [01:22<04:10]\n",
      "Epoch [5/5]: [31/119]  26%|██▌       , loss=1.02 [01:22<04:07]\n",
      "2025-07-15 08:40:50,909 - INFO - Current learning rate: 0.0001038536493921899\n",
      "2025-07-15 08:40:52,199 - INFO - Epoch: 5/5, Iter: 32/119 -- train_loss: 1.0182\n",
      "Epoch [5/5]: [31/119]  26%|██▌       , loss=1.02 [01:23<04:07]\n",
      "Epoch [5/5]: [31/119]  26%|██▌       , loss=1.02 [01:23<04:07]\n",
      "Epoch [5/5]: [32/119]  27%|██▋       , loss=1.02 [01:23<03:24]\n",
      "2025-07-15 08:40:52,200 - INFO - Current learning rate: 0.0001015638862422206\n",
      "2025-07-15 08:40:53,539 - INFO - Epoch: 5/5, Iter: 33/119 -- train_loss: 1.0118\n",
      "Epoch [5/5]: [32/119]  27%|██▋       , loss=1.02 [01:25<03:24]\n",
      "Epoch [5/5]: [32/119]  27%|██▋       , loss=1.01 [01:25<03:24]\n",
      "Epoch [5/5]: [33/119]  28%|██▊       , loss=1.01 [01:25<02:56]\n",
      "2025-07-15 08:40:53,540 - INFO - Current learning rate: 9.929679189326547e-05\n",
      "2025-07-15 08:40:59,457 - INFO - Epoch: 5/5, Iter: 34/119 -- train_loss: 1.0041\n",
      "Epoch [5/5]: [33/119]  28%|██▊       , loss=1.01 [01:31<02:56]\n",
      "Epoch [5/5]: [33/119]  28%|██▊       , loss=1 [01:31<02:56]\n",
      "Epoch [5/5]: [34/119]  29%|██▊       , loss=1 [01:31<04:32]\n",
      "2025-07-15 08:40:59,458 - INFO - Current learning rate: 9.705249532974846e-05\n",
      "2025-07-15 08:41:03,458 - INFO - Epoch: 5/5, Iter: 35/119 -- train_loss: 1.0149\n",
      "Epoch [5/5]: [34/119]  29%|██▊       , loss=1 [01:35<04:32]\n",
      "Epoch [5/5]: [34/119]  29%|██▊       , loss=1.01 [01:35<04:32]\n",
      "Epoch [5/5]: [35/119]  29%|██▉       , loss=1.01 [01:35<04:49]\n",
      "2025-07-15 08:41:03,459 - INFO - Current learning rate: 9.483112423903319e-05\n",
      "2025-07-15 08:41:04,694 - INFO - Epoch: 5/5, Iter: 36/119 -- train_loss: 1.0177\n",
      "Epoch [5/5]: [35/119]  29%|██▉       , loss=1.01 [01:36<04:49]\n",
      "Epoch [5/5]: [35/119]  29%|██▉       , loss=1.02 [01:36<04:49]\n",
      "Epoch [5/5]: [36/119]  30%|███       , loss=1.02 [01:36<03:51]\n",
      "2025-07-15 08:41:04,695 - INFO - Current learning rate: 9.263280500415739e-05\n",
      "2025-07-15 08:41:05,905 - INFO - Epoch: 5/5, Iter: 37/119 -- train_loss: 1.0189\n",
      "Epoch [5/5]: [36/119]  30%|███       , loss=1.02 [01:37<03:51]\n",
      "Epoch [5/5]: [36/119]  30%|███       , loss=1.02 [01:37<03:51]\n",
      "Epoch [5/5]: [37/119]  31%|███       , loss=1.02 [01:37<03:09]\n",
      "2025-07-15 08:41:05,906 - INFO - Current learning rate: 9.045766269664314e-05\n",
      "2025-07-15 08:41:09,086 - INFO - Epoch: 5/5, Iter: 38/119 -- train_loss: 1.0145\n",
      "Epoch [5/5]: [37/119]  31%|███       , loss=1.02 [01:40<03:09]\n",
      "Epoch [5/5]: [37/119]  31%|███       , loss=1.01 [01:40<03:09]\n",
      "Epoch [5/5]: [38/119]  32%|███▏      , loss=1.01 [01:40<03:28]\n",
      "2025-07-15 08:41:09,088 - INFO - Current learning rate: 8.830582106938051e-05\n",
      "2025-07-15 08:41:11,149 - INFO - Epoch: 5/5, Iter: 39/119 -- train_loss: 1.0039\n",
      "Epoch [5/5]: [38/119]  32%|███▏      , loss=1.01 [01:42<03:28]\n",
      "Epoch [5/5]: [38/119]  32%|███▏      , loss=1 [01:42<03:28]\n",
      "Epoch [5/5]: [39/119]  33%|███▎      , loss=1 [01:42<03:13]\n",
      "2025-07-15 08:41:11,152 - INFO - Current learning rate: 8.617740254958729e-05\n",
      "2025-07-15 08:41:12,512 - INFO - Epoch: 5/5, Iter: 40/119 -- train_loss: 1.0165\n",
      "Epoch [5/5]: [39/119]  33%|███▎      , loss=1 [01:44<03:13]\n",
      "Epoch [5/5]: [39/119]  33%|███▎      , loss=1.02 [01:44<03:13]\n",
      "Epoch [5/5]: [40/119]  34%|███▎      , loss=1.02 [01:44<02:46]\n",
      "2025-07-15 08:41:12,513 - INFO - Current learning rate: 8.407252823184297e-05\n",
      "2025-07-15 08:41:13,923 - INFO - Epoch: 5/5, Iter: 41/119 -- train_loss: 0.9972\n",
      "Epoch [5/5]: [40/119]  34%|███▎      , loss=1.02 [01:45<02:46]\n",
      "Epoch [5/5]: [40/119]  34%|███▎      , loss=0.997 [01:45<02:46]\n",
      "Epoch [5/5]: [41/119]  34%|███▍      , loss=0.997 [01:45<02:27]\n",
      "2025-07-15 08:41:13,925 - INFO - Current learning rate: 8.199131787119973e-05\n",
      "2025-07-15 08:41:16,542 - INFO - Epoch: 5/5, Iter: 42/119 -- train_loss: 1.0174\n",
      "Epoch [5/5]: [41/119]  34%|███▍      , loss=0.997 [01:48<02:27]\n",
      "Epoch [5/5]: [41/119]  34%|███▍      , loss=1.02 [01:48<02:27]\n",
      "Epoch [5/5]: [42/119]  35%|███▌      , loss=1.02 [01:48<02:42]\n",
      "2025-07-15 08:41:16,544 - INFO - Current learning rate: 7.993388987636881e-05\n",
      "2025-07-15 08:41:21,317 - INFO - Epoch: 5/5, Iter: 43/119 -- train_loss: 1.0108\n",
      "Epoch [5/5]: [42/119]  35%|███▌      , loss=1.02 [01:52<02:42]\n",
      "Epoch [5/5]: [42/119]  35%|███▌      , loss=1.01 [01:52<02:42]\n",
      "Epoch [5/5]: [43/119]  36%|███▌      , loss=1.01 [01:52<03:41]\n",
      "2025-07-15 08:41:21,319 - INFO - Current learning rate: 7.790036130298354e-05\n",
      "2025-07-15 08:41:23,537 - INFO - Epoch: 5/5, Iter: 44/119 -- train_loss: 1.0182\n",
      "Epoch [5/5]: [43/119]  36%|███▌      , loss=1.01 [01:55<03:41]\n",
      "Epoch [5/5]: [43/119]  36%|███▌      , loss=1.02 [01:55<03:41]\n",
      "Epoch [5/5]: [44/119]  37%|███▋      , loss=1.02 [01:55<03:22]\n",
      "2025-07-15 08:41:23,538 - INFO - Current learning rate: 7.589084784694004e-05\n",
      "2025-07-15 08:41:24,849 - INFO - Epoch: 5/5, Iter: 45/119 -- train_loss: 1.0134\n",
      "Epoch [5/5]: [44/119]  37%|███▋      , loss=1.02 [01:56<03:22]\n",
      "Epoch [5/5]: [44/119]  37%|███▋      , loss=1.01 [01:56<03:22]\n",
      "Epoch [5/5]: [45/119]  38%|███▊      , loss=1.01 [01:56<02:49]\n",
      "2025-07-15 08:41:24,851 - INFO - Current learning rate: 7.39054638378146e-05\n",
      "2025-07-15 08:41:27,783 - INFO - Epoch: 5/5, Iter: 46/119 -- train_loss: 1.0169\n",
      "Epoch [5/5]: [45/119]  38%|███▊      , loss=1.01 [01:59<02:49]\n",
      "Epoch [5/5]: [45/119]  38%|███▊      , loss=1.02 [01:59<02:49]\n",
      "Epoch [5/5]: [46/119]  39%|███▊      , loss=1.02 [01:59<03:01]\n",
      "2025-07-15 08:41:27,785 - INFO - Current learning rate: 7.194432223235858e-05\n",
      "2025-07-15 08:41:34,986 - INFO - Epoch: 5/5, Iter: 47/119 -- train_loss: 1.0132\n",
      "Epoch [5/5]: [46/119]  39%|███▊      , loss=1.02 [02:06<03:01]\n",
      "Epoch [5/5]: [46/119]  39%|███▊      , loss=1.01 [02:06<03:01]\n",
      "Epoch [5/5]: [47/119]  39%|███▉      , loss=1.01 [02:06<04:40]\n",
      "2025-07-15 08:41:34,987 - INFO - Current learning rate: 7.000753460807261e-05\n",
      "2025-07-15 08:41:36,167 - INFO - Epoch: 5/5, Iter: 48/119 -- train_loss: 1.0174\n",
      "Epoch [5/5]: [47/119]  39%|███▉      , loss=1.01 [02:07<04:40]\n",
      "Epoch [5/5]: [47/119]  39%|███▉      , loss=1.02 [02:07<04:40]\n",
      "Epoch [5/5]: [48/119]  40%|████      , loss=1.02 [02:07<03:38]\n",
      "2025-07-15 08:41:36,170 - INFO - Current learning rate: 6.809521115685772e-05\n",
      "2025-07-15 08:41:37,518 - INFO - Epoch: 5/5, Iter: 49/119 -- train_loss: 1.0175\n",
      "Epoch [5/5]: [48/119]  40%|████      , loss=1.02 [02:09<03:38]\n",
      "Epoch [5/5]: [48/119]  40%|████      , loss=1.02 [02:09<03:38]\n",
      "Epoch [5/5]: [49/119]  41%|████      , loss=1.02 [02:09<02:59]\n",
      "2025-07-15 08:41:37,520 - INFO - Current learning rate: 6.620746067874675e-05\n",
      "2025-07-15 08:41:38,949 - INFO - Epoch: 5/5, Iter: 50/119 -- train_loss: 1.0101\n",
      "Epoch [5/5]: [49/119]  41%|████      , loss=1.02 [02:10<02:59]\n",
      "Epoch [5/5]: [49/119]  41%|████      , loss=1.01 [02:10<02:59]\n",
      "Epoch [5/5]: [50/119]  42%|████▏     , loss=1.01 [02:10<02:33]\n",
      "2025-07-15 08:41:38,952 - INFO - Current learning rate: 6.434439057571353e-05\n",
      "2025-07-15 08:41:47,033 - INFO - Epoch: 5/5, Iter: 51/119 -- train_loss: 1.0087\n",
      "Epoch [5/5]: [50/119]  42%|████▏     , loss=1.01 [02:18<02:33]\n",
      "Epoch [5/5]: [50/119]  42%|████▏     , loss=1.01 [02:18<02:33]\n",
      "Epoch [5/5]: [51/119]  43%|████▎     , loss=1.01 [02:18<04:30]2025-07-15 08:41:47,034 - INFO - Current learning rate: 6.250610684556319e-05\n",
      "2025-07-15 08:41:48,541 - INFO - Epoch: 5/5, Iter: 52/119 -- train_loss: 1.0173\n",
      "Epoch [5/5]: [51/119]  43%|████▎     , loss=1.01 [02:20<04:30]\n",
      "Epoch [5/5]: [51/119]  43%|████▎     , loss=1.02 [02:20<04:30]\n",
      "Epoch [5/5]: [52/119]  44%|████▎     , loss=1.02 [02:20<03:37]\n",
      "2025-07-15 08:41:48,542 - INFO - Current learning rate: 6.069271407590054e-05\n",
      "2025-07-15 08:41:49,752 - INFO - Epoch: 5/5, Iter: 53/119 -- train_loss: 1.0170\n",
      "Epoch [5/5]: [52/119]  44%|████▎     , loss=1.02 [02:21<03:37]\n",
      "Epoch [5/5]: [52/119]  44%|████▎     , loss=1.02 [02:21<03:37]\n",
      "Epoch [5/5]: [53/119]  45%|████▍     , loss=1.02 [02:21<02:53]\n",
      "2025-07-15 08:41:49,754 - INFO - Current learning rate: 5.890431543818061e-05\n",
      "2025-07-15 08:41:51,058 - INFO - Epoch: 5/5, Iter: 54/119 -- train_loss: 1.0106\n",
      "Epoch [5/5]: [53/119]  45%|████▍     , loss=1.02 [02:22<02:53]\n",
      "Epoch [5/5]: [53/119]  45%|████▍     , loss=1.01 [02:22<02:53]\n",
      "Epoch [5/5]: [54/119]  45%|████▌     , loss=1.01 [02:22<02:25]\n",
      "2025-07-15 08:41:51,060 - INFO - Current learning rate: 5.7141012681838016e-05\n",
      "2025-07-15 08:42:04,813 - INFO - Epoch: 5/5, Iter: 55/119 -- train_loss: 0.9633\n",
      "Epoch [5/5]: [54/119]  45%|████▌     , loss=1.01 [02:36<02:25]\n",
      "Epoch [5/5]: [54/119]  45%|████▌     , loss=0.963 [02:36<02:25]\n",
      "Epoch [5/5]: [55/119]  46%|████▌     , loss=0.963 [02:36<06:04]\n",
      "2025-07-15 08:42:04,814 - INFO - Current learning rate: 5.540290612849853e-05\n",
      "2025-07-15 08:42:06,075 - INFO - Epoch: 5/5, Iter: 56/119 -- train_loss: 1.0157\n",
      "Epoch [5/5]: [55/119]  46%|████▌     , loss=0.963 [02:37<06:04]\n",
      "Epoch [5/5]: [55/119]  46%|████▌     , loss=1.02 [02:37<06:04] #015Epoch [5/5]: [56/119]  47%|████▋     , loss=1.02 [02:37<04:34]\n",
      "2025-07-15 08:42:06,077 - INFO - Current learning rate: 5.3690094666271054e-05\n",
      "2025-07-15 08:42:07,217 - INFO - Epoch: 5/5, Iter: 57/119 -- train_loss: 0.9917\n",
      "Epoch [5/5]: [56/119]  47%|████▋     , loss=1.02 [02:38<04:34]\n",
      "Epoch [5/5]: [56/119]  47%|████▋     , loss=0.992 [02:38<04:34]\n",
      "Epoch [5/5]: [57/119]  48%|████▊     , loss=0.992 [02:38<03:30]\n",
      "2025-07-15 08:42:07,218 - INFO - Current learning rate: 5.200267574412165e-05\n",
      "2025-07-15 08:42:08,748 - INFO - Epoch: 5/5, Iter: 58/119 -- train_loss: 1.0022\n",
      "Epoch [5/5]: [57/119]  48%|████▊     , loss=0.992 [02:40<03:30]\n",
      "Epoch [5/5]: [57/119]  48%|████▊     , loss=1 [02:40<03:30]\n",
      "Epoch [5/5]: [58/119]  49%|████▊     , loss=1 [02:40<02:53]\n",
      "2025-07-15 08:42:08,750 - INFO - Current learning rate: 5.034074536632907e-05\n",
      "2025-07-15 08:42:14,357 - INFO - Epoch: 5/5, Iter: 59/119 -- train_loss: 1.0152\n",
      "Epoch [5/5]: [58/119]  49%|████▊     , loss=1 [02:45<02:53]\n",
      "Epoch [5/5]: [58/119]  49%|████▊     , loss=1.02 [02:45<02:53]\n",
      "Epoch [5/5]: [59/119]  50%|████▉     , loss=1.02 [02:45<03:40]\n",
      "2025-07-15 08:42:14,358 - INFO - Current learning rate: 4.870439808702304e-05\n",
      "2025-07-15 08:42:15,677 - INFO - Epoch: 5/5, Iter: 60/119 -- train_loss: 1.0172\n",
      "Epoch [5/5]: [59/119]  50%|████▉     , loss=1.02 [02:47<03:40]\n",
      "Epoch [5/5]: [59/119]  50%|████▉     , loss=1.02 [02:47<03:40]\n",
      "Epoch [5/5]: [60/119]  50%|█████     , loss=1.02 [02:47<02:54]\n",
      "2025-07-15 08:42:15,679 - INFO - Current learning rate: 4.709372700480409e-05\n",
      "2025-07-15 08:42:16,889 - INFO - Epoch: 5/5, Iter: 61/119 -- train_loss: 1.0057\n",
      "Epoch [5/5]: [60/119]  50%|█████     , loss=1.02 [02:48<02:54]\n",
      "Epoch [5/5]: [60/119]  50%|█████     , loss=1.01 [02:48<02:54]\n",
      "Epoch [5/5]: [61/119]  51%|█████▏    , loss=1.01 [02:48<02:21]\n",
      "2025-07-15 08:42:16,890 - INFO - Current learning rate: 4.5508823757447466e-05\n",
      "2025-07-15 08:42:18,051 - INFO - Epoch: 5/5, Iter: 62/119 -- train_loss: 1.0172\n",
      "Epoch [5/5]: [61/119]  51%|█████▏    , loss=1.01 [02:49<02:21]\n",
      "Epoch [5/5]: [61/119]  51%|█████▏    , loss=1.02 [02:49<02:21]\n",
      "Epoch [5/5]: [62/119]  52%|█████▏    , loss=1.02 [02:49<01:57]\n",
      "2025-07-15 08:42:18,052 - INFO - Current learning rate: 4.394977851668893e-05\n",
      "2025-07-15 08:42:24,680 - INFO - Epoch: 5/5, Iter: 63/119 -- train_loss: 1.0175\n",
      "Epoch [5/5]: [62/119]  52%|█████▏    , loss=1.02 [02:56<01:57]\n",
      "Epoch [5/5]: [62/119]  52%|█████▏    , loss=1.02 [02:56<01:57]\n",
      "Epoch [5/5]: [63/119]  53%|█████▎    , loss=1.02 [02:56<03:11]\n",
      "2025-07-15 08:42:24,682 - INFO - Current learning rate: 4.24166799830949e-05\n",
      "2025-07-15 08:42:26,104 - INFO - Epoch: 5/5, Iter: 64/119 -- train_loss: 0.9814\n",
      "Epoch [5/5]: [63/119]  53%|█████▎    , loss=1.02 [02:57<03:11]\n",
      "Epoch [5/5]: [63/119]  53%|█████▎    , loss=0.981 [02:57<03:11]\n",
      "Epoch [5/5]: [64/119]  54%|█████▍    , loss=0.981 [02:57<02:35]\n",
      "2025-07-15 08:42:26,106 - INFO - Current learning rate: 4.090961538101549e-05\n",
      "2025-07-15 08:42:27,610 - INFO - Epoch: 5/5, Iter: 65/119 -- train_loss: 1.0163\n",
      "Epoch [5/5]: [64/119]  54%|█████▍    , loss=0.981 [02:59<02:35]\n",
      "Epoch [5/5]: [64/119]  54%|█████▍    , loss=1.02 [02:59<02:35]\n",
      "Epoch [5/5]: [65/119]  55%|█████▍    , loss=1.02 [02:59<02:11]\n",
      "2025-07-15 08:42:27,611 - INFO - Current learning rate: 3.942867045362252e-05\n",
      "2025-07-15 08:42:29,031 - INFO - Epoch: 5/5, Iter: 66/119 -- train_loss: 1.0177\n",
      "Epoch [5/5]: [65/119]  55%|█████▍    , loss=1.02 [03:00<02:11]\n",
      "Epoch [5/5]: [65/119]  55%|█████▍    , loss=1.02 [03:00<02:11]\n",
      "Epoch [5/5]: [66/119]  55%|█████▌    , loss=1.02 [03:00<01:52]\n",
      "2025-07-15 08:42:29,033 - INFO - Current learning rate: 3.797392945803054e-05\n",
      "2025-07-15 08:42:38,895 - INFO - Epoch: 5/5, Iter: 67/119 -- train_loss: 1.0179\n",
      "Epoch [5/5]: [66/119]  55%|█████▌    , loss=1.02 [03:10<01:52]\n",
      "Epoch [5/5]: [66/119]  55%|█████▌    , loss=1.02 [03:10<01:52]\n",
      "Epoch [5/5]: [67/119]  56%|█████▋    , loss=1.02 [03:10<03:51]\n",
      "2025-07-15 08:42:38,896 - INFO - Current learning rate: 3.6545475160503876e-05\n",
      "2025-07-15 08:42:40,009 - INFO - Epoch: 5/5, Iter: 68/119 -- train_loss: 1.0133\n",
      "Epoch [5/5]: [67/119]  56%|█████▋    , loss=1.02 [03:11<03:51]\n",
      "Epoch [5/5]: [67/119]  56%|█████▋    , loss=1.01 [03:11<03:51]\n",
      "Epoch [5/5]: [68/119]  57%|█████▋    , loss=1.01 [03:11<02:55]\n",
      "2025-07-15 08:42:40,010 - INFO - Current learning rate: 3.5143388831746917e-05\n",
      "2025-07-15 08:42:41,226 - INFO - Epoch: 5/5, Iter: 69/119 -- train_loss: 1.0172\n",
      "Epoch [5/5]: [68/119]  57%|█████▋    , loss=1.01 [03:12<02:55]\n",
      "Epoch [5/5]: [68/119]  57%|█████▋    , loss=1.02 [03:12<02:55]\n",
      "Epoch [5/5]: [69/119]  58%|█████▊    , loss=1.02 [03:12<02:18]\n",
      "2025-07-15 08:42:41,227 - INFO - Current learning rate: 3.3767750242280956e-05\n",
      "2025-07-15 08:42:42,622 - INFO - Epoch: 5/5, Iter: 70/119 -- train_loss: 1.0172\n",
      "Epoch [5/5]: [69/119]  58%|█████▊    , loss=1.02 [03:14<02:18]\n",
      "Epoch [5/5]: [69/119]  58%|█████▊    , loss=1.02 [03:14<02:18]\n",
      "Epoch [5/5]: [70/119]  59%|█████▉    , loss=1.02 [03:14<01:55]\n",
      "2025-07-15 08:42:42,623 - INFO - Current learning rate: 3.2418637657905245e-05\n",
      "2025-07-15 08:42:48,420 - INFO - Epoch: 5/5, Iter: 71/119 -- train_loss: 1.0159\n",
      "Epoch [5/5]: [70/119]  59%|█████▉    , loss=1.02 [03:20<01:55]\n",
      "Epoch [5/5]: [70/119]  59%|█████▉    , loss=1.02 [03:20<01:55]\n",
      "Epoch [5/5]: [71/119]  60%|█████▉    , loss=1.02 [03:20<02:42]\n",
      "2025-07-15 08:42:48,422 - INFO - Current learning rate: 3.109612783524434e-05\n",
      "2025-07-15 08:42:49,579 - INFO - Epoch: 5/5, Iter: 72/119 -- train_loss: 0.9690\n",
      "Epoch [5/5]: [71/119]  60%|█████▉    , loss=1.02 [03:21<02:42]\n",
      "Epoch [5/5]: [71/119]  60%|█████▉    , loss=0.969 [03:21<02:42]\n",
      "Epoch [5/5]: [72/119]  61%|██████    , loss=0.969 [03:21<02:08]\n",
      "2025-07-15 08:42:49,580 - INFO - Current learning rate: 2.9800296017381288e-05\n",
      "2025-07-15 08:42:51,053 - INFO - Epoch: 5/5, Iter: 73/119 -- train_loss: 0.9636\n",
      "Epoch [5/5]: [72/119]  61%|██████    , loss=0.969 [03:22<02:08]\n",
      "Epoch [5/5]: [72/119]  61%|██████    , loss=0.964 [03:22<02:08]\n",
      "Epoch [5/5]: [73/119]  61%|██████▏   , loss=0.964 [03:22<01:48]\n",
      "2025-07-15 08:42:51,055 - INFO - Current learning rate: 2.8531215929576077e-05\n",
      "2025-07-15 08:42:52,267 - INFO - Epoch: 5/5, Iter: 74/119 -- train_loss: 1.0174\n",
      "Epoch [5/5]: [73/119]  61%|██████▏   , loss=0.964 [03:23<01:48]\n",
      "Epoch [5/5]: [73/119]  61%|██████▏   , loss=1.02 [03:23<01:48]\n",
      "Epoch [5/5]: [74/119]  62%|██████▏   , loss=1.02 [03:23<01:30]\n",
      "2025-07-15 08:42:52,269 - INFO - Current learning rate: 2.7288959775071935e-05\n",
      "2025-07-15 08:42:57,735 - INFO - Epoch: 5/5, Iter: 75/119 -- train_loss: 0.9894\n",
      "Epoch [5/5]: [74/119]  62%|██████▏   , loss=1.02 [03:29<01:30]\n",
      "Epoch [5/5]: [74/119]  62%|██████▏   , loss=0.989 [03:29<01:30]\n",
      "Epoch [5/5]: [75/119]  63%|██████▎   , loss=0.989 [03:29<02:14]\n",
      "2025-07-15 08:42:57,736 - INFO - Current learning rate: 2.6073598230986913e-05\n",
      "2025-07-15 08:42:59,283 - INFO - Epoch: 5/5, Iter: 76/119 -- train_loss: 1.0015\n",
      "Epoch [5/5]: [75/119]  63%|██████▎   , loss=0.989 [03:30<02:14]\n",
      "Epoch [5/5]: [75/119]  63%|██████▎   , loss=1 [03:30<02:14]\n",
      "Epoch [5/5]: [76/119]  64%|██████▍   , loss=1 [03:30<01:51]\n",
      "2025-07-15 08:42:59,285 - INFO - Current learning rate: 2.488520044429263e-05\n",
      "2025-07-15 08:43:00,644 - INFO - Epoch: 5/5, Iter: 77/119 -- train_loss: 1.0143\n",
      "Epoch [5/5]: [76/119]  64%|██████▍   , loss=1 [03:32<01:51]\n",
      "Epoch [5/5]: [76/119]  64%|██████▍   , loss=1.01 [03:32<01:51]\n",
      "Epoch [5/5]: [77/119]  65%|██████▍   , loss=1.01 [03:32<01:33]\n",
      "2025-07-15 08:43:00,646 - INFO - Current learning rate: 2.3723834027880643e-05\n",
      "2025-07-15 08:43:01,938 - INFO - Epoch: 5/5, Iter: 78/119 -- train_loss: 0.9963\n",
      "Epoch [5/5]: [77/119]  65%|██████▍   , loss=1.01 [03:33<01:33]\n",
      "Epoch [5/5]: [77/119]  65%|██████▍   , loss=0.996 [03:33<01:33]\n",
      "Epoch [5/5]: [78/119]  66%|██████▌   , loss=0.996 [03:33<01:19]\n",
      "2025-07-15 08:43:01,940 - INFO - Current learning rate: 2.258956505671539e-05\n",
      "2025-07-15 08:43:05,842 - INFO - Epoch: 5/5, Iter: 79/119 -- train_loss: 1.0178\n",
      "Epoch [5/5]: [78/119]  66%|██████▌   , loss=0.996 [03:37<01:19]\n",
      "Epoch [5/5]: [78/119]  66%|██████▌   , loss=1.02 [03:37<01:19]\n",
      "Epoch [5/5]: [79/119]  66%|██████▋   , loss=1.02 [03:37<01:41]\n",
      "2025-07-15 08:43:05,843 - INFO - Current learning rate: 2.1482458064075025e-05\n",
      "2025-07-15 08:43:10,381 - INFO - Epoch: 5/5, Iter: 80/119 -- train_loss: 1.0174\n",
      "Epoch [5/5]: [79/119]  66%|██████▋   , loss=1.02 [03:41<01:41]\n",
      "Epoch [5/5]: [79/119]  66%|██████▋   , loss=1.02 [03:41<01:41]\n",
      "Epoch [5/5]: [80/119]  67%|██████▋   , loss=1.02 [03:41<02:02]\n",
      "2025-07-15 08:43:10,382 - INFO - Current learning rate: 2.040257603787961e-05\n",
      "2025-07-15 08:43:11,583 - INFO - Epoch: 5/5, Iter: 81/119 -- train_loss: 1.0101\n",
      "Epoch [5/5]: [80/119]  67%|██████▋   , loss=1.02 [03:43<02:02]\n",
      "Epoch [5/5]: [80/119]  67%|██████▋   , loss=1.01 [03:43<02:02]\n",
      "Epoch [5/5]: [81/119]  68%|██████▊   , loss=1.01 [03:43<01:37]\n",
      "2025-07-15 08:43:11,585 - INFO - Current learning rate: 1.9349980417107983e-05\n",
      "2025-07-15 08:43:12,870 - INFO - Epoch: 5/5, Iter: 82/119 -- train_loss: 1.0171\n",
      "Epoch [5/5]: [81/119]  68%|██████▊   , loss=1.01 [03:44<01:37]\n",
      "Epoch [5/5]: [81/119]  68%|██████▊   , loss=1.02 [03:44<01:37]\n",
      "Epoch [5/5]: [82/119]  69%|██████▉   , loss=1.02 [03:44<01:20]\n",
      "2025-07-15 08:43:12,872 - INFO - Current learning rate: 1.832473108830163e-05\n",
      "2025-07-15 08:43:16,329 - INFO - Epoch: 5/5, Iter: 83/119 -- train_loss: 1.0174\n",
      "Epoch [5/5]: [82/119]  69%|██████▉   , loss=1.02 [03:47<01:20]\n",
      "Epoch [5/5]: [82/119]  69%|██████▉   , loss=1.02 [03:47<01:20]\n",
      "Epoch [5/5]: [83/119]  70%|██████▉   , loss=1.02 [03:47<01:32]\n",
      "2025-07-15 08:43:16,331 - INFO - Current learning rate: 1.732688638215798e-05\n",
      "2025-07-15 08:43:18,836 - INFO - Epoch: 5/5, Iter: 84/119 -- train_loss: 1.0067\n",
      "Epoch [5/5]: [83/119]  70%|██████▉   , loss=1.02 [03:50<01:32]\n",
      "Epoch [5/5]: [83/119]  70%|██████▉   , loss=1.01 [03:50<01:32]\n",
      "Epoch [5/5]: [84/119]  71%|███████   , loss=1.01 [03:50<01:29]\n",
      "2025-07-15 08:43:18,838 - INFO - Current learning rate: 1.635650307021135e-05\n",
      "2025-07-15 08:43:20,142 - INFO - Epoch: 5/5, Iter: 85/119 -- train_loss: 1.0111\n",
      "Epoch [5/5]: [84/119]  71%|███████   , loss=1.01 [03:51<01:29]\n",
      "Epoch [5/5]: [84/119]  71%|███████   , loss=1.01 [03:51<01:29]\n",
      "Epoch [5/5]: [85/119]  71%|███████▏  , loss=1.01 [03:51<01:13]\n",
      "2025-07-15 08:43:20,143 - INFO - Current learning rate: 1.5413636361603376e-05\n",
      "2025-07-15 08:43:21,561 - INFO - Epoch: 5/5, Iter: 86/119 -- train_loss: 1.0093\n",
      "Epoch [5/5]: [85/119]  71%|███████▏  , loss=1.01 [03:53<01:13]\n",
      "Epoch [5/5]: [85/119]  71%|███████▏  , loss=1.01 [03:53<01:13]\n",
      "Epoch [5/5]: [86/119]  72%|███████▏  , loss=1.01 [03:53<01:04]\n",
      "2025-07-15 08:43:21,562 - INFO - Current learning rate: 1.4498339899941473e-05\n",
      "2025-07-15 08:43:33,907 - INFO - Epoch: 5/5, Iter: 87/119 -- train_loss: 1.0172\n",
      "Epoch [5/5]: [86/119]  72%|███████▏  , loss=1.01 [04:05<01:04]\n",
      "Epoch [5/5]: [86/119]  72%|███████▏  , loss=1.02 [04:05<01:04]\n",
      "Epoch [5/5]: [87/119]  73%|███████▎  , loss=1.02 [04:05<02:42]\n",
      "2025-07-15 08:43:33,909 - INFO - Current learning rate: 1.3610665760247248e-05\n",
      "2025-07-15 08:43:35,325 - INFO - Epoch: 5/5, Iter: 88/119 -- train_loss: 1.0143\n",
      "Epoch [5/5]: [87/119]  73%|███████▎  , loss=1.02 [04:06<02:42]\n",
      "Epoch [5/5]: [87/119]  73%|███████▎  , loss=1.01 [04:06<02:42]\n",
      "Epoch [5/5]: [88/119]  74%|███████▍  , loss=1.01 [04:06<02:03]\n",
      "2025-07-15 08:43:35,327 - INFO - Current learning rate: 1.275066444599343e-05\n",
      "2025-07-15 08:43:36,597 - INFO - Epoch: 5/5, Iter: 89/119 -- train_loss: 1.0171\n",
      "Epoch [5/5]: [88/119]  74%|███████▍  , loss=1.01 [04:08<02:03]\n",
      "Epoch [5/5]: [88/119]  74%|███████▍  , loss=1.02 [04:08<02:03]\n",
      "Epoch [5/5]: [89/119]  75%|███████▍  , loss=1.02 [04:08<01:34]\n",
      "2025-07-15 08:43:36,598 - INFO - Current learning rate: 1.1918384886230842e-05\n",
      "2025-07-15 08:43:37,987 - INFO - Epoch: 5/5, Iter: 90/119 -- train_loss: 1.0115\n",
      "Epoch [5/5]: [89/119]  75%|███████▍  , loss=1.02 [04:09<01:34]\n",
      "Epoch [5/5]: [89/119]  75%|███████▍  , loss=1.01 [04:09<01:34]\n",
      "Epoch [5/5]: [90/119]  76%|███████▌  , loss=1.01 [04:09<01:16]\n",
      "2025-07-15 08:43:37,988 - INFO - Current learning rate: 1.111387443280415e-05\n",
      "2025-07-15 08:43:42,299 - INFO - Epoch: 5/5, Iter: 91/119 -- train_loss: 1.0123\n",
      "Epoch [5/5]: [90/119]  76%|███████▌  , loss=1.01 [04:13<01:16]\n",
      "Epoch [5/5]: [90/119]  76%|███████▌  , loss=1.01 [04:13<01:16]\n",
      "Epoch [5/5]: [91/119]  76%|███████▋  , loss=1.01 [04:13<01:27]\n",
      "2025-07-15 08:43:42,301 - INFO - Current learning rate: 1.03371788576583e-05\n",
      "2025-07-15 08:43:43,546 - INFO - Epoch: 5/5, Iter: 92/119 -- train_loss: 1.0070\n",
      "Epoch [5/5]: [91/119]  76%|███████▋  , loss=1.01 [04:15<01:27]\n",
      "Epoch [5/5]: [91/119]  76%|███████▋  , loss=1.01 [04:15<01:27]\n",
      "Epoch [5/5]: [92/119]  77%|███████▋  , loss=1.01 [04:15<01:09]\n",
      "2025-07-15 08:43:43,547 - INFO - Current learning rate: 9.588342350234049e-06\n",
      "2025-07-15 08:43:45,098 - INFO - Epoch: 5/5, Iter: 93/119 -- train_loss: 1.0175\n",
      "Epoch [5/5]: [92/119]  77%|███████▋  , loss=1.01 [04:16<01:09]\n",
      "Epoch [5/5]: [92/119]  77%|███████▋  , loss=1.02 [04:16<01:09]\n",
      "Epoch [5/5]: [93/119]  78%|███████▊  , loss=1.02 [04:16<00:58]\n",
      "2025-07-15 08:43:45,100 - INFO - Current learning rate: 8.867407514954112e-06\n",
      "2025-07-15 08:43:46,358 - INFO - Epoch: 5/5, Iter: 94/119 -- train_loss: 1.0172\n",
      "Epoch [5/5]: [93/119]  78%|███████▊  , loss=1.02 [04:17<00:58]\n",
      "Epoch [5/5]: [93/119]  78%|███████▊  , loss=1.02 [04:17<00:58]\n",
      "Epoch [5/5]: [94/119]  79%|███████▉  , loss=1.02 [04:17<00:49]\n",
      "2025-07-15 08:43:46,360 - INFO - Current learning rate: 8.174415368798826e-06\n",
      "2025-07-15 08:43:50,391 - INFO - Epoch: 5/5, Iter: 95/119 -- train_loss: 1.0169\n",
      "Epoch [5/5]: [94/119]  79%|███████▉  , loss=1.02 [04:21<00:49]\n",
      "Epoch [5/5]: [94/119]  79%|███████▉  , loss=1.02 [04:21<00:49]\n",
      "Epoch [5/5]: [95/119]  80%|███████▉  , loss=1.02 [04:21<01:02]\n",
      "2025-07-15 08:43:50,392 - INFO - Current learning rate: 7.509405338972965e-06\n",
      "2025-07-15 08:43:51,849 - INFO - Epoch: 5/5, Iter: 96/119 -- train_loss: 1.0021\n",
      "Epoch [5/5]: [95/119]  80%|███████▉  , loss=1.02 [04:23<01:02]\n",
      "Epoch [5/5]: [95/119]  80%|███████▉  , loss=1 [04:23<01:02]\n",
      "Epoch [5/5]: [96/119]  81%|████████  , loss=1 [04:23<00:51]\n",
      "2025-07-15 08:43:51,850 - INFO - Current learning rate: 6.8724152606621624e-06\n",
      "2025-07-15 08:43:53,143 - INFO - Epoch: 5/5, Iter: 97/119 -- train_loss: 1.0047\n",
      "Epoch [5/5]: [96/119]  81%|████████  , loss=1 [04:24<00:51]\n",
      "Epoch [5/5]: [96/119]  81%|████████  , loss=1 [04:24<00:51]\n",
      "Epoch [5/5]: [97/119]  82%|████████▏ , loss=1 [04:24<00:43]\n",
      "2025-07-15 08:43:53,144 - INFO - Current learning rate: 6.26348137488075e-06\n",
      "2025-07-15 08:43:54,371 - INFO - Epoch: 5/5, Iter: 98/119 -- train_loss: 1.0174\n",
      "Epoch [5/5]: [97/119]  82%|████████▏ , loss=1 [04:25<00:43]\n",
      "Epoch [5/5]: [97/119]  82%|████████▏ , loss=1.02 [04:25<00:43]\n",
      "Epoch [5/5]: [98/119]  82%|████████▏ , loss=1.02 [04:25<00:36]\n",
      "2025-07-15 08:43:54,372 - INFO - Current learning rate: 5.682638326409303e-06\n",
      "2025-07-15 08:44:07,805 - INFO - Epoch: 5/5, Iter: 99/119 -- train_loss: 1.0170\n",
      "Epoch [5/5]: [98/119]  82%|████████▏ , loss=1.02 [04:39<00:36]\n",
      "Epoch [5/5]: [98/119]  82%|████████▏ , loss=1.02 [04:39<00:36]\n",
      "Epoch [5/5]: [99/119]  83%|████████▎ , loss=1.02 [04:39<01:44]\n",
      "2025-07-15 08:44:07,806 - INFO - Current learning rate: 5.1299191618241175e-06\n",
      "2025-07-15 08:44:09,433 - INFO - Epoch: 5/5, Iter: 100/119 -- train_loss: 1.0174\n",
      "Epoch [5/5]: [99/119]  83%|████████▎ , loss=1.02 [04:41<01:44]\n",
      "Epoch [5/5]: [99/119]  83%|████████▎ , loss=1.02 [04:41<01:44]\n",
      "Epoch [5/5]: [100/119]  84%|████████▍ , loss=1.02 [04:41<01:19]\n",
      "2025-07-15 08:44:09,434 - INFO - Current learning rate: 4.605355327616659e-06\n",
      "2025-07-15 08:44:11,312 - INFO - Epoch: 5/5, Iter: 101/119 -- train_loss: 1.0088\n",
      "Epoch [5/5]: [100/119]  84%|████████▍ , loss=1.02 [04:42<01:19]\n",
      "Epoch [5/5]: [100/119]  84%|████████▍ , loss=1.01 [04:42<01:19]\n",
      "Epoch [5/5]: [101/119]  85%|████████▍ , loss=1.01 [04:42<01:02]\n",
      "2025-07-15 08:44:11,314 - INFO - Current learning rate: 4.1089766684046205e-06\n",
      "2025-07-15 08:44:12,824 - INFO - Epoch: 5/5, Iter: 102/119 -- train_loss: 1.0137\n",
      "Epoch [5/5]: [101/119]  85%|████████▍ , loss=1.01 [04:44<01:02]\n",
      "Epoch [5/5]: [101/119]  85%|████████▍ , loss=1.01 [04:44<01:02]\n",
      "Epoch [5/5]: [102/119]  86%|████████▌ , loss=1.01 [04:44<00:49]\n",
      "2025-07-15 08:44:12,826 - INFO - Current learning rate: 3.6408114252338922e-06\n",
      "2025-07-15 08:44:19,980 - INFO - Epoch: 5/5, Iter: 103/119 -- train_loss: 1.0173\n",
      "Epoch [5/5]: [102/119]  86%|████████▌ , loss=1.01 [04:51<00:49]\n",
      "Epoch [5/5]: [102/119]  86%|████████▌ , loss=1.02 [04:51<00:49]\n",
      "Epoch [5/5]: [103/119]  87%|████████▋ , loss=1.02 [04:51<01:06]\n",
      "2025-07-15 08:44:19,982 - INFO - Current learning rate: 3.200886233971803e-06\n",
      "2025-07-15 08:44:21,189 - INFO - Epoch: 5/5, Iter: 104/119 -- train_loss: 1.0176\n",
      "Epoch [5/5]: [103/119]  87%|████████▋ , loss=1.02 [04:52<01:06]\n",
      "Epoch [5/5]: [103/119]  87%|████████▋ , loss=1.02 [04:52<01:06]\n",
      "Epoch [5/5]: [104/119]  87%|████████▋ , loss=1.02 [04:52<00:49]\n",
      "2025-07-15 08:44:21,191 - INFO - Current learning rate: 2.7892261237917216e-06\n",
      "2025-07-15 08:44:22,260 - INFO - Epoch: 5/5, Iter: 105/119 -- train_loss: 1.0154\n",
      "Epoch [5/5]: [104/119]  87%|████████▋ , loss=1.02 [04:53<00:49]\n",
      "Epoch [5/5]: [104/119]  87%|████████▋ , loss=1.02 [04:53<00:49]\n",
      "Epoch [5/5]: [105/119]  88%|████████▊ , loss=1.02 [04:53<00:36]\n",
      "2025-07-15 08:44:22,261 - INFO - Current learning rate: 2.4058545157490397e-06\n",
      "2025-07-15 08:44:23,713 - INFO - Epoch: 5/5, Iter: 106/119 -- train_loss: 0.9463\n",
      "Epoch [5/5]: [105/119]  88%|████████▊ , loss=1.02 [04:55<00:36]\n",
      "Epoch [5/5]: [105/119]  88%|████████▊ , loss=0.946 [04:55<00:36]\n",
      "Epoch [5/5]: [106/119]  89%|████████▉ , loss=0.946 [04:55<00:29]\n",
      "2025-07-15 08:44:23,715 - INFO - Current learning rate: 2.0507932214485736e-06\n",
      "2025-07-15 08:44:32,265 - INFO - Epoch: 5/5, Iter: 107/119 -- train_loss: 1.0172\n",
      "Epoch [5/5]: [106/119]  89%|████████▉ , loss=0.946 [05:03<00:29]\n",
      "Epoch [5/5]: [106/119]  89%|████████▉ , loss=1.02 [05:03<00:29]\n",
      "Epoch [5/5]: [107/119]  90%|████████▉ , loss=1.02 [05:03<00:49]\n",
      "2025-07-15 08:44:32,267 - INFO - Current learning rate: 1.7240624418037309e-06\n",
      "2025-07-15 08:44:33,662 - INFO - Epoch: 5/5, Iter: 108/119 -- train_loss: 1.0169\n",
      "Epoch [5/5]: [107/119]  90%|████████▉ , loss=1.02 [05:05<00:49]\n",
      "Epoch [5/5]: [107/119]  90%|████████▉ , loss=1.02 [05:05<00:49]\n",
      "Epoch [5/5]: [108/119]  91%|█████████ , loss=1.02 [05:05<00:36]2025-07-15 08:44:33,663 - INFO - Current learning rate: 1.4256807658871537e-06\n",
      "2025-07-15 08:44:34,828 - INFO - Epoch: 5/5, Iter: 109/119 -- train_loss: 1.0173\n",
      "Epoch [5/5]: [108/119]  91%|█████████ , loss=1.02 [05:06<00:36]\n",
      "Epoch [5/5]: [108/119]  91%|█████████ , loss=1.02 [05:06<00:36]\n",
      "Epoch [5/5]: [109/119]  92%|█████████▏, loss=1.02 [05:06<00:26]\n",
      "2025-07-15 08:44:34,829 - INFO - Current learning rate: 1.155665169873016e-06\n",
      "2025-07-15 08:44:36,296 - INFO - Epoch: 5/5, Iter: 110/119 -- train_loss: 1.0176\n",
      "Epoch [5/5]: [109/119]  92%|█████████▏, loss=1.02 [05:07<00:26]\n",
      "Epoch [5/5]: [109/119]  92%|█████████▏, loss=1.02 [05:07<00:26]\n",
      "Epoch [5/5]: [110/119]  92%|█████████▏, loss=1.02 [05:07<00:20]\n",
      "2025-07-15 08:44:36,298 - INFO - Current learning rate: 9.140310160712988e-07\n",
      "2025-07-15 08:44:44,711 - INFO - Epoch: 5/5, Iter: 111/119 -- train_loss: 1.0179\n",
      "Epoch [5/5]: [110/119]  92%|█████████▏, loss=1.02 [05:16<00:20]\n",
      "Epoch [5/5]: [110/119]  92%|█████████▏, loss=1.02 [05:16<00:20]\n",
      "Epoch [5/5]: [111/119]  93%|█████████▎, loss=1.02 [05:16<00:33]\n",
      "2025-07-15 08:44:44,712 - INFO - Current learning rate: 7.007920520538265e-07\n",
      "2025-07-15 08:44:46,006 - INFO - Epoch: 5/5, Iter: 112/119 -- train_loss: 1.0173\n",
      "Epoch [5/5]: [111/119]  93%|█████████▎, loss=1.02 [05:17<00:33]\n",
      "Epoch [5/5]: [111/119]  93%|█████████▎, loss=1.02 [05:17<00:33]\n",
      "Epoch [5/5]: [112/119]  94%|█████████▍, loss=1.02 [05:17<00:23]\n",
      "2025-07-15 08:44:46,008 - INFO - Current learning rate: 5.159604098718399e-07\n",
      "2025-07-15 08:44:47,183 - INFO - Epoch: 5/5, Iter: 113/119 -- train_loss: 1.0175\n",
      "Epoch [5/5]: [112/119]  94%|█████████▍, loss=1.02 [05:18<00:23]\n",
      "Epoch [5/5]: [112/119]  94%|█████████▍, loss=1.02 [05:18<00:23]\n",
      "Epoch [5/5]: [113/119]  95%|█████████▍, loss=1.02 [05:18<00:15]\n",
      "2025-07-15 08:44:47,184 - INFO - Current learning rate: 3.5954660536605104e-07\n",
      "2025-07-15 08:44:48,246 - INFO - Epoch: 5/5, Iter: 114/119 -- train_loss: 1.0173\n",
      "Epoch [5/5]: [113/119]  95%|█████████▍, loss=1.02 [05:19<00:15]\n",
      "Epoch [5/5]: [113/119]  95%|█████████▍, loss=1.02 [05:19<00:15]\n",
      "Epoch [5/5]: [114/119]  96%|█████████▌, loss=1.02 [05:19<00:10]\n",
      "2025-07-15 08:44:48,247 - INFO - Current learning rate: 2.3155953756812456e-07\n",
      "2025-07-15 08:44:53,266 - INFO - Epoch: 5/5, Iter: 115/119 -- train_loss: 1.0150\n",
      "Epoch [5/5]: [114/119]  96%|█████████▌, loss=1.02 [05:24<00:10]\n",
      "Epoch [5/5]: [114/119]  96%|█████████▌, loss=1.02 [05:24<00:10]\n",
      "Epoch [5/5]: [115/119]  97%|█████████▋, loss=1.02 [05:24<00:12]\n",
      "2025-07-15 08:44:53,268 - INFO - Current learning rate: 1.3200648819452887e-07\n",
      "2025-07-15 08:44:54,329 - INFO - Epoch: 5/5, Iter: 116/119 -- train_loss: 1.0168\n",
      "Epoch [5/5]: [115/119]  97%|█████████▋, loss=1.02 [05:25<00:12]\n",
      "Epoch [5/5]: [115/119]  97%|█████████▋, loss=1.02 [05:25<00:12]\n",
      "Epoch [5/5]: [116/119]  97%|█████████▋, loss=1.02 [05:25<00:07]\n",
      "2025-07-15 08:44:54,330 - INFO - Current learning rate: 6.08931212322026e-08\n",
      "2025-07-15 08:44:55,243 - INFO - Epoch: 5/5, Iter: 117/119 -- train_loss: 1.0175\n",
      "Epoch [5/5]: [116/119]  97%|█████████▋, loss=1.02 [05:26<00:07]\n",
      "Epoch [5/5]: [116/119]  97%|█████████▋, loss=1.02 [05:26<00:07]\n",
      "Epoch [5/5]: [117/119]  98%|█████████▊, loss=1.02 [05:26<00:03]\n",
      "2025-07-15 08:44:55,244 - INFO - Current learning rate: 1.8223482616313685e-08\n",
      "2025-07-15 08:44:56,231 - INFO - Epoch: 5/5, Iter: 118/119 -- train_loss: 1.0161\n",
      "Epoch [5/5]: [117/119]  98%|█████████▊, loss=1.02 [05:27<00:03]\n",
      "Epoch [5/5]: [117/119]  98%|█████████▊, loss=1.02 [05:27<00:03]\n",
      "Epoch [5/5]: [118/119]  99%|█████████▉, loss=1.02 [05:27<00:01]\n",
      "2025-07-15 08:44:56,232 - INFO - Current learning rate: 4e-09\n",
      "2025-07-15 08:44:58,647 - INFO - Epoch: 5/5, Iter: 119/119 -- train_loss: 1.0140\n",
      "Epoch [5/5]: [118/119]  99%|█████████▉, loss=1.02 [05:30<00:01]\n",
      "Epoch [5/5]: [118/119]  99%|█████████▉, loss=1.01 [05:30<00:01]\n",
      "Epoch [5/5]: [119/119] 100%|██████████, loss=1.01 [05:30<00:00]\n",
      "2025-07-15 08:44:58,648 - INFO - Current learning rate: 1.8223482616313685e-08\n",
      "2025-07-15 08:44:58,648 - INFO - Engine run resuming from iteration 0, epoch 4 until 5 epochs\n",
      "[1/20]   5%|#033[32m▌         #033[0m [00:00<?]#033[A\n",
      "Epoch [5/5]: [1/20]   5%|#033[32m▌         #033[0m [00:00<?]#033[A\n",
      "Epoch [5/5]: [1/20]   5%|#033[32m▌         #033[0m [00:08<?]#033[A\n",
      "Epoch [5/5]: [2/20]  10%|#033[32m█         #033[0m [00:08<02:35]#033[A\n",
      "Epoch [5/5]: [2/20]  10%|#033[32m█         #033[0m [00:17<02:35]#033[A\n",
      "Epoch [5/5]: [3/20]  15%|#033[32m█▌        #033[0m [00:17<02:29]#033[A\n",
      "Epoch [5/5]: [3/20]  15%|#033[32m█▌        #033[0m [00:23<02:29]#033[A\n",
      "Epoch [5/5]: [4/20]  20%|#033[32m██        #033[0m [00:23<02:00]#033[A\n",
      "Epoch [5/5]: [4/20]  20%|#033[32m██        #033[0m [00:30<02:00]#033[A\n",
      "Epoch [5/5]: [5/20]  25%|#033[32m██▌       #033[0m [00:30<01:51]#033[A\n",
      "Epoch [5/5]: [5/20]  25%|#033[32m██▌       #033[0m [00:39<01:51]#033[A\n",
      "Epoch [5/5]: [6/20]  30%|#033[32m███       #033[0m [00:39<01:48]#033[A\n",
      "Epoch [5/5]: [6/20]  30%|#033[32m███       #033[0m [00:47<01:48]#033[A\n",
      "Epoch [5/5]: [7/20]  35%|#033[32m███▌      #033[0m [00:47<01:44]#033[A\n",
      "Epoch [5/5]: [7/20]  35%|#033[32m███▌      #033[0m [00:56<01:44]#033[A\n",
      "Epoch [5/5]: [8/20]  40%|#033[32m████      #033[0m [00:56<01:38]#033[A\n",
      "Epoch [5/5]: [8/20]  40%|#033[32m████      #033[0m [01:02<01:38]#033[A\n",
      "Epoch [5/5]: [9/20]  45%|#033[32m████▌     #033[0m [01:02<01:23]#033[A\n",
      "Epoch [5/5]: [9/20]  45%|#033[32m████▌     #033[0m [01:15<01:23]#033[A\n",
      "Epoch [5/5]: [10/20]  50%|#033[32m█████     #033[0m [01:15<01:30]#033[A\n",
      "Epoch [5/5]: [10/20]  50%|#033[32m█████     #033[0m [01:25<01:30]#033[A\n",
      "Epoch [5/5]: [11/20]  55%|#033[32m█████▌    #033[0m [01:25<01:26]#033[A\n",
      "Epoch [5/5]: [11/20]  55%|#033[32m█████▌    #033[0m [01:36<01:26]#033[A\n",
      "Epoch [5/5]: [12/20]  60%|#033[32m██████    #033[0m [01:36<01:20]#033[A\n",
      "Epoch [5/5]: [12/20]  60%|#033[32m██████    #033[0m [01:46<01:20]#033[A\n",
      "Epoch [5/5]: [13/20]  65%|#033[32m██████▌   #033[0m [01:46<01:09]#033[A\n",
      "Epoch [5/5]: [13/20]  65%|#033[32m██████▌   #033[0m [01:54<01:09]#033[A\n",
      "Epoch [5/5]: [14/20]  70%|#033[32m███████   #033[0m [01:54<00:55]#033[A\n",
      "Epoch [5/5]: [14/20]  70%|#033[32m███████   #033[0m [01:59<00:55]#033[A\n",
      "Epoch [5/5]: [15/20]  75%|#033[32m███████▌  #033[0m [01:59<00:40]#033[A\n",
      "Epoch [5/5]: [15/20]  75%|#033[32m███████▌  #033[0m [02:04<00:40]#033[A\n",
      "Epoch [5/5]: [16/20]  80%|#033[32m████████  #033[0m [02:04<00:28]#033[A\n",
      "Epoch [5/5]: [16/20]  80%|#033[32m████████  #033[0m [02:12<00:28]#033[A\n",
      "Epoch [5/5]: [17/20]  85%|#033[32m████████▌ #033[0m [02:12<00:22]#033[A\n",
      "Epoch [5/5]: [17/20]  85%|#033[32m████████▌ #033[0m [02:19<00:22]#033[A\n",
      "Epoch [5/5]: [18/20]  90%|#033[32m█████████ #033[0m [02:19<00:14]#033[A\n",
      "Epoch [5/5]: [18/20]  90%|#033[32m█████████ #033[0m [02:27<00:14]#033[A\n",
      "Epoch [5/5]: [19/20]  95%|#033[32m█████████▌#033[0m [02:27<00:07]#033[A\n",
      "Epoch [5/5]: [19/20]  95%|#033[32m█████████▌#033[0m [02:35<00:07]#033[A\n",
      "Epoch [5/5]: [20/20] 100%|#033[32m██████████#033[0m [02:35<00:00]#033[A\n",
      "#033[A\n",
      "2025-07-15 08:47:48,633 - INFO - Epoch[5] Complete. Time taken: 00:02:49.883\n",
      "2025-07-15 08:47:48,634 - INFO - Engine run finished. Time taken: 00:02:49.985\n",
      "2025-07-15 08:47:48,732 - INFO - Epoch[5] Complete. Time taken: 00:08:35.053\n",
      "2025-07-15 08:47:48,732 - INFO - Engine run finished. Time taken: 00:43:34.854\n",
      "2025-07-15 08:47:48,910 - INFO - Training completed in 43.58 minutes\n",
      "2025-07-15 08:47:49,906 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2025-07-15 08:47:49,906 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2025-07-15 08:47:49,907 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\n",
      "2025-07-15 08:47:57 Uploading - Uploading generated training model\n",
      "2025-07-15 08:48:10 Completed - Training job completed\n",
      "Training seconds: 2732\n",
      "Billable seconds: 2732\n",
      "CPU times: user 5.59 s, sys: 385 ms, total: 5.98 s\n",
      "Wall time: 46min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sagemaker.pytorch.estimator import PyTorch\n",
    "import os\n",
    "\n",
    "image = \"763104351884.dkr.ecr.ap-southeast-1.amazonaws.com/pytorch-training:2.6.0-gpu-py312-cu124-ubuntu22.04-sagemaker\"\n",
    "\n",
    "def run_training():\n",
    "    estimator = PyTorch(\n",
    "        image_uri=image,\n",
    "        entry_point=\"train_gpt.py\",\n",
    "        source_dir=\".\",\n",
    "        role=role,\n",
    "        region=\"ap-southeast-1\",\n",
    "        instance_type=\"ml.g4dn.2xlarge\", # ml.g5.xlarge ran out of memory halfway through 2nd epoch (free tier instance)\n",
    "        instance_count=1,\n",
    "        framework_version=\"2.6.0\",\n",
    "        py_version=\"py312\",\n",
    "        enable_spot_training=True,  # for cost savings\n",
    "        # max_run=3600,\n",
    "        input_mode=\"File\",\n",
    "        sagemaker_session=sess,\n",
    "    )\n",
    "    inputs={\n",
    "        \"training\": train_channel,\n",
    "        \"model\": model_channel,\n",
    "        \"training_config\": os.environ.get(\"SM_CHANNEL_TRAINING_CONFIG\", \"./tumor.yaml\"),\n",
    "    }\n",
    "    estimator.fit(inputs)\n",
    "\n",
    "run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34073fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
      "network_tumor_1_key_metric=0.0066.pt\n"
     ]
    }
   ],
   "source": [
    "# !aws s3 cp s3://sagemaker-ap-southeast-1-345594598345/pytorch-training-2025-07-15-08-01-51-392/output/model.tar.gz .\n",
    "!tar -xvzf model.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4976e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p158",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
