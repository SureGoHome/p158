{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created: UNet\n",
      "Data loaders created - Train: 119 batches, Val: 20 batches\n",
      "loss_params: This is the loss params Munch({'include_background': False, 'softmax': True, 'to_onehot_y': True})\n",
      "Optimizer structure: Munch({'Novograd': Munch({'lr': 0.001, 'weight_decay': 0.01, 'amsgrad': True})})\n",
      "Training setup complete - Loss: DiceFocalLoss, Optimizer: Novograd\n",
      "Starting training for 1000 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:   0%|          | 0/119 [00:29<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 31 but got size 32 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 397\u001b[39m\n\u001b[32m    393\u001b[39m                 f.write(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    396\u001b[39m trainer = SimpleSegmentationTrainer(\u001b[33m\"\u001b[39m\u001b[33mtumor.yaml\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m397\u001b[39m best_metric, best_epoch = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 335\u001b[39m, in \u001b[36mSimpleSegmentationTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStarting training for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.config.training.max_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m epochs\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.config.training.max_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m    334\u001b[39m     \u001b[38;5;66;03m# Train for one epoch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     train_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     train_losses.append(train_loss)\n\u001b[32m    338\u001b[39m     \u001b[38;5;66;03m# Validate\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 303\u001b[39m, in \u001b[36mSimpleSegmentationTrainer.train_epoch\u001b[39m\u001b[34m(self, epoch)\u001b[39m\n\u001b[32m    301\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m    302\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[32m    306\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.loss_function(outputs, labels)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arsonist\\Documents\\p158\\p158-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arsonist\\Documents\\p158\\p158-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arsonist\\Documents\\p158\\p158-env\\Lib\\site-packages\\monai\\networks\\nets\\unet.py:297\u001b[39m, in \u001b[36mUNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arsonist\\Documents\\p158\\p158-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arsonist\\Documents\\p158\\p158-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arsonist\\Documents\\p158\\p158-env\\Lib\\site-packages\\torch\\nn\\modules\\container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arsonist\\Documents\\p158\\p158-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arsonist\\Documents\\p158\\p158-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arsonist\\Documents\\p158\\p158-env\\Lib\\site-packages\\monai\\networks\\layers\\simplelayers.py:129\u001b[39m, in \u001b[36mSkipConnection.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33mcat\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    132\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat([x, y], dim=\u001b[38;5;28mself\u001b[39m.dim)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arsonist\\Documents\\p158\\p158-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arsonist\\Documents\\p158\\p158-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arsonist\\Documents\\p158\\p158-env\\Lib\\site-packages\\torch\\nn\\modules\\container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arsonist\\Documents\\p158\\p158-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arsonist\\Documents\\p158\\p158-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arsonist\\Documents\\p158\\p158-env\\Lib\\site-packages\\monai\\networks\\layers\\simplelayers.py:129\u001b[39m, in \u001b[36mSkipConnection.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33mcat\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    132\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat([x, y], dim=\u001b[38;5;28mself\u001b[39m.dim)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arsonist\\Documents\\p158\\p158-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arsonist\\Documents\\p158\\p158-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arsonist\\Documents\\p158\\p158-env\\Lib\\site-packages\\torch\\nn\\modules\\container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arsonist\\Documents\\p158\\p158-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arsonist\\Documents\\p158\\p158-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arsonist\\Documents\\p158\\p158-env\\Lib\\site-packages\\monai\\networks\\layers\\simplelayers.py:132\u001b[39m, in \u001b[36mSkipConnection.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    129\u001b[39m y = \u001b[38;5;28mself\u001b[39m.submodule(x)\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33mcat\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33madd\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.add(x, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arsonist\\Documents\\p158\\p158-env\\Lib\\site-packages\\monai\\data\\meta_tensor.py:282\u001b[39m, in \u001b[36mMetaTensor.__torch_function__\u001b[39m\u001b[34m(cls, func, types, args, kwargs)\u001b[39m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    281\u001b[39m     kwargs = {}\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m ret = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;66;03m# if `out` has been used as argument, metadata is not copied, nothing to do.\u001b[39;00m\n\u001b[32m    284\u001b[39m \u001b[38;5;66;03m# if \"out\" in kwargs:\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[38;5;66;03m#     return ret\u001b[39;00m\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _not_requiring_metadata(ret):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arsonist\\Documents\\p158\\p158-env\\Lib\\site-packages\\torch\\_tensor.py:1668\u001b[39m, in \u001b[36mTensor.__torch_function__\u001b[39m\u001b[34m(cls, func, types, args, kwargs)\u001b[39m\n\u001b[32m   1665\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m   1667\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _C.DisableTorchFunctionSubclass():\n\u001b[32m-> \u001b[39m\u001b[32m1668\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[32m   1670\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[31mRuntimeError\u001b[39m: Sizes of tensors must match except in dimension 1. Expected size 31 but got size 32 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import monai\n",
    "from monai.networks.nets import UNet\n",
    "from monai.transforms import (\n",
    "    LoadImaged,\n",
    "    EnsureChannelFirstd,\n",
    "    ScaleIntensityd,\n",
    "    Orientationd,\n",
    "    Spacingd,\n",
    "    CropForegroundd,\n",
    "    ToTensord,\n",
    "    Compose,\n",
    ")\n",
    "from monai.data import Dataset, DataLoader, PersistentDataset\n",
    "from monai.losses import DiceFocalLoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.inferers import SlidingWindowInferer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import munch\n",
    "\n",
    "# Add MetaTensor to safe globals for pickle\n",
    "from monai.data.meta_tensor import MetaTensor\n",
    "\n",
    "torch.serialization.add_safe_globals([MetaTensor])\n",
    "\n",
    "\n",
    "def load_config(fn: str = \"config.yaml\"):\n",
    "    \"Load config from YAML and return a serialized dictionary object\"\n",
    "    with open(fn, \"r\") as stream:\n",
    "        config = yaml.safe_load(stream)\n",
    "    config = munch.munchify(config)\n",
    "\n",
    "    if not config.overwrite:\n",
    "        i = 1\n",
    "        while os.path.exists(config.run_id + f\"_{i}\"):\n",
    "            i += 1\n",
    "        config.run_id += f\"_{i}\"\n",
    "\n",
    "    config.out_dir = os.path.join(config.run_id, config.out_dir)\n",
    "    config.log_dir = os.path.join(config.run_id, config.log_dir)\n",
    "\n",
    "    if not isinstance(config.data.image_cols, (tuple, list)):\n",
    "        config.data.image_cols = [config.data.image_cols]\n",
    "    if not isinstance(config.data.label_cols, (tuple, list)):\n",
    "        config.data.label_cols = [config.data.label_cols]\n",
    "\n",
    "    config.transforms.mode = (\"bilinear\",) * len(config.data.image_cols) + (\n",
    "        \"nearest\",\n",
    "    ) * len(config.data.label_cols)\n",
    "    return config\n",
    "\n",
    "\n",
    "# Simple 3D UNet Trainer\n",
    "class SimpleSegmentationTrainer:\n",
    "    def __init__(self, config_file=\"tumor.yaml\"):\n",
    "        self.config = load_config(config_file)\n",
    "        self.device = torch.device(\n",
    "            self.config.device if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "\n",
    "        # Create output directories\n",
    "        self.setup_directories()\n",
    "\n",
    "        # Set random seed\n",
    "        torch.manual_seed(self.config.seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(self.config.seed)\n",
    "\n",
    "        # Initialize the model\n",
    "        self.create_model()\n",
    "\n",
    "        # Setup data loaders\n",
    "        self.setup_data()\n",
    "\n",
    "        # Setup loss function and optimizer\n",
    "        self.setup_training()\n",
    "\n",
    "    def setup_directories(self):\n",
    "        \"\"\"Create necessary directories for outputs\"\"\"\n",
    "        os.makedirs(self.config.out_dir, exist_ok=True)\n",
    "        os.makedirs(self.config.model_dir, exist_ok=True)\n",
    "        os.makedirs(self.config.log_dir, exist_ok=True)\n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\"Initialize a simple UNet model\"\"\"\n",
    "        self.model = UNet(\n",
    "            spatial_dims=self.config.ndim,\n",
    "            in_channels=self.config.model.in_channels,\n",
    "            out_channels=self.config.model.out_channels,\n",
    "            channels=self.config.model.channels,\n",
    "            strides=self.config.model.strides,\n",
    "            dropout=self.config.model.dropout,\n",
    "            num_res_units=self.config.model.num_res_units,\n",
    "        )\n",
    "\n",
    "        self.model = self.model.to(self.device)\n",
    "        print(f\"Model created: {type(self.model).__name__}\")\n",
    "\n",
    "    def setup_data(self):\n",
    "        \"\"\"Setup data loaders for training and validation\"\"\"\n",
    "        # Read CSV files\n",
    "        train_df = pd.read_csv(self.config.data.train_csv)\n",
    "        valid_df = pd.read_csv(self.config.data.valid_csv)\n",
    "\n",
    "        if self.config.debug:\n",
    "            train_df = train_df.sample(4)\n",
    "            valid_df = valid_df.sample(2)\n",
    "\n",
    "        # Process data paths\n",
    "        data_dir = self.config.data.data_dir\n",
    "        image_cols = (\n",
    "            self.config.data.image_cols\n",
    "            if isinstance(self.config.data.image_cols, list)\n",
    "            else [self.config.data.image_cols]\n",
    "        )\n",
    "        label_cols = (\n",
    "            self.config.data.label_cols\n",
    "            if isinstance(self.config.data.label_cols, list)\n",
    "            else [self.config.data.label_cols]\n",
    "        )\n",
    "\n",
    "        # Create data dictionaries\n",
    "        train_files = []\n",
    "        for _, row in train_df.iterrows():\n",
    "            data_dict = {}\n",
    "            for col in image_cols:\n",
    "                data_dict[col] = os.path.join(data_dir, row[col])\n",
    "            for col in label_cols:\n",
    "                data_dict[col] = os.path.join(data_dir, row[col])\n",
    "            train_files.append(data_dict)\n",
    "\n",
    "        val_files = []\n",
    "        for _, row in valid_df.iterrows():\n",
    "            data_dict = {}\n",
    "            for col in image_cols:\n",
    "                data_dict[col] = os.path.join(data_dir, row[col])\n",
    "            for col in label_cols:\n",
    "                data_dict[col] = os.path.join(data_dir, row[col])\n",
    "            val_files.append(data_dict)\n",
    "\n",
    "        # Create transforms\n",
    "        train_transforms = self.get_transforms(image_cols, label_cols)\n",
    "        val_transforms = self.get_transforms(image_cols, label_cols)\n",
    "\n",
    "        # Create datasets and data loaders\n",
    "        if self.config.data.dataset_type == \"persistent\":\n",
    "            # Create cache directory\n",
    "            os.makedirs(self.config.data.cache_dir, exist_ok=True)\n",
    "            train_ds = Dataset(data=train_files, transform=train_transforms)\n",
    "            val_ds = Dataset(data=val_files, transform=val_transforms)\n",
    "        else:\n",
    "            train_ds = Dataset(data=train_files, transform=train_transforms)\n",
    "            val_ds = Dataset(data=val_files, transform=val_transforms)\n",
    "\n",
    "        self.train_loader = DataLoader(\n",
    "            train_ds,\n",
    "            batch_size=self.config.data.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            persistent_workers=False,  # Make sure this is False\n",
    "        )\n",
    "\n",
    "        self.val_loader = DataLoader(\n",
    "            val_ds,\n",
    "            batch_size=1,  # Always use batch size 1 for validation\n",
    "            num_workers=4,\n",
    "            persistent_workers=False,  # Make sure this is False\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Data loaders created - Train: {len(self.train_loader)} batches, Val: {len(self.val_loader)} batches\"\n",
    "        )\n",
    "\n",
    "    def get_transforms(self, image_cols, label_cols):\n",
    "        \"\"\"Create transforms with proper mode handling\"\"\"\n",
    "        all_keys = [*image_cols, *label_cols]\n",
    "\n",
    "        # Generate modes with bilinear for images and nearest for labels\n",
    "        modes = [\"bilinear\"] * len(image_cols) + [\"nearest\"] * len(label_cols)\n",
    "\n",
    "        # Basic transforms\n",
    "        transforms_list = [\n",
    "            LoadImaged(keys=all_keys),\n",
    "            EnsureChannelFirstd(keys=all_keys),\n",
    "            Spacingd(\n",
    "                keys=all_keys,\n",
    "                pixdim=self.config.transforms.spacing,\n",
    "                mode=modes,\n",
    "            ),\n",
    "            Orientationd(keys=all_keys, axcodes=self.config.transforms.orientation),\n",
    "            ScaleIntensityd(keys=image_cols),\n",
    "            CropForegroundd(keys=all_keys, source_key=image_cols[0]),\n",
    "            ToTensord(keys=all_keys),\n",
    "        ]\n",
    "\n",
    "        return Compose(transforms_list)\n",
    "\n",
    "    def setup_training(self):\n",
    "        \"\"\"Setup loss function, optimizer and learning rate scheduler\"\"\"\n",
    "        # Loss function\n",
    "        loss_params = self.config.loss.DiceFocalLoss\n",
    "        print(f\"loss_params: This is the loss params {loss_params}\")\n",
    "\n",
    "        # Configure loss function\n",
    "        self.loss_function = DiceFocalLoss(\n",
    "            include_background=loss_params.include_background,\n",
    "            to_onehot_y=loss_params.to_onehot_y,\n",
    "            softmax=loss_params.softmax,\n",
    "        )\n",
    "\n",
    "        # Print the optimizer structure to debug\n",
    "        print(f\"Optimizer structure: {self.config.optimizer}\")\n",
    "\n",
    "        # Optimizer - use Novograd with hardcoded parameters from tumor.yaml\n",
    "        from monai.optimizers import Novograd\n",
    "\n",
    "        self.optimizer = Novograd(\n",
    "            self.model.parameters(),\n",
    "            lr=0.001,  # Use hardcoded value from your YAML\n",
    "            weight_decay=0.01,  # Use hardcoded value from your YAML\n",
    "            amsgrad=True,  # Use hardcoded value from your YAML\n",
    "        )\n",
    "\n",
    "        # Learning rate scheduler - assume OneCycleLR\n",
    "        scheduler_name = \"OneCycleLR\"\n",
    "        scheduler_params = self.config.lr_scheduler.OneCycleLR\n",
    "\n",
    "        self.scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "            self.optimizer,\n",
    "            max_lr=scheduler_params.max_lr,\n",
    "            steps_per_epoch=len(self.train_loader),\n",
    "            epochs=self.config.training.max_epochs,\n",
    "        )\n",
    "\n",
    "        # Metrics\n",
    "        self.metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
    "\n",
    "        # Inferer for sliding window inference\n",
    "        self.inferer = SlidingWindowInferer(\n",
    "            roi_size=(64, 64, 64), sw_batch_size=4, overlap=0.5\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Training setup complete - Loss: {type(self.loss_function).__name__}, Optimizer: Novograd\"\n",
    "        )\n",
    "\n",
    "    def validate(self, epoch):\n",
    "        \"\"\"Run validation\"\"\"\n",
    "        self.model.eval()\n",
    "        metric_values = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_data in tqdm(self.val_loader, desc=\"Validation\"):\n",
    "                # Process inputs\n",
    "                inputs = torch.cat(\n",
    "                    [batch_data[key] for key in self.config.data.image_cols], dim=1\n",
    "                ).to(self.device)\n",
    "                label_key = self.config.data.label_cols\n",
    "                if isinstance(label_key, list):\n",
    "                    label_key = label_key[0]\n",
    "                labels = batch_data[label_key].to(self.device)\n",
    "\n",
    "                # Use sliding window inference for validation\n",
    "                outputs = self.inferer(inputs, self.model)\n",
    "\n",
    "                # Calculate metrics\n",
    "                self.metric(y_pred=outputs, y=labels)\n",
    "                metric_values.append(self.metric.aggregate().item())\n",
    "                self.metric.reset()\n",
    "\n",
    "        # Calculate mean Dice score\n",
    "        mean_metric = np.mean(metric_values)\n",
    "        print(f\"Epoch {epoch} - Validation Dice: {mean_metric:.4f}\")\n",
    "\n",
    "        return mean_metric\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        \"\"\"Run one epoch of training\"\"\"\n",
    "        self.model.train()\n",
    "        epoch_loss = 0\n",
    "        progress = tqdm(\n",
    "            self.train_loader, desc=f\"Epoch {epoch}/{self.config.training.max_epochs}\"\n",
    "        )\n",
    "\n",
    "        for batch_data in progress:\n",
    "            # Process inputs\n",
    "            inputs = torch.cat(\n",
    "                [batch_data[key] for key in self.config.data.image_cols], dim=1\n",
    "            ).to(self.device)\n",
    "            label_key = self.config.data.label_cols\n",
    "            if isinstance(label_key, list):\n",
    "                label_key = label_key[0]\n",
    "            labels = batch_data[label_key].to(self.device)\n",
    "\n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = self.loss_function(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Update learning rate if using OneCycleLR\n",
    "            if hasattr(self, \"scheduler\"):\n",
    "                self.scheduler.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            epoch_loss += loss.item()\n",
    "            progress.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "        # Return average loss for the epoch\n",
    "        return epoch_loss / len(self.train_loader)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Main training loop\"\"\"\n",
    "        best_metric = -1\n",
    "        best_epoch = -1\n",
    "        patience_counter = 0\n",
    "        train_losses = []\n",
    "        val_metrics = []\n",
    "\n",
    "        print(f\"Starting training for {self.config.training.max_epochs} epochs\")\n",
    "\n",
    "        for epoch in range(1, self.config.training.max_epochs + 1):\n",
    "            # Train for one epoch\n",
    "            train_loss = self.train_epoch(epoch)\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "            # Validate\n",
    "            val_metric = self.validate(epoch)\n",
    "            val_metrics.append(val_metric)\n",
    "\n",
    "            # Save best model\n",
    "            if val_metric > best_metric:\n",
    "                best_metric = val_metric\n",
    "                best_epoch = epoch\n",
    "                patience_counter = 0\n",
    "\n",
    "                # Save model\n",
    "                model_path = os.path.join(\n",
    "                    self.config.model_dir, f\"{self.config.run_id}_best_model.pth\"\n",
    "                )\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"epoch\": epoch,\n",
    "                        \"model_state_dict\": self.model.state_dict(),\n",
    "                        \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "                        \"val_metric\": val_metric,\n",
    "                    },\n",
    "                    model_path,\n",
    "                )\n",
    "\n",
    "                print(\n",
    "                    f\"Saved new best model at epoch {epoch} with Dice: {val_metric:.4f}\"\n",
    "                )\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            # Early stopping\n",
    "            if patience_counter >= self.config.training.early_stopping_patience:\n",
    "                print(f\"Early stopping triggered after {epoch} epochs\")\n",
    "                break\n",
    "\n",
    "            # Save training progress\n",
    "            self.save_metrics(train_losses, val_metrics)\n",
    "\n",
    "        print(\n",
    "            f\"Training completed. Best model at epoch {best_epoch} with Dice: {best_metric:.4f}\"\n",
    "        )\n",
    "        return best_metric, best_epoch\n",
    "\n",
    "    def save_metrics(self, train_losses, val_metrics):\n",
    "        \"\"\"Save metrics to files\"\"\"\n",
    "        # Save training loss\n",
    "        with open(os.path.join(self.config.log_dir, \"train_logs.csv\"), \"w\") as f:\n",
    "            f.write(\"epoch,loss\\n\")\n",
    "            for i, loss in enumerate(train_losses):\n",
    "                f.write(f\"{i+1},{loss}\\n\")\n",
    "\n",
    "        # Save validation metrics\n",
    "        with open(os.path.join(self.config.log_dir, \"metric_logs.csv\"), \"w\") as f:\n",
    "            f.write(\"epoch,dice\\n\")\n",
    "            for i, metric in enumerate(val_metrics):\n",
    "                f.write(f\"{i+1},{metric}\\n\")\n",
    "\n",
    "\n",
    "trainer = SimpleSegmentationTrainer(\"tumor.yaml\")\n",
    "best_metric, best_epoch = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created: UNet\n",
      "Data loaders created - Train: 119 batches, Val: 20 batches\n",
      "loss_params: This is the loss params Munch({'include_background': False, 'softmax': True, 'to_onehot_y': True})\n",
      "Optimizer structure: Munch({'Novograd': Munch({'lr': 0.001, 'weight_decay': 0.01, 'amsgrad': True})})\n",
      "Training setup complete - Loss: DiceFocalLoss, Optimizer: Novograd\n",
      "Starting training for 1000 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000: 100%|██████████| 119/119 [02:32<00:00,  1.29s/it, loss=1.35] \n",
      "Validation: 100%|██████████| 20/20 [00:40<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Validation Dice: 0.0483\n",
      "Saved new best model at epoch 1 with Dice: 0.0483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000: 100%|██████████| 119/119 [02:33<00:00,  1.29s/it, loss=1.21] \n",
      "Validation: 100%|██████████| 20/20 [00:40<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Validation Dice: 0.0483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000: 100%|██████████| 119/119 [03:27<00:00,  1.75s/it, loss=1.04] \n",
      "Validation: 100%|██████████| 20/20 [00:40<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Validation Dice: 0.0483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000: 100%|██████████| 119/119 [02:30<00:00,  1.26s/it, loss=1.13] \n",
      "Validation: 100%|██████████| 20/20 [00:40<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Validation Dice: 0.0483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000: 100%|██████████| 119/119 [02:31<00:00,  1.27s/it, loss=1.13]\n",
      "Validation: 100%|██████████| 20/20 [00:40<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Validation Dice: 0.0483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000: 100%|██████████| 119/119 [02:33<00:00,  1.29s/it, loss=1.11] \n",
      "Validation: 100%|██████████| 20/20 [00:40<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Validation Dice: 0.0483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000: 100%|██████████| 119/119 [02:36<00:00,  1.31s/it, loss=0.77] \n",
      "Validation: 100%|██████████| 20/20 [00:40<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Validation Dice: 0.0483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000: 100%|██████████| 119/119 [02:34<00:00,  1.30s/it, loss=1.09] \n",
      "Validation: 100%|██████████| 20/20 [00:40<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Validation Dice: 0.0483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000: 100%|██████████| 119/119 [02:31<00:00,  1.27s/it, loss=1.09] \n",
      "Validation: 100%|██████████| 20/20 [00:40<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Validation Dice: 0.0483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000: 100%|██████████| 119/119 [02:32<00:00,  1.29s/it, loss=1.08] \n",
      "Validation: 100%|██████████| 20/20 [00:40<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Validation Dice: 0.0483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000: 100%|██████████| 119/119 [02:30<00:00,  1.27s/it, loss=1.07] \n",
      "Validation: 100%|██████████| 20/20 [00:40<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Validation Dice: 0.0483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000: 100%|██████████| 119/119 [02:35<00:00,  1.30s/it, loss=0.979]\n",
      "Validation: 100%|██████████| 20/20 [00:40<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Validation Dice: 0.0483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000: 100%|██████████| 119/119 [02:32<00:00,  1.28s/it, loss=1.06] \n",
      "Validation: 100%|██████████| 20/20 [00:41<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Validation Dice: 0.0483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/1000: 100%|██████████| 119/119 [02:34<00:00,  1.30s/it, loss=1.06] \n",
      "Validation: 100%|██████████| 20/20 [00:40<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Validation Dice: 0.0483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/1000: 100%|██████████| 119/119 [02:31<00:00,  1.28s/it, loss=1.02] \n",
      "Validation: 100%|██████████| 20/20 [00:40<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Validation Dice: 0.0483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/1000: 100%|██████████| 119/119 [02:33<00:00,  1.29s/it, loss=1.05] \n",
      "Validation: 100%|██████████| 20/20 [00:40<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Validation Dice: 0.0483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/1000: 100%|██████████| 119/119 [02:32<00:00,  1.29s/it, loss=1.05] \n",
      "Validation: 100%|██████████| 20/20 [00:40<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Validation Dice: 0.0483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/1000: 100%|██████████| 119/119 [02:33<00:00,  1.29s/it, loss=1.01] \n",
      "Validation: 100%|██████████| 20/20 [00:40<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Validation Dice: 0.0483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/1000: 100%|██████████| 119/119 [02:32<00:00,  1.28s/it, loss=1.04] \n",
      "Validation: 100%|██████████| 20/20 [00:40<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Validation Dice: 0.0483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/1000: 100%|██████████| 119/119 [02:33<00:00,  1.29s/it, loss=1.04] \n",
      "Validation: 100%|██████████| 20/20 [00:40<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Validation Dice: 0.0483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/1000: 100%|██████████| 119/119 [02:32<00:00,  1.28s/it, loss=0.741]\n",
      "Validation: 100%|██████████| 20/20 [00:40<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 - Validation Dice: 0.0483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/1000: 100%|██████████| 119/119 [02:33<00:00,  1.29s/it, loss=0.952]\n",
      "Validation: 100%|██████████| 20/20 [00:40<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 - Validation Dice: 0.0483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/1000: 100%|██████████| 119/119 [02:34<00:00,  1.29s/it, loss=0.893]\n",
      "Validation: 100%|██████████| 20/20 [00:40<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 - Validation Dice: 0.0483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/1000: 100%|██████████| 119/119 [02:30<00:00,  1.27s/it, loss=1.03] \n",
      "Validation: 100%|██████████| 20/20 [00:40<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 - Validation Dice: 0.0483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/1000: 100%|██████████| 119/119 [02:31<00:00,  1.27s/it, loss=1.02] \n",
      "Validation: 100%|██████████| 20/20 [00:40<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 - Validation Dice: 0.0483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/1000:  56%|█████▋    | 67/119 [01:33<01:12,  1.40s/it, loss=0.807]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 400\u001b[39m\n\u001b[32m    396\u001b[39m                 f.write(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    399\u001b[39m trainer = SimpleSegmentationTrainer(\u001b[33m\"\u001b[39m\u001b[33mtumor.yaml\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m best_metric, best_epoch = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 338\u001b[39m, in \u001b[36mSimpleSegmentationTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    334\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStarting training for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.config.training.max_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m epochs\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.config.training.max_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m    337\u001b[39m     \u001b[38;5;66;03m# Train for one epoch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m     train_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m     train_losses.append(train_loss)\n\u001b[32m    341\u001b[39m     \u001b[38;5;66;03m# Validate\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 294\u001b[39m, in \u001b[36mSimpleSegmentationTrainer.train_epoch\u001b[39m\u001b[34m(self, epoch)\u001b[39m\n\u001b[32m    289\u001b[39m epoch_loss = \u001b[32m0\u001b[39m\n\u001b[32m    290\u001b[39m progress = tqdm(\n\u001b[32m    291\u001b[39m     \u001b[38;5;28mself\u001b[39m.train_loader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.config.training.max_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    292\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Process inputs\u001b[39;49;00m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimage_cols\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_key\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlabel_cols\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arsonist\\Documents\\p158\\p158-env\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arsonist\\Documents\\p158\\p158-env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arsonist\\Documents\\p158\\p158-env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1491\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1488\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1490\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1491\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1494\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arsonist\\Documents\\p158\\p158-env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1453\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1449\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1450\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1451\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1452\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1453\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1454\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1455\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arsonist\\Documents\\p158\\p158-env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1284\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1272\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1281\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1282\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1283\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1284\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1285\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1286\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1287\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1288\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1289\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\queues.py:113\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[32m    112\u001b[39m     timeout = deadline - time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\connection.py:257\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\connection.py:346\u001b[39m, in \u001b[36mPipeConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._got_empty_message \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m    344\u001b[39m             _winapi.PeekNamedPipe(\u001b[38;5;28mself\u001b[39m._handle)[\u001b[32m0\u001b[39m] != \u001b[32m0\u001b[39m):\n\u001b[32m    345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\connection.py:896\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m    893\u001b[39m                 ready_objects.add(o)\n\u001b[32m    894\u001b[39m                 timeout = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m896\u001b[39m     ready_handles = \u001b[43m_exhaustive_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaithandle_to_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    898\u001b[39m     \u001b[38;5;66;03m# request that overlapped reads stop\u001b[39;00m\n\u001b[32m    899\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m ov \u001b[38;5;129;01min\u001b[39;00m ov_list:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\connection.py:828\u001b[39m, in \u001b[36m_exhaustive_wait\u001b[39m\u001b[34m(handles, timeout)\u001b[39m\n\u001b[32m    826\u001b[39m ready = []\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m L:\n\u001b[32m--> \u001b[39m\u001b[32m828\u001b[39m     res = _winapi.WaitForMultipleObjects(L, \u001b[38;5;28;01mFalse\u001b[39;00m, timeout)\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m res == WAIT_TIMEOUT:\n\u001b[32m    830\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import monai\n",
    "from monai.networks.nets import UNet\n",
    "from monai.transforms import (\n",
    "    LoadImaged,\n",
    "    EnsureChannelFirstd,\n",
    "    ScaleIntensityd,\n",
    "    Orientationd,\n",
    "    Spacingd,\n",
    "    CropForegroundd,\n",
    "    ToTensord,\n",
    "    Compose,\n",
    "    RandCropByPosNegLabeld,\n",
    "    SpatialPadd,\n",
    "    ResizeWithPadOrCropd,\n",
    ")\n",
    "from monai.data import Dataset, DataLoader\n",
    "from monai.losses import DiceFocalLoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.inferers import SlidingWindowInferer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import munch\n",
    "\n",
    "# Add MetaTensor to safe globals for pickle\n",
    "from monai.data.meta_tensor import MetaTensor\n",
    "\n",
    "torch.serialization.add_safe_globals([MetaTensor])\n",
    "\n",
    "\n",
    "def load_config(fn: str = \"config.yaml\"):\n",
    "    \"Load config from YAML and return a serialized dictionary object\"\n",
    "    with open(fn, \"r\") as stream:\n",
    "        config = yaml.safe_load(stream)\n",
    "    config = munch.munchify(config)\n",
    "\n",
    "    if not config.overwrite:\n",
    "        i = 1\n",
    "        while os.path.exists(config.run_id + f\"_{i}\"):\n",
    "            i += 1\n",
    "        config.run_id += f\"_{i}\"\n",
    "\n",
    "    config.out_dir = os.path.join(config.run_id, config.out_dir)\n",
    "    config.log_dir = os.path.join(config.run_id, config.log_dir)\n",
    "\n",
    "    if not isinstance(config.data.image_cols, (tuple, list)):\n",
    "        config.data.image_cols = [config.data.image_cols]\n",
    "    if not isinstance(config.data.label_cols, (tuple, list)):\n",
    "        config.data.label_cols = [config.data.label_cols]\n",
    "\n",
    "    config.transforms.mode = (\"bilinear\",) * len(config.data.image_cols) + (\n",
    "        \"nearest\",\n",
    "    ) * len(config.data.label_cols)\n",
    "    return config\n",
    "\n",
    "\n",
    "# Simple 3D UNet Trainer\n",
    "class SimpleSegmentationTrainer:\n",
    "    def __init__(self, config_file=\"tumor.yaml\"):\n",
    "        self.config = load_config(config_file)\n",
    "        self.device = torch.device(\n",
    "            self.config.device if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "\n",
    "        # Create output directories\n",
    "        self.setup_directories()\n",
    "\n",
    "        # Set random seed\n",
    "        torch.manual_seed(self.config.seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(self.config.seed)\n",
    "\n",
    "        # Initialize the model\n",
    "        self.create_model()\n",
    "\n",
    "        # Setup data loaders\n",
    "        self.setup_data()\n",
    "\n",
    "        # Setup loss function and optimizer\n",
    "        self.setup_training()\n",
    "\n",
    "    def setup_directories(self):\n",
    "        \"\"\"Create necessary directories for outputs\"\"\"\n",
    "        os.makedirs(self.config.out_dir, exist_ok=True)\n",
    "        os.makedirs(self.config.model_dir, exist_ok=True)\n",
    "        os.makedirs(self.config.log_dir, exist_ok=True)\n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\"Initialize a simple UNet model\"\"\"\n",
    "        self.model = UNet(\n",
    "            spatial_dims=self.config.ndim,\n",
    "            in_channels=self.config.model.in_channels,\n",
    "            out_channels=self.config.model.out_channels,\n",
    "            channels=self.config.model.channels,\n",
    "            strides=self.config.model.strides,\n",
    "            dropout=self.config.model.dropout,\n",
    "            num_res_units=self.config.model.num_res_units,\n",
    "        )\n",
    "\n",
    "        self.model = self.model.to(self.device)\n",
    "        print(f\"Model created: {type(self.model).__name__}\")\n",
    "\n",
    "    def setup_data(self):\n",
    "        \"\"\"Setup data loaders for training and validation\"\"\"\n",
    "        # Read CSV files\n",
    "        train_df = pd.read_csv(self.config.data.train_csv)\n",
    "        valid_df = pd.read_csv(self.config.data.valid_csv)\n",
    "\n",
    "        if self.config.debug:\n",
    "            train_df = train_df.sample(4)\n",
    "            valid_df = valid_df.sample(2)\n",
    "\n",
    "        # Process data paths\n",
    "        data_dir = self.config.data.data_dir\n",
    "        image_cols = (\n",
    "            self.config.data.image_cols\n",
    "            if isinstance(self.config.data.image_cols, list)\n",
    "            else [self.config.data.image_cols]\n",
    "        )\n",
    "        label_cols = (\n",
    "            self.config.data.label_cols\n",
    "            if isinstance(self.config.data.label_cols, list)\n",
    "            else [self.config.data.label_cols]\n",
    "        )\n",
    "\n",
    "        # Create data dictionaries\n",
    "        train_files = []\n",
    "        for _, row in train_df.iterrows():\n",
    "            data_dict = {}\n",
    "            for col in image_cols:\n",
    "                data_dict[col] = os.path.join(data_dir, row[col])\n",
    "            for col in label_cols:\n",
    "                data_dict[col] = os.path.join(data_dir, row[col])\n",
    "            train_files.append(data_dict)\n",
    "\n",
    "        val_files = []\n",
    "        for _, row in valid_df.iterrows():\n",
    "            data_dict = {}\n",
    "            for col in image_cols:\n",
    "                data_dict[col] = os.path.join(data_dir, row[col])\n",
    "            for col in label_cols:\n",
    "                data_dict[col] = os.path.join(data_dir, row[col])\n",
    "            val_files.append(data_dict)\n",
    "\n",
    "        # Create transforms\n",
    "        train_transforms = self.get_transforms(image_cols, label_cols)\n",
    "        val_transforms = self.get_transforms(image_cols, label_cols)\n",
    "\n",
    "        # Create datasets and data loaders\n",
    "        train_ds = Dataset(data=train_files, transform=train_transforms)\n",
    "        val_ds = Dataset(data=val_files, transform=val_transforms)\n",
    "\n",
    "        self.train_loader = DataLoader(\n",
    "            train_ds,\n",
    "            batch_size=self.config.data.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            persistent_workers=False,  # Make sure this is False\n",
    "        )\n",
    "\n",
    "        self.val_loader = DataLoader(\n",
    "            val_ds,\n",
    "            batch_size=1,  # Always use batch size 1 for validation\n",
    "            num_workers=4,\n",
    "            persistent_workers=False,  # Make sure this is False\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Data loaders created - Train: {len(self.train_loader)} batches, Val: {len(self.val_loader)} batches\"\n",
    "        )\n",
    "\n",
    "    def get_transforms(self, image_cols, label_cols):\n",
    "        \"\"\"Create transforms with proper mode handling and ensuring consistent sizes\"\"\"\n",
    "        all_keys = [*image_cols, *label_cols]\n",
    "\n",
    "        # Generate modes with bilinear for images and nearest for labels\n",
    "        modes = [\"bilinear\"] * len(image_cols) + [\"nearest\"] * len(label_cols)\n",
    "\n",
    "        # Define a fixed size to enforce consistent dimensions\n",
    "        # Using a multiple of 8 or 16 is good for UNet architectures\n",
    "        # This helps avoid size mismatch issues in skip connections\n",
    "        roi_size = (64, 64, 64)  # Fixed size that works well with UNet\n",
    "\n",
    "        # Basic transforms\n",
    "        transforms_list = [\n",
    "            LoadImaged(keys=all_keys),\n",
    "            EnsureChannelFirstd(keys=all_keys),\n",
    "            Spacingd(\n",
    "                keys=all_keys,\n",
    "                pixdim=self.config.transforms.spacing,\n",
    "                mode=modes,\n",
    "            ),\n",
    "            Orientationd(keys=all_keys, axcodes=self.config.transforms.orientation),\n",
    "            ScaleIntensityd(keys=image_cols),\n",
    "            CropForegroundd(keys=all_keys, source_key=image_cols[0]),\n",
    "            # Add padding to ensure dimensions are multiples of 8\n",
    "            SpatialPadd(keys=all_keys, spatial_size=roi_size),\n",
    "            # Crop or pad to the exact ROI size\n",
    "            ResizeWithPadOrCropd(keys=all_keys, spatial_size=roi_size),\n",
    "            ToTensord(keys=all_keys),\n",
    "        ]\n",
    "\n",
    "        return Compose(transforms_list)\n",
    "\n",
    "    def setup_training(self):\n",
    "        \"\"\"Setup loss function, optimizer and learning rate scheduler\"\"\"\n",
    "        # Loss function\n",
    "        loss_params = self.config.loss.DiceFocalLoss\n",
    "        print(f\"loss_params: This is the loss params {loss_params}\")\n",
    "\n",
    "        # Configure loss function\n",
    "        self.loss_function = DiceFocalLoss(\n",
    "            include_background=loss_params.include_background,\n",
    "            to_onehot_y=loss_params.to_onehot_y,\n",
    "            softmax=loss_params.softmax,\n",
    "        )\n",
    "\n",
    "        # Print the optimizer structure to debug\n",
    "        print(f\"Optimizer structure: {self.config.optimizer}\")\n",
    "\n",
    "        # Optimizer - use Novograd with hardcoded parameters from tumor.yaml\n",
    "        from monai.optimizers import Novograd\n",
    "\n",
    "        self.optimizer = Novograd(\n",
    "            self.model.parameters(),\n",
    "            lr=0.001,  # Use hardcoded value from your YAML\n",
    "            weight_decay=0.01,  # Use hardcoded value from your YAML\n",
    "            amsgrad=True,  # Use hardcoded value from your YAML\n",
    "        )\n",
    "\n",
    "        # Learning rate scheduler - assume OneCycleLR\n",
    "        scheduler_params = self.config.lr_scheduler.OneCycleLR\n",
    "\n",
    "        self.scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "            self.optimizer,\n",
    "            max_lr=scheduler_params.max_lr,\n",
    "            steps_per_epoch=len(self.train_loader),\n",
    "            epochs=self.config.training.max_epochs,\n",
    "        )\n",
    "\n",
    "        # Metrics\n",
    "        self.metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
    "\n",
    "        # Inferer for sliding window inference\n",
    "        self.inferer = SlidingWindowInferer(\n",
    "            roi_size=(64, 64, 64), sw_batch_size=4, overlap=0.5\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Training setup complete - Loss: {type(self.loss_function).__name__}, Optimizer: Novograd\"\n",
    "        )\n",
    "\n",
    "    def validate(self, epoch):\n",
    "        \"\"\"Run validation\"\"\"\n",
    "        self.model.eval()\n",
    "        metric_values = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_data in tqdm(self.val_loader, desc=\"Validation\"):\n",
    "                # Process inputs\n",
    "                inputs = torch.cat(\n",
    "                    [batch_data[key] for key in self.config.data.image_cols], dim=1\n",
    "                ).to(self.device)\n",
    "                label_key = self.config.data.label_cols\n",
    "                if isinstance(label_key, list):\n",
    "                    label_key = label_key[0]\n",
    "                labels = batch_data[label_key].to(self.device)\n",
    "\n",
    "                # Use sliding window inference for validation\n",
    "                outputs = self.inferer(inputs, self.model)\n",
    "\n",
    "                # Calculate metrics\n",
    "                self.metric(y_pred=outputs, y=labels)\n",
    "                metric_values.append(self.metric.aggregate().item())\n",
    "                self.metric.reset()\n",
    "\n",
    "        # Calculate mean Dice score\n",
    "        mean_metric = np.mean(metric_values)\n",
    "        print(f\"Epoch {epoch} - Validation Dice: {mean_metric:.4f}\")\n",
    "\n",
    "        return mean_metric\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        \"\"\"Run one epoch of training\"\"\"\n",
    "        self.model.train()\n",
    "        epoch_loss = 0\n",
    "        progress = tqdm(\n",
    "            self.train_loader, desc=f\"Epoch {epoch}/{self.config.training.max_epochs}\"\n",
    "        )\n",
    "\n",
    "        for batch_data in progress:\n",
    "            # Process inputs\n",
    "            inputs = torch.cat(\n",
    "                [batch_data[key] for key in self.config.data.image_cols], dim=1\n",
    "            ).to(self.device)\n",
    "            label_key = self.config.data.label_cols\n",
    "            if isinstance(label_key, list):\n",
    "                label_key = label_key[0]\n",
    "            labels = batch_data[label_key].to(self.device)\n",
    "\n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = self.loss_function(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Update learning rate if using OneCycleLR\n",
    "            if hasattr(self, \"scheduler\"):\n",
    "                self.scheduler.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            epoch_loss += loss.item()\n",
    "            progress.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "        # Return average loss for the epoch\n",
    "        return epoch_loss / len(self.train_loader)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Main training loop\"\"\"\n",
    "        best_metric = -1\n",
    "        best_epoch = -1\n",
    "        patience_counter = 0\n",
    "        train_losses = []\n",
    "        val_metrics = []\n",
    "\n",
    "        print(f\"Starting training for {self.config.training.max_epochs} epochs\")\n",
    "\n",
    "        for epoch in range(1, self.config.training.max_epochs + 1):\n",
    "            # Train for one epoch\n",
    "            train_loss = self.train_epoch(epoch)\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "            # Validate\n",
    "            val_metric = self.validate(epoch)\n",
    "            val_metrics.append(val_metric)\n",
    "\n",
    "            # Save best model\n",
    "            if val_metric > best_metric:\n",
    "                best_metric = val_metric\n",
    "                best_epoch = epoch\n",
    "                patience_counter = 0\n",
    "\n",
    "                # Save model\n",
    "                model_path = os.path.join(\n",
    "                    self.config.model_dir, f\"{self.config.run_id}_best_model.pth\"\n",
    "                )\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"epoch\": epoch,\n",
    "                        \"model_state_dict\": self.model.state_dict(),\n",
    "                        \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "                        \"val_metric\": val_metric,\n",
    "                    },\n",
    "                    model_path,\n",
    "                )\n",
    "\n",
    "                print(\n",
    "                    f\"Saved new best model at epoch {epoch} with Dice: {val_metric:.4f}\"\n",
    "                )\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            # Early stopping\n",
    "            if patience_counter >= self.config.training.early_stopping_patience:\n",
    "                print(f\"Early stopping triggered after {epoch} epochs\")\n",
    "                break\n",
    "\n",
    "            # Save training progress\n",
    "            self.save_metrics(train_losses, val_metrics)\n",
    "\n",
    "        print(\n",
    "            f\"Training completed. Best model at epoch {best_epoch} with Dice: {best_metric:.4f}\"\n",
    "        )\n",
    "        return best_metric, best_epoch\n",
    "\n",
    "    def save_metrics(self, train_losses, val_metrics):\n",
    "        \"\"\"Save metrics to files\"\"\"\n",
    "        # Save training loss\n",
    "        with open(os.path.join(self.config.log_dir, \"train_logs.csv\"), \"w\") as f:\n",
    "            f.write(\"epoch,loss\\n\")\n",
    "            for i, loss in enumerate(train_losses):\n",
    "                f.write(f\"{i+1},{loss}\\n\")\n",
    "\n",
    "        # Save validation metrics\n",
    "        with open(os.path.join(self.config.log_dir, \"metric_logs.csv\"), \"w\") as f:\n",
    "            f.write(\"epoch,dice\\n\")\n",
    "            for i, metric in enumerate(val_metrics):\n",
    "                f.write(f\"{i+1},{metric}\\n\")\n",
    "\n",
    "\n",
    "trainer = SimpleSegmentationTrainer(\"tumor.yaml\")\n",
    "best_metric, best_epoch = trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p158-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
